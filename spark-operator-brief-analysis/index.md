# Spark Operator æµ…æ


{{< admonition type=tip title="" open=true >}}
ä¸­ç§‹å¿«ä¹ ğŸŒ•ğŸ¥®
{{< /admonition >}}

## Spark è¿è¡Œæ—¶æ¶æ„

ç»è¿‡è¿‘å‡ å¹´çš„é«˜é€Ÿå‘å±•ï¼Œåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶çš„æ¶æ„é€æ¸è¶‹åŒã€‚èµ„æºç®¡ç†æ¨¡å—ä½œä¸ºå…¶ä¸­æœ€é€šç”¨çš„æ¨¡å—é€æ¸ä¸æ¡†æ¶è§£è€¦ï¼Œç‹¬ç«‹æˆé€šç”¨çš„ç»„ä»¶ã€‚ç›®å‰å¤§éƒ¨åˆ†åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶éƒ½æ”¯æŒæ¥å…¥å¤šæ¬¾ä¸åŒçš„èµ„æºç®¡ç†å™¨ã€‚èµ„æºç®¡ç†å™¨è´Ÿè´£é›†ç¾¤èµ„æºçš„ç®¡ç†å’Œè°ƒåº¦ï¼Œä¸ºè®¡ç®—ä»»åŠ¡åˆ†é…èµ„æºå®¹å™¨å¹¶ä¿è¯èµ„æºéš”ç¦»ã€‚Apache Spark ä½œä¸ºé€šç”¨åˆ†å¸ƒå¼è®¡ç®—å¹³å°ï¼Œç›®å‰åŒæ—¶æ”¯æŒå¤šæ¬¾èµ„æºç®¡ç†å™¨ï¼ŒåŒ…æ‹¬ï¼š
* YARN
* Mesos
* Kubernetes
* Spark Standalone

Apache Spark çš„è¿è¡Œæ—¶æ¡†æ¶å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå…¶ä¸å„ç±»èµ„æºè°ƒåº¦å™¨çš„äº¤äº’æµç¨‹æ¯”è¾ƒç±»ä¼¼ï¼š

{{< figure src="/images/202109/spark-operator-brief-analysis/spark-runtime-architecture.png" title="Sparkè¿è¡Œæ—¶æ¡†æ¶ï¼ˆClientæ¨¡å¼ï¼‰" height="500" width="700">}}

å…¶ä¸­ï¼ŒDriver è´Ÿè´£ä½œä¸šé€»è¾‘çš„è°ƒåº¦å’Œä»»åŠ¡çš„ç›‘æ§ï¼Œèµ„æºç®¡ç†å™¨è´Ÿè´£èµ„æºåˆ†é…å’Œç›‘æ§ã€‚Driver æ ¹æ®éƒ¨ç½²æ¨¡å¼çš„ä¸åŒï¼Œå¯åŠ¨å’Œè¿è¡Œçš„ç‰©ç†ä½ç½®æœ‰æ‰€ä¸åŒã€‚å…¶ä¸­ï¼ŒClient æ¨¡å¼ä¸‹ï¼ŒDriver æ¨¡å—è¿è¡Œåœ¨ Spark-Submit è¿›ç¨‹ä¸­ï¼ŒCluster æ¨¡å¼ä¸‹ï¼ŒDriver çš„å¯åŠ¨è¿‡ç¨‹å’ŒExecutor ç±»ä¼¼ï¼Œè¿è¡Œåœ¨èµ„æºè°ƒåº¦å™¨åˆ†é…çš„èµ„æºå®¹å™¨å†…ã€‚

K8s æ˜¯ Spark åœ¨ 2.3 å¼€å§‹æ”¯æŒèµ„æºç®¡ç†å™¨ï¼Œè€Œç›¸å…³è®¨è®ºæ—©åœ¨2016å¹´å°±å·²ç»å¼€å§‹å±•å¼€ã€‚Sparkå¯¹ K8s çš„æ”¯æŒéšç€ç‰ˆæœ¬çš„è¿­ä»£ä¹Ÿé€æ­¥æ·±å…¥ï¼Œåœ¨å‘å¸ƒçš„ 3.0 ä¸­ï¼ŒSpark on K8s æä¾›äº†æ›´å¥½çš„ Kerberos æ”¯æŒå’Œèµ„æºåŠ¨æ€æ”¯æŒçš„ç‰¹æ€§ã€‚

## Spark on Kubernetes
Kubernetes æ˜¯ç”± Google å¼€æºçš„ä¸€æ¬¾é¢å‘åº”ç”¨çš„å®¹å™¨é›†ç¾¤éƒ¨ç½²å’Œç®¡ç†ç³»ç»Ÿ,è¿‘å¹´æ¥å‘å±•ååˆ†è¿…çŒ›ï¼Œç›¸å…³ç”Ÿæ€å·²ç»æ—¥è¶‹å®Œå–„ã€‚åœ¨ Spark å®˜æ–¹æ¥å…¥ K8s å‰ï¼Œç¤¾åŒºé€šå¸¸é€šè¿‡åœ¨ K8s é›†ç¾¤ä¸Šéƒ¨ç½²ä¸€ä¸ª Spark Standalone é›†ç¾¤çš„æ–¹å¼æ¥å®ç°åœ¨ K8s é›†ç¾¤ä¸Šè¿è¡Œ Spark ä»»åŠ¡çš„ç›®çš„ã€‚æ–¹æ¡ˆæ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

{{< figure src="/images/202109/spark-operator-brief-analysis/spark-standalone-on-k8s.png" title="Spark Standalone on K8s" height="500" width="700">}}

è¿™ä¸ªæ¨¡å¼ç®€å•æ˜“ç”¨ï¼Œä½†å­˜åœ¨ç›¸å½“å¤§çš„ç¼ºé™·ï¼š
* æ— æ³•æŒ‰éœ€æ‰©å±•ï¼ŒSpark Standalone éƒ¨ç½²åé›†ç¾¤è§„æ¨¡å›ºå®šï¼Œæ— æ³•æ ¹æ®ä½œä¸šéœ€æ±‚è‡ªåŠ¨æ‰©å±•é›†ç¾¤ï¼›
* æ— æ³•åˆ©ç”¨K8såŸç”Ÿèƒ½åŠ›ï¼ŒSpark Standalone å†…å»ºçš„èµ„æºè°ƒåº¦å™¨ä¸æ”¯æŒæ‰©å±•ï¼Œéš¾ä»¥æ¥å…¥K8s è°ƒåº¦ï¼Œæ— æ³•åˆ©ç”¨ K8s æä¾›çš„äº‘åŸç”Ÿç‰¹æ€§ï¼›
* Spark Standalone é›†ç¾¤åœ¨å¤šç§Ÿæˆ·èµ„æºéš”ç¦»ä¸Šå¤©ç”Ÿå­˜åœ¨çŸ­æ¿ã€‚

Spark on Kubernetes æ–¹æ¡ˆæ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè®¾è®¡ç»†èŠ‚å¯ä»¥å‚è€ƒï¼š[SPARK-18278](https://issues.apache.org/jira/browse/SPARK-18278?spm=a2c6h.12873639.0.0.35f321c4BcQgLG)

{{< figure src="/images/202109/spark-operator-brief-analysis/spark-on-k8s.png" title="Spark on K8s" height="500" width="700">}}


åœ¨è¿™ä¸ªæ–¹æ¡ˆä¸­ï¼Œ
1. Spark-Submit é€šè¿‡è°ƒç”¨ K8s API åœ¨ K8s é›†ç¾¤ä¸­å¯åŠ¨ä¸€ä¸ª Spark Driver Podï¼›
2. Driver é€šè¿‡è°ƒç”¨ K8s API å¯åŠ¨ç›¸åº”çš„ Executor Podï¼Œç»„æˆä¸€ä¸ª Spark Application é›†ç¾¤ï¼Œå¹¶æŒ‡æ´¾ä½œä¸šä»»åŠ¡åˆ°è¿™äº› Executor ä¸­æ‰§è¡Œ;
3. ä½œä¸šç»“æŸåï¼ŒExecutor Pod ä¼šè¢«é”€æ¯ï¼Œè€Œ Driver Pod ä¼šæŒä¹…åŒ–ç›¸å…³æ—¥å¿—ï¼Œå¹¶ä¿æŒåœ¨ `completed` çŠ¶æ€ï¼Œç›´åˆ°ç”¨æˆ·æ‰‹æ¸…ç†æˆ–è¢« K8s é›†ç¾¤çš„åƒåœ¾å›æ”¶æœºåˆ¶å›æ”¶ã€‚

å½“å‰çš„æ–¹æ¡ˆå·²ç»è§£å†³äº† Spark Standalone on K8s æ–¹æ¡ˆçš„éƒ¨åˆ†ç¼ºé™·ï¼Œç„¶è€Œï¼ŒSpark Application çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†æ–¹å¼å’Œè°ƒåº¦æ–¹å¼ä¸ K8s å†…ç½®çš„å·¥ä½œè´Ÿè½½ï¼ˆå¦‚ Deploymentsã€DaemonSetsã€StatefulSetsç­‰ï¼‰å­˜åœ¨è¾ƒå¤§å·®å¼‚ï¼Œåœ¨ K8s ä¸Šæ‰§è¡Œä½œä¸šä»ç„¶å­˜åœ¨ä¸å°‘é—®é¢˜ï¼š
1. Spark-submit åœ¨ K8s é›†ç¾¤ä¹‹å¤–ï¼Œä½¿ç”¨éå£°æ˜å¼çš„æäº¤æ¥å£ï¼›
2. Spark Application ä¹‹é—´æ²¡æœ‰ååŒè°ƒåº¦ï¼Œåœ¨å°é›†ç¾¤ä¸­å¾ˆå®¹æ˜“å‡ºç°è°ƒåº¦é¥¿æ­»çš„æƒ…å†µï¼›
3. éœ€è¦æ‰‹åŠ¨é…ç½®ç½‘ç»œï¼Œæ¥è®¿é—® WebUIï¼›
4. ä»»åŠ¡ç›‘æ§æ¯”è¾ƒéº»çƒ¦ï¼Œæ²¡æœ‰æ¥å…¥ Prometheus é›†ç¾¤ç›‘æ§ã€‚

å½“ç„¶Spark on Kubernetes æ–¹æ¡ˆç›®å‰è¿˜åœ¨å¿«é€Ÿå¼€å‘ä¸­ï¼Œæ›´å¤šç‰¹æ€§ä¸ä¹…ä¼šå‘å¸ƒå‡ºæ¥ï¼Œç›¸ä¿¡æœªæ¥å’Œ K8s çš„é›†æˆä¼šæ›´åŠ ç´§å¯†å’Œ Nativeï¼Œè¿™äº›ç‰¹æ€§åŒ…æ‹¬ï¼š

* åŠ¨æ€èµ„æºåˆ†é…å’Œå¤–éƒ¨ Shuffle æœåŠ¡
* æœ¬åœ°æ–‡ä»¶ä¾èµ–ç®¡ç†å™¨
* Spark Application ç®¡ç†å™¨
* ä½œä¸šé˜Ÿåˆ—å’Œèµ„æºç®¡ç†å™¨

## Spark Operator æµ…æ
åœ¨åˆ†æ Spark Operator çš„å®ç°ä¹‹å‰ï¼Œå…ˆç®€å•æ¢³ç†ä¸‹ Kubernetes Operator çš„åŸºæœ¬æ¦‚å¿µã€‚Kubernetes Operator æ˜¯ç”± CoreOS å¼€å‘çš„ Kubernetes æ‰©å±•ç‰¹æ€§ï¼Œç›®æ ‡æ˜¯é€šè¿‡å®šä¹‰ä¸€ç³»åˆ— CRD å’Œå®ç°æ§åˆ¶å™¨ï¼Œå°†ç‰¹å®šé¢†åŸŸçš„åº”ç”¨ç¨‹åºè¿ç»´æŠ€æœ¯å’ŒçŸ¥è¯†ï¼ˆå¦‚éƒ¨ç½²æ–¹æ³•ã€ç›‘æ§ã€æ•…éšœæ¢å¤ç­‰ï¼‰é€šè¿‡ä»£ç çš„æ–¹å¼å›ºåŒ–ä¸‹æ¥ã€‚Spark Operator æ˜¯ Google åŸºäº Operator æ¨¡å¼å¼€å‘çš„ä¸€æ¬¾çš„å·¥å…·(https://github.com/GoogleCloudPlatform/spark-on-k8s-operator)ï¼Œç”¨äºé€šè¿‡å£°æ˜å¼çš„æ–¹å¼å‘ K8s é›†ç¾¤æäº¤ Spark ä½œä¸šã€‚ä½¿ç”¨ Spark Operator ç®¡ç† Spark åº”ç”¨ï¼Œèƒ½æ›´å¥½çš„åˆ©ç”¨ K8s åŸç”Ÿèƒ½åŠ›æ§åˆ¶å’Œç®¡ç† Spark åº”ç”¨çš„ç”Ÿå‘½å‘¨æœŸï¼ŒåŒ…æ‹¬åº”ç”¨çŠ¶æ€ç›‘æ§ã€æ—¥å¿—è·å–ã€åº”ç”¨è¿è¡Œæ§åˆ¶ç­‰ï¼Œå¼¥è¡¥ Spark on K8s æ–¹æ¡ˆåœ¨é›†æˆ K8s ä¸Šä¸å…¶ä»–ç±»å‹çš„è´Ÿè½½ä¹‹é—´å­˜åœ¨çš„å·®è·ã€‚
ä¸‹é¢ç®€å•åˆ†æä¸‹Spark Operatorçš„å®ç°ç»†èŠ‚ã€‚
### ç³»ç»Ÿæ¶æ„

{{< figure src="/images/202109/spark-operator-brief-analysis/spark-operator-architecture.png" title="Spark Operator æ¶æ„" height="500" width="700">}}

å¯ä»¥çœ‹å‡ºï¼ŒSpark Operator ç›¸æ¯” Spark on K8sï¼Œæ¶æ„ä¸Šè¦å¤æ‚ä¸€äº›ï¼Œå®é™…ä¸Š Spark Operator é›†æˆäº† Spark on K8s çš„æ–¹æ¡ˆï¼Œæä¾›äº†æ›´å…¨é¢ç®¡æ§ç‰¹æ€§ã€‚é€šè¿‡ Spark Operatorï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨æ›´åŠ ç¬¦åˆ K8s ç†å¿µçš„æ–¹å¼æ¥æ§åˆ¶ Spark åº”ç”¨çš„ç”Ÿå‘½å‘¨æœŸã€‚Spark Operator åŒ…æ‹¬å¦‚ä¸‹å‡ ä¸ªç»„ä»¶ï¼š
1. SparkApplication æ§åˆ¶å™¨ï¼šè¯¥æ§åˆ¶å™¨ç”¨äºåˆ›å»ºã€æ›´æ–°ã€åˆ é™¤SparkApplicationå¯¹è±¡ï¼ŒåŒæ—¶æ§åˆ¶å™¨è¿˜ä¼šç›‘æ§ç›¸åº”çš„äº‹ä»¶,æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œï¼›
2. Submission Runnerï¼šè´Ÿè´£è°ƒç”¨ spark-submit æäº¤ Spark ä½œä¸šï¼Œä½œä¸šæäº¤çš„æµç¨‹å®Œå…¨å¤ç”¨ Spark on K8s çš„æ¨¡å¼ï¼›
3. Spark Pod Monitorï¼šç›‘æ§ Spark ä½œä¸šç›¸å…³ Pod çš„çŠ¶æ€ï¼Œå¹¶åŒæ­¥åˆ°æ§åˆ¶å™¨ä¸­ï¼›
4. Mutating Admission Webhookï¼šå¯é€‰æ¨¡å—ï¼ŒåŸºäºæ³¨è§£æ¥å®ç° Driver/Executor Pod çš„ä¸€äº›å®šåˆ¶åŒ–éœ€æ±‚ï¼›
5. SparkCtlï¼šç”¨äºå’Œ Spark Operator äº¤äº’çš„å‘½ä»¤è¡Œå·¥å…·ã€‚

Spark Operator é™¤äº†å®ç°åŸºæœ¬çš„ä½œä¸šæäº¤å¤–ï¼Œè¿˜æ”¯æŒå¦‚ä¸‹ç‰¹æ€§ï¼š
* å£°æ˜å¼çš„ä½œä¸šç®¡ç†ï¼›
* æ”¯æŒæ›´æ–° SparkApplication å¯¹è±¡åè‡ªåŠ¨é‡æ–°æäº¤ä½œä¸šï¼›
* æ”¯æŒå¯é…ç½®çš„é‡å¯ç­–ç•¥ï¼›
* æ”¯æŒå¤±è´¥é‡è¯•ï¼›
* é›†æˆ Prometheusï¼Œå¯ä»¥æ”¶é›†å’Œè½¬å‘ Spark åº”ç”¨çº§åˆ«çš„åº¦é‡å’Œ Driver/Executor çš„åº¦é‡åˆ° Prometheus ä¸­ã€‚

### å·¥ç¨‹ç»“æ„
Spark Operator çš„é¡¹ç›®æ˜¯æ ‡å‡†çš„ K8s Operator ç»“æ„ï¼Œå…¶ä¸­æœ€é‡è¦çš„åŒ…æ‹¬ï¼š

* manifestï¼šå®šä¹‰äº† Spark ç›¸å…³çš„ CRDï¼ŒåŒ…æ‹¬:
  * ScheduledSparkApplicationï¼šè¡¨ç¤ºä¸€ä¸ªå®šæ—¶æ‰§è¡Œçš„ Spark ä½œä¸š
  * SparkApplicationï¼šè¡¨ç¤ºä¸€ä¸ª Spark ä½œä¸š
* pkgï¼šå…·ä½“çš„ Operator é€»è¾‘å®ç°
  * apiï¼šå®šä¹‰äº† Operator çš„å¤šä¸ªç‰ˆæœ¬çš„ API
  * clientï¼šç”¨äºå¯¹æ¥çš„ client-go SDK
  * controllerï¼šè‡ªå®šä¹‰æ§åˆ¶å™¨çš„å®ç°,åŒ…æ‹¬ï¼š
    * ScheduledSparkApplication æ§åˆ¶å™¨
    * SparkApplication æ§åˆ¶å™¨
  * batchscheduler: æ‰¹å¤„ç†è°ƒåº¦å™¨é›†æˆæ¨¡å—, ç›®å‰å·²ç»é›†æˆäº† K8s volcano è°ƒåº¦å™¨
* spark-dockerï¼šspark docker é•œåƒ
* sparkctlï¼šspark operator å‘½ä»¤è¡Œå·¥å…·

### Spark Application æ§åˆ¶å™¨
æ§åˆ¶å™¨çš„ä»£ç ä¸»è¦ä½äº`pkg/controller/sparkapplication/controller.go`ä¸­ã€‚
#### æäº¤ä½œä¸š
æäº¤ä½œä¸šçš„æäº¤ä½œä¸šçš„ä¸»æµç¨‹åœ¨ submitSparkApplication æ–¹æ³•ä¸­ã€‚

```go
// controller.go
// submitSparkApplication creates a new submission for the given SparkApplication and submits it using spark-submit.
func (c *Controller) submitSparkApplication(app *v1beta2.SparkApplication) *v1beta2.SparkApplication {
    if app.PrometheusMonitoringEnabled() {
        ...
        configPrometheusMonitoring(app, c.kubeClient)
    }

    // Use batch scheduler to perform scheduling task before submitting (before build command arguments).
    if needScheduling, scheduler := c.shouldDoBatchScheduling(app); needScheduling {
        newApp, err := scheduler.DoBatchSchedulingOnSubmission(app)
        ...
        //Spark submit will use the updated app to submit tasks(Spec will not be updated into API server)
        app = newApp
    }

    driverPodName := getDriverPodName(app)
    submissionID := uuid.New().String()
    submissionCmdArgs, err := buildSubmissionCommandArgs(app, driverPodName, submissionID)
    ...
    // Try submitting the application by running spark-submit.
    submitted, err := runSparkSubmit(newSubmission(submissionCmdArgs, app))
    ...
    app.Status = v1beta2.SparkApplicationStatus{
        SubmissionID: submissionID,
        AppState: v1beta2.ApplicationState{
            State: v1beta2.SubmittedState,
        },
        DriverInfo: v1beta2.DriverInfo{
            PodName: driverPodName,
        },
        SubmissionAttempts:        app.Status.SubmissionAttempts + 1,
        ExecutionAttempts:         app.Status.ExecutionAttempts + 1,
        LastSubmissionAttemptTime: metav1.Now(),
    }
    c.recordSparkApplicationEvent(app)

    service, err := createSparkUIService(app, c.kubeClient)
    ...
    ingress, err := createSparkUIIngress(app, *service, c.ingressURLFormat, c.kubeClient)
    return app
}
```

æäº¤ä½œä¸šçš„æ ¸å¿ƒé€»è¾‘åœ¨ `submission.go` è¿™ä¸ªæ¨¡å—ä¸­ï¼š

```go
// submission.go
func runSparkSubmit(submission *submission) (bool, error) {
    sparkHome, present := os.LookupEnv(sparkHomeEnvVar)
    if !present {
        glog.Error("SPARK_HOME is not specified")
    }
    var command = filepath.Join(sparkHome, "/bin/spark-submit")

    cmd := execCommand(command, submission.args...)
    glog.V(2).Infof("spark-submit arguments: %v", cmd.Args)
    output, err := cmd.Output()
    glog.V(3).Infof("spark-submit output: %s", string(output))
    if err != nil {
        var errorMsg string
        if exitErr, ok := err.(*exec.ExitError); ok {
            errorMsg = string(exitErr.Stderr)
        }
        // The driver pod of the application already exists.
        if strings.Contains(errorMsg, podAlreadyExistsErrorCode) {
            glog.Warningf("trying to resubmit an already submitted SparkApplication %s/%s", submission.namespace, submission.name)
            return false, nil
        }
        if errorMsg != "" {
            return false, fmt.Errorf("failed to run spark-submit for SparkApplication %s/%s: %s", submission.namespace, submission.name, errorMsg)
        }
        return false, fmt.Errorf("failed to run spark-submit for SparkApplication %s/%s: %v", submission.namespace, submission.name, err)
    }

    return true, nil
}

func buildSubmissionCommandArgs(app *v1beta2.SparkApplication, driverPodName string, submissionID string) ([]string, error) {
    ...
    options, err := addDriverConfOptions(app, submissionID)
    ...
    options, err = addExecutorConfOptions(app, submissionID)
    ...
}

func getMasterURL() (string, error) {
    kubernetesServiceHost := os.Getenv(kubernetesServiceHostEnvVar)
    ...
    kubernetesServicePort := os.Getenv(kubernetesServicePortEnvVar)
    ...
    return fmt.Sprintf("k8s://https://%s:%s", kubernetesServiceHost, kubernetesServicePort), nil
}
```
å¯ä»¥çœ‹å‡º,

1. å¯ä»¥é…ç½®æ§åˆ¶å™¨å¯ç”¨ Prometheus è¿›è¡Œåº¦é‡æ”¶é›†ï¼›
2. Spark Operator é€šè¿‡æ‹¼è£…ä¸€ä¸ª spark-submit å‘½ä»¤å¹¶æ‰§è¡Œï¼Œå®ç°æäº¤ Spark ä½œä¸šåˆ° K8s é›†ç¾¤ä¸­çš„ç›®çš„ï¼›
3. åœ¨æ¯æ¬¡æäº¤å‰ï¼ŒSpark Operator éƒ½ä¼šç”Ÿæˆä¸€ä¸ª UUID ä½œä¸º Session IDï¼Œå¹¶é€šè¿‡ Spark ç›¸å…³é…ç½®å¯¹ Driver/Executor çš„ Pod è¿›è¡Œæ ‡è®°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ª ID æ¥è·Ÿè¸ªå’Œæ§åˆ¶è¿™ä¸ª Spark ä½œä¸šï¼›
4. Controller é€šè¿‡ç›‘æ§ç›¸å…³ä½œä¸šçš„ Pod çš„çŠ¶æ€æ¥æ›´æ–° SparkApplication çš„çŠ¶æ€ï¼ŒåŒæ—¶é©±åŠ¨ SparkApplication å¯¹è±¡çš„çŠ¶æ€æµè½¬ï¼›
5. æäº¤æˆåŠŸåï¼Œè¿˜ä¼šåšå¦‚ä¸‹å‡ ä»¶äº‹æƒ…:
  1. æ›´æ–°ä½œä¸šçš„çŠ¶æ€ï¼›
  2. å¯åŠ¨ä¸€ä¸ªServiceï¼Œå¹¶é…ç½®å¥½Ingressï¼Œæ–¹ä¾¿ç”¨æˆ·è®¿é—®Spark WebUIã€‚

#### çŠ¶æ€æµè½¬æ§åˆ¶

åœ¨ Spark Operator ä¸­ï¼ŒController ä½¿ç”¨çŠ¶æ€æœºæ¥ç»´æŠ¤ Spark Application çš„çŠ¶æ€ä¿¡æ¯ï¼ŒçŠ¶æ€æµè½¬å’Œ Action çš„å…³ç³»å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

{{< figure src="/images/202109/spark-operator-brief-analysis/state-machine-for-sparkapplication.png" title="State Machine for SparkApplication" height="500" width="700">}}

ä½œä¸šæäº¤åï¼ŒSpark Application çš„çŠ¶æ€æ›´æ–°éƒ½æ˜¯é€šè¿‡ `getAndUpdateAppState()` æ–¹æ³•æ¥å®ç°çš„ã€‚
```go
// controller.go
func (c *Controller) getAndUpdateAppState(app *v1beta2.SparkApplication) error {
    if err := c.getAndUpdateDriverState(app); err != nil {
        return err
    }
    if err := c.getAndUpdateExecutorState(app); err != nil {
        return err
    }
    return nil
}
// getAndUpdateDriverState finds the driver pod of the application
// and updates the driver state based on the current phase of the pod.
func (c *Controller) getAndUpdateDriverState(app *v1beta2.SparkApplication) error {
    // Either the driver pod doesn't exist yet or its name has not been updated.
    ...
    driverPod, err := c.getDriverPod(app)
    ...
    if driverPod == nil {
        app.Status.AppState.ErrorMessage = "Driver Pod not found"
        app.Status.AppState.State = v1beta2.FailingState
        app.Status.TerminationTime = metav1.Now()
        return nil
    }
    
    app.Status.SparkApplicationID = getSparkApplicationID(driverPod)
    ...
    newState := driverStateToApplicationState(driverPod.Status)
    // Only record a driver event if the application state (derived from the driver pod phase) has changed.
    if newState != app.Status.AppState.State {
        c.recordDriverEvent(app, driverPod.Status.Phase, driverPod.Name)
    }
    app.Status.AppState.State = newState

    return nil
}

// getAndUpdateExecutorState lists the executor pods of the application
// and updates the executor state based on the current phase of the pods.
func (c *Controller) getAndUpdateExecutorState(app *v1beta2.SparkApplication) error {
    pods, err := c.getExecutorPods(app)
    ...
    executorStateMap := make(map[string]v1beta2.ExecutorState)
    var executorApplicationID string
    for _, pod := range pods {
        if util.IsExecutorPod(pod) {
            newState := podPhaseToExecutorState(pod.Status.Phase)
            oldState, exists := app.Status.ExecutorState[pod.Name]
            // Only record an executor event if the executor state is new or it has changed.
            if !exists || newState != oldState {
                c.recordExecutorEvent(app, newState, pod.Name)
            }
            executorStateMap[pod.Name] = newState

            if executorApplicationID == "" {
                executorApplicationID = getSparkApplicationID(pod)
            }
        }
    }

    // ApplicationID label can be different on driver/executors. Prefer executor ApplicationID if set.
    // Refer https://issues.apache.org/jira/projects/SPARK/issues/SPARK-25922 for details.
    ...
    if app.Status.ExecutorState == nil {
        app.Status.ExecutorState = make(map[string]v1beta2.ExecutorState)
    }
    for name, execStatus := range executorStateMap {
        app.Status.ExecutorState[name] = execStatus
    }

    // Handle missing/deleted executors.
    for name, oldStatus := range app.Status.ExecutorState {
        _, exists := executorStateMap[name]
        if !isExecutorTerminated(oldStatus) && !exists {
            // If ApplicationState is SUCCEEDING, in other words, the driver pod has been completed
            // successfully. The executor pods terminate and are cleaned up, so we could not found
            // the executor pod, under this circumstances, we assume the executor pod are completed.
            if app.Status.AppState.State == v1beta2.SucceedingState {
                app.Status.ExecutorState[name] = v1beta2.ExecutorCompletedState
            } else {
                glog.Infof("Executor pod %s not found, assuming it was deleted.", name)
                app.Status.ExecutorState[name] = v1beta2.ExecutorFailedState
            }
        }
    }

    return nil
}
```
ä»è¿™æ®µä»£ç å¯ä»¥çœ‹åˆ°ï¼ŒSpark Application æäº¤åï¼ŒController ä¼šé€šè¿‡ç›‘å¬ Driver Pod å’ŒExecutor Pod çŠ¶æ€æ¥è®¡ç®— Spark Application çš„çŠ¶æ€ï¼Œæ¨åŠ¨çŠ¶æ€æœºçš„æµè½¬ã€‚
#### åº¦é‡ç›‘æ§
å¦‚æœä¸€ä¸ª SparkApplication ç¤ºä¾‹é…ç½®äº†å¼€å¯åº¦é‡ç›‘æ§ç‰¹æ€§ï¼Œé‚£ä¹ˆ Spark Operator ä¼šåœ¨Spark-Submit æäº¤å‚æ•°ä¸­å‘ Driver å’Œ Executor çš„ JVM å‚æ•°ä¸­æ·»åŠ ç±»ä¼¼`-javaagent:/prometheus/jmx_prometheus_javaagent-0.11.0.jar=8090:/etc/metrics/conf/prometheus.yaml`çš„ JavaAgent å‚æ•°æ¥å¼€å¯ SparkApplication åº¦é‡ç›‘æ§ï¼Œå®ç°é€šè¿‡ JmxExporter å‘ Prometheus å‘é€åº¦é‡æ•°æ®ã€‚

{{< figure src="/images/202109/spark-operator-brief-analysis/prometheus-architecture.png" title="Prometheus æ¶æ„" height="500" width="700">}}
