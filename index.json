[{"categories":["云原生"],"content":"本文介绍了监控和日志的背景信息、监控方案的演进、日志采集的一些细节以及常见的日志的开源系统","date":"2021-08-15","objectID":"/kubernetes-monitor-and-log/","tags":["云原生","Kubernetes","监控","日志","问题诊断"],"title":"Kubernetes 监控与日志","uri":"/kubernetes-monitor-and-log/"},{"categories":["云原生"],"content":"背景监控和日志是大型分布式系统的重要基础设施，监控可以帮助开发者查看系统的运行状态，而日志可以协助问题的排查和诊断。 在 Kubernetes 中，监控和日志属于生态的一部分，它并不是核心组件，因此大部分的能力依赖上层的云厂商的适配。Kubernetes 定义了介入的接口标准和规范，任何符合接口标准的组件都可以快速集成。 ","date":"2021-08-15","objectID":"/kubernetes-monitor-and-log/:1:0","tags":["云原生","Kubernetes","监控","日志","问题诊断"],"title":"Kubernetes 监控与日志","uri":"/kubernetes-monitor-and-log/"},{"categories":["云原生"],"content":"监控","date":"2021-08-15","objectID":"/kubernetes-monitor-and-log/:2:0","tags":["云原生","Kubernetes","监控","日志","问题诊断"],"title":"Kubernetes 监控与日志","uri":"/kubernetes-monitor-and-log/"},{"categories":["云原生"],"content":"监控类型 资源监控 比较常见的像 CPU、内存、网络这种资源类的一个指标，通常这些指标会以数值、百分比的单位进行统计，是最常见的一个监控方式。这种监控方式在常规的监控里面，类似项目 zabbix telegraph，这些系统都是可以做到的。 性能监控 性能监控指的就是 APM 监控，也就是说常见的一些应用性能类的监控指标的检查。通常是通过一些 Hook 的机制在虚拟机层、字节码执行层通过隐式调用，或者是在应用层显示注入，获取更深层次的一个监控指标，一般是用来应用的调优和诊断的。比较常见的类似像 jvm 或者 php 的 Zend Engine，通过一些常见的 Hook 机制，拿到类似像 jvm 里面的 GC 的次数，各种内存代的一个分布以及网络连接数的一些指标，通过这种方式来进行应用的性能诊断和调优。 安全监控 安全监控主要是对安全进行的一系列的监控策略，类似像越权管理、安全漏洞扫描等等。 事件监控 从正常的状态转换成另一个正常的状态的时候，会发生一个 normal 的事件，而从一个正常状态转换成一个异常状态的时候，会发生一个 warning 的事件。通常情况下，warning 的事件是我们比较关心的，而事件监控就是可以把 normal 的事件或者是 warning 事件离线到一个数据中心，然后通过数据中心的分析以及报警。 ","date":"2021-08-15","objectID":"/kubernetes-monitor-and-log/:2:1","tags":["云原生","Kubernetes","监控","日志","问题诊断"],"title":"Kubernetes 监控与日志","uri":"/kubernetes-monitor-and-log/"},{"categories":["云原生"],"content":"K8s 监控演进在早期，也就是 1.10 以前的 K8s 版本。大家都会使用类似像 Heapster 这样的组件来去进行监控的采集，Heapster 的设计原理其实也比较简单。 Heapster 采集和消费链路 首先，我们在每一个 Kubernetes 上面有一个包裹好的 cadvisor，这个 cadvisor 是负责数据采集的组件。当 cadvisor 把数据采集完成，Kubernetes 会把 cadvisor 采集到的数据进行包裹，暴露成相应的 API。在早期的时候，实际上是有三种不同的 API： summary 接口； kubelet 接口； Prometheus 接口； 这三种接口，其实对应的数据源都是 cadvisor，只是数据格式有所不同。而在 Heapster 里面，其实支持了 summary 接口和 kubelet 两种数据采集接口，Heapster 会定期去每一个节点拉取数据，在自己的内存里面进行聚合，然后再暴露相应的 service，供上层的消费者进行使用。在 K8s 中比较常见的消费者，类似像 dashboard，或者是 HPA-Controller，它通过调用 service 获取相应的监控数据，来实现相应的弹性伸缩，以及监控数据的一个展示。 这个是以前的一个数据消费链路，这条消费链路看上去很清晰，也没有太多的一个问题，那为什么 Kubernetes 会将 Heapster 放弃掉而转换到 metrics-service 呢？其实这个主要的一个动力来源是由于 Heapster 在做监控数据接口的标准化。为什么要做监控数据接口标准化呢？ 第一点在于客户的需求是千变万化的，比如说今天用 Heapster 进行了基础数据的一个资源采集，那明天的时候，我想在应用里面暴露在线人数的一个数据接口，放到自己的接口系统里进行数据的一个展现，以及类似像 HPA 的一个数据消费。那这个场景在 Heapster 下能不能做呢？答案是不可以的，所以这就是 Heapster 自身扩展性的弊端； 第二点是 Heapster 里面为了保证数据的离线能力，提供了很多的 sink，而这个 sink 包含了类似像 influxdb、sls、钉钉等等一系列 sink。这个 sink 主要做的是把数据采集下来，并且把这个数据离线走，然后很多客户会用 influxdb 做这个数据离线，在 influxdb 上去接入类似像 grafana 监控数据的一个可视化的软件，来实践监控数据的可视化。 但是后来社区发现，这些 sink 很多时候都是没有人来维护的。这也导致整个 Heapster 的项目有很多的 bug，这个 bug 一直存留在社区里面，是没有人修复的，这个也是会给社区的项目的活跃度包括项目的稳定性带来了很多的挑战。 基于这两点原因，K8s 把 Heapster 进行了 break 掉，然后做了一个精简版的监控采集组件，叫做 metrics-server。 Heapster 内部组成 上图是 Heapster 内部的一个架构。大家可以发现它分为几个部分： 第一个部分是 core 部分，然后上层是有一个通过标准的 http 或者 https 暴露的这个 API。 然后中间是 source 的部分，source 部分相当于是采集数据暴露的不同的接口。 然后 processor 的部分是进行数据转换以及数据聚合的部分。 最后是 sink 部分，sink 部分是负责数据离线的，这个是早期的 Heapster 的一个应用的架构。 那到后期的时候呢，K8s 做了这个监控接口的一个标准化，逐渐就把 Heapster 进行了裁剪，转化成了 metrics-server。 Metrics Server 内部组成 目前 0.3.1 版本的 metrics-server 大致的一个结构就变成了上图这样，是非常简单的：有一个 core 层、中间的 source 层，以及简单的 API 层，额外增加了 API Registration 这层。这层的作用就是它可以把相应的数据接口注册到 K8s 的 API server 之上，以后客户不再需要通过这个 API 层去访问 metrics-server，而是可以通过这个 API 注册层，通过 API server 访问 API 注册层，再到 metrics-server。这样的话，真正的数据消费方可能感知到的并不是一个 metrics-server，而是说感知到的是实现了这样一个 API 的具体的实现，而这个实现是 metrics-server。这个就是 metrics-server 改动最大的一个地方。 ","date":"2021-08-15","objectID":"/kubernetes-monitor-and-log/:2:2","tags":["云原生","Kubernetes","监控","日志","问题诊断"],"title":"Kubernetes 监控与日志","uri":"/kubernetes-monitor-and-log/"},{"categories":["云原生"],"content":"K8s 监控接口标准在 K8s 里面针对于监控，有三种不同的接口标准。它将监控的数据消费能力进行了标准化和解耦，实现了一个与社区的融合，社区里面主要分为三类。 第一类 Resource Metrice 对应的接口是 metrics.k8s.io，主要的实现就是 metrics-server，它提供的是资源的监控，比较常见的是节点级别、pod 级别、namespace 级别、class 级别。这类的监控指标都可以通过 metrics.k8s.io 这个接口获取到。 第二类 Custom Metrics 对应的 API 是 custom.metrics.k8s.io，主要的实现是 Prometheus。它提供的是资源监控和自定义监控，资源监控和上面的资源监控其实是有覆盖关系的，而这个自定义监控指的是：比如应用上面想暴露一个类似像在线人数，或者说调用后面的这个数据库的 MySQL 的慢查询。这些其实都是可以在应用层做自己的定义的，然后并通过标准的 Prometheus 的 client，暴露出相应的 metrics，然后再被 Prometheus 进行采集。 而这类的接口一旦采集上来也是可以通过类似像 custom.metrics.k8s.io 这样一个接口的标准来进行数据消费的，也就是说现在如果以这种方式接入的 Prometheus，那你就可以通过 custom.metrics.k8s.io 这个接口来进行 HPA，进行数据消费。 第三类 External Metrics External Metrics 其实是比较特殊的一类，因为我们知道 K8s 现在已经成为了云原生接口的一个实现标准。很多时候在云上打交道的是云服务，比如说在一个应用里面用到了前面的是消息队列，后面的是 RBS 数据库。那有时在进行数据消费的时候，同时需要去消费一些云产品的监控指标，类似像消息队列中消息的数目，或者是接入层 SLB 的 connection 数目，SLB 上层的 200 个请求数目等等，这些监控指标。 那怎么去消费呢？也是在 K8s 里面实现了一个标准，就是 external.metrics.k8s.io。主要的实现厂商就是各个云厂商的 provider，通过这个 provider 可以通过云资源的监控指标。 ","date":"2021-08-15","objectID":"/kubernetes-monitor-and-log/:2:3","tags":["云原生","Kubernetes","监控","日志","问题诊断"],"title":"Kubernetes 监控与日志","uri":"/kubernetes-monitor-and-log/"},{"categories":["云原生"],"content":"Promethues - 开源社区的监控“标准”接下来我们来看一个比较常见的开源社区里面的监控方案，就是 Prometheus。Prometheus 为什么说是开源社区的监控标准呢？ 一是因为首先 Prometheus 是 CNCF 云原生社区的一个毕业项目。然后第二个是现在有越来越多的开源项目都以 Prometheus 作为监控标准，类似说我们比较常见的 Spark、Tensorflow、Flink 这些项目，其实它都有标准的 Prometheus 的采集接口。 第二个是对于类似像比较常见的一些数据库、中间件这类的项目，它都有相应的 Prometheus 采集客户端。类似像 ETCD、zookeeper、MySQL 或者说 PostgreSQL，这些其实都有相应的这个 Prometheus 的接口，如果没有的，社区里面也会有相应的 exporter 进行接口的一个实现。 Promethues 上图是 Prometheus 采集的数据链路，它主要可以分为三种不同的数据采集链路。 第一种，是这个 push 的方式，就是通过 pushgateway 进行数据采集，然后数据线到 pushgateway，然后 Prometheus 再通过 pull 的方式去 pushgateway 去拉数据。这种采集方式主要应对的场景就是你的这个任务可能是比较短暂的，比如说我们知道 Prometheus，最常见的采集方式是拉模式，那带来一个问题就是，一旦你的数据声明周期短于数据的采集周期，比如我采集周期是 30s，而我这个任务可能运行 15s 就完了。这种场景之下，可能会造成有些数据漏采。对于这种场景最简单的一个做法就是先通过 pushgateway，先把你的 metrics push下来，然后再通过 pull 的方式从 pushgateway 去拉数据，通过这种方式可以做到，短时间的不丢作业任务。 第二种是标准的 pull 模式，它是直接通过拉模式去对应的数据的任务上面去拉取数据。 第三种是 Prometheus on Prometheus，就是可以通过另一个 Prometheus 来去同步数据到这个 Prometheus。 这是三种 Prometheus 中的采集方式。那从数据源上面，除了标准的静态配置，Prometheus 也支持 service discovery。也就是说可以通过一些服务发现的机制，动态地去发现一些采集对象。在 K8s 里面比较常见的是可以有 Kubernetes 的这种动态发现机制，只需要配置一些 annotation，它就可以自动地来配置采集任务来进行数据采集，是非常方便的。 在报警上面，Prometheus 提供了一个外置组件叫 Alentmanager，它可以将相应的报警信息通过邮件或者短信的方式进行数据的一个告警。在数据消费上面，可以通过上层的 API clients，可以通过 web UI，可以通过 Grafana 进行数据的展现和数据的消费。 总结起来 Prometheus 有如下五个特点： 第一个特点就是简介强大的接入标准，开发者只需要实现 Prometheus Client 这样一个接口标准，就可以直接实现数据的一个采集； 第二种就是多种的数据采集、离线的方式。可以通过 push 的方式、 pull 的方式、Prometheus on Prometheus的方式来进行数据的采集和离线； 第三种就是和 K8s 的兼容； 第四种就是丰富的插件机制与生态； 第五个是 Prometheus Operator 的一个助力，Prometheus Operator 可能是目前我们见到的所有 Operator 里面做的最复杂的，但是它里面也是把 Prometheus 这种动态能力做到淋漓尽致的一个 Operator，如果在 K8s 里面使用 Prometheus，比较推荐大家使用 Prometheus Operator 的方式来去进行部署和运维。 ","date":"2021-08-15","objectID":"/kubernetes-monitor-and-log/:2:4","tags":["云原生","Kubernetes","监控","日志","问题诊断"],"title":"Kubernetes 监控与日志","uri":"/kubernetes-monitor-and-log/"},{"categories":["云原生"],"content":"日志","date":"2021-08-15","objectID":"/kubernetes-monitor-and-log/:3:0","tags":["云原生","Kubernetes","监控","日志","问题诊断"],"title":"Kubernetes 监控与日志","uri":"/kubernetes-monitor-and-log/"},{"categories":["云原生"],"content":"日志的场景1. 主机内核的日志 第一个是主机内核的日志，主机内核日志可以协助开发者进行一些常见的问题与诊断，比如说网栈的异常，类似像我们的 iptables mark，它可以看到有 controller table 这样的一些 message； 第二个是驱动异常，比较常见的是一些网络方案里面有的时候可能会出现驱动异常，或者说是类似 GPU 的一些场景，驱动异常可能是比较常见的一些错误； 第三个就是文件系统异常，在早期 docker 还不是很成熟的场景之下，overlayfs 或者是 AUFS，实际上是会经常出现问题的。在这些出现问题后，开发者是没有太好的办法来去进行监控和诊断的。这一部分，其实是可以主机内核日志里面来查看到一些异常； 再往下是影响节点的一些异常，比如说内核里面的一些 kernel panic，或者是一些 OOM，这些也会在主机日志里面有相应的一些反映。 2. Runtime 的日志 第二个是 runtime 的日志，比较常见的是 Docker 的一些日志，我们可以通过 docker 的日志来排查类似像删除一些 Pod Hang 这一系列的问题。 3. 核心组件的日志 第三个是核心组件的日志，在 K8s 里面核心组件包含了类似像一些外置的中间件，类似像 etcd，或者像一些内置的组件，类似像 API server、kube-scheduler、controller-manger、kubelet 等等这一系列的组件。而这些组件的日志可以帮我们来看到整个 K8s 集群里面管控面的一个资源的使用量，然后以及目前运行的一个状态是否有一些异常。 还有的就是类似像一些核心的中间件，如 Ingress 这种网络中间件，它可以帮我们来看到整个的一个接入层的一个流量，通过 Ingress 的日志，可以做到一个很好的接入层的一个应用分析。 4. 部署应用的日志 最后是部署应用的日志，可以通过应用的日志来查看业务层的一个状态。比如说可以看业务层有没有 500 的请求？有没有一些 panic？有没有一些异常的错误的访问？那这些其实都可以通过应用日志来进行查看的。 ","date":"2021-08-15","objectID":"/kubernetes-monitor-and-log/:3:1","tags":["云原生","Kubernetes","监控","日志","问题诊断"],"title":"Kubernetes 监控与日志","uri":"/kubernetes-monitor-and-log/"},{"categories":["云原生"],"content":"日志的采集 Logging 首先是宿主机文件，这种场景比较常见的是说我的这个容器里面，通过类似像 volume，把日志文件写到了宿主机之上。通过宿主机的日志轮转的策略进行日志的轮转，然后再通过我的宿主机上的这个 agent 进行采集； 第二种是容器内有日志文件，那这种常见方式怎么处理呢，比较常见的一个方式是说我通过一个 Sidecar 的 streaming 的 container，转写到 stdout，通过 stdout 写到相应的 log-file，然后再通过本地的一个日志轮转，然后以及外部的一个 agent 采集； 第三种我们直接写到 stdout，这种比较常见的一个策略，第一种就是直接我拿这个 agent 去采集到远端，第二种我直接通过类似像一些 sls 的标准 API 采集到远端。 那社区里面其实比较推荐的是使用 Fluentd 的一个采集方案，Fluentd 是在每一个节点上面都会起相应的 agent，然后这个 agent 会把数据汇集到一个 Fluentd 的一个 server，这个 server 里面可以将数据离线到相应的类似像 elasticsearch，然后再通过 kibana 做展现；或者是离线到 influxdb，然后通过 Grafana 做展现。这个其实是社区里目前比较推荐的一个做法。 ","date":"2021-08-15","objectID":"/kubernetes-monitor-and-log/:3:2","tags":["云原生","Kubernetes","监控","日志","问题诊断"],"title":"Kubernetes 监控与日志","uri":"/kubernetes-monitor-and-log/"},{"categories":["云原生"],"content":"总结 首先主要为大家介绍了监控，其中包括：四种容器场景下的常见的监控方式；Kubernetes 的监控演进和接口标准；两种常用的来源的监控方案； 在日志上我们主要介绍了四种不同的场景，介绍了 Fluentd 的一个采集方案。 ","date":"2021-08-15","objectID":"/kubernetes-monitor-and-log/:4:0","tags":["云原生","Kubernetes","监控","日志","问题诊断"],"title":"Kubernetes 监控与日志","uri":"/kubernetes-monitor-and-log/"},{"categories":["云原生"],"content":"本文介绍了 K8s Liveness 和 Readiness 的使用方式、常见问题的诊断、远程调试的方式","date":"2021-08-07","objectID":"/kubernetes-debug/","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"需求来源首先来看一下，整个需求的来源：当把应用迁移到 Kubernetes 之后，要如何去保障应用的健康与稳定呢？其实很简单，可以从两个方面来进行增强： 首先是提高应用的可观测性； 第二是提高应用的可恢复能力。 从可观测性上来讲，可以在三个方面来去做增强： 首先是应用的健康状态上面，可以实时地进行观测； 第二个是可以获取应用的资源使用情况； 第三个是可以拿到应用的实时日志，进行问题的诊断与分析。 当出现了问题之后，首先要做的事情是要降低影响的范围，进行问题的调试与诊断。最后当出现问题的时候，理想的状况是：可以通过和 K8s 集成的自愈机制进行完整的恢复。 ","date":"2021-08-07","objectID":"/kubernetes-debug/:1:0","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"Liveness 与 Readiness","date":"2021-08-07","objectID":"/kubernetes-debug/:2:0","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"初识 Liveness 与 ReadinessLiveness probe 也叫就绪指针，用来判断一个 pod 是否处在就绪状态。当一个 pod 处在就绪状态的时候，它才能够对外提供相应的服务，也就是说接入层的流量才能打到相应的 pod。当这个 pod 不处在就绪状态的时候，接入层会把相应的流量从这个 pod 上面进行摘除。 来看一下简单的一个例子： 如下图其实就是一个 Readiness 就绪的一个例子： Readiness Probe Ready 当这个 pod 指针判断一直处在失败状态的时候，其实接入层的流量不会打到现在这个 pod 上。 Readiness Probe Fail 当这个 pod 的状态从 FAIL 的状态转换成 success 的状态时，它才能够真实地承载这个流量。 Liveness 指针也是类似的，它是存活指针，用来判断一个 pod 是否处在存活状态。当一个 pod 处在不存活状态的时候，会出现什么事情呢？ Liveness Probe Fail 这个时候会由上层的判断机制来判断这个 pod 是否需要被重新拉起。那如果上层配置的重启策略是 restart always 的话，那么此时这个 pod 会直接被重新拉起。 ","date":"2021-08-07","objectID":"/kubernetes-debug/:2:1","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"使用方式探测方式Liveness 指针和 Readiness 指针支持三种不同的探测方式： 第一种是 httpGet。它是通过发送 http Get 请求来进行判断的，当返回码是 200-399 之间的状态码时，标识这个应用是健康的； 第二种探测方式是 Exec。它是通过执行容器中的一个命令来判断当前的服务是否是正常的，当命令行的返回结果是 0，则标识容器是健康的； 第三种探测方式是 tcpSocket。它是通过探测容器的 IP 和 Port 进行 TCP 健康检查，如果这个 TCP 的链接能够正常被建立，那么标识当前这个容器是健康的。 探测结果从探测结果来讲主要分为三种： 第一种是 success，当状态是 success 的时候，表示 container 通过了健康检查，也就是 Liveness probe 或 Readiness probe 是正常的一个状态； 第二种是 Failure，Failure 表示的是这个 container 没有通过健康检查，如果没有通过健康检查的话，那么此时就会进行相应的一个处理，那在 Readiness 处理的一个方式就是通过 service。service 层将没有通过 Readiness 的 pod 进行摘除，而 Liveness 就是将这个 pod 进行重新拉起，或者是删除。 第三种状态是 Unknown，Unknown 是表示说当前的执行的机制没有进行完整的一个执行，可能是因为类似像超时或者像一些脚本没有及时返回，那么此时 Readiness-probe 或 Liveness-probe 会不做任何的一个操作，会等待下一次的机制来进行检验。 那在 kubelet 里面有一个叫 ProbeManager 的组件，这个组件里面会包含 Liveness-probe 或 Readiness-probe，这两个 probe 会将相应的 Liveness 诊断和 Readiness 诊断作用在 pod 之上，来实现一个具体的判断。 ","date":"2021-08-07","objectID":"/kubernetes-debug/:2:2","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"Pod Probe Spec下面介绍这三种方式不同的检测方式的一个 yaml 文件的使用。 首先先看一下 exec，exec 的使用其实非常简单。如下YAML所示，大家可以看到这是一个 Liveness probe，它里面配置了一个 exec 的一个诊断。接下来，它又配置了一个 command 的字段，这个 command 字段里面通过 cat 一个具体的文件来判断当前 Liveness probe 的状态，当这个文件里面返回的结果是 0 时，或者说这个命令返回是 0 时，它会认为此时这个 pod 是处在健康的一个状态。 apiVersion:v1kind:Podmetadata:labels:test:livenessname:liveness-execspec:containers:- name:livenessimage:k8s.gcr.io/busyboxargs:- /bin/sh- -c- touch /tmp/healthy:sleep 30; rm -rf /tmp/healthy; sleep 600livenessProbe:exec:command:- cat- /tmp/healthyinitialDelaySeconds:5periodSeconds:5 那再来看一下这个 httpGet，httpGet 里面有一个字段是路径，第二个字段是 port，第三个是 headers。这个地方有时需要通过类似像 header 头的一个机制做 health 的一个判断时，需要配置这个 header，通常情况下，可能只需要通过 health 和 port 的方式就可以了。 apiVersion:v1kind:Podmetadata:labels:test:livenessname:liveness-httpspec:containers:- name:livenessimage:k8s.gcr.io/livenessargs:- /serverlivenessProbe:httpGet:path:/healthzport:8080httpHeaders:- name:Custom-Headervalue:AwesomeinitialDelaySeconds:3periodSeconds:3 第三种是 tcpSocket，tcpSocket 的使用方式其实也比较简单，你只需要设置一个检测的端口，像这个例子里面使用的是 8080 端口，当这个 8080 端口 tcp connect 审核正常被建立的时候，那 tecSocket，Probe 会认为是健康的一个状态。 apiVersion:v1kind:Podmetadata:labels:app:goproxyname:goproxyspec:containers:- name:goproxyimage:k8s.gcr.io/goproxy:0.1ports:- containerPort:8080readinessProbe:tcpSocket:port:8080initialDelaySeconds:5periodSeconds:10livenessProbe:tcpSocket:port:8080initialDelaySeconds:15periodSeconds:20 此外还有如下的五个参数，是 Global 的参数。 第一个参数叫 initialDelaySeconds，它表示的是说这个 pod 启动延迟多久进行一次检查，比如说现在有一个 Java 的应用，它启动的时间可能会比较长，因为涉及到 jvm 的启动，包括 Java 自身 jar 的加载。所以前期，可能有一段时间是没有办法被检测的，而这个时间又是可预期的，那这时可能要设置一下 initialDelaySeconds； 第二个是 periodSeconds，它表示的是检测的时间间隔，正常默认的这个值是 10 秒； 第三个字段是 timeoutSeconds，它表示的是检测的超时时间，当超时时间之内没有检测成功，那它会认为是失败的一个状态； 第四个是 successThreshold，它表示的是：当这个 pod 从探测失败到再一次判断探测成功，所需要的阈值次数，默认情况下是 1 次，表示原本是失败的，那接下来探测这一次成功了，就会认为这个 pod 是处在一个探针状态正常的一个状态； 最后一个参数是 failureThreshold，它表示的是探测失败的重试次数，默认值是 3，表示的是当从一个健康的状态连续探测 3 次失败，那此时会判断当前这个pod的状态处在一个失败的状态。 ","date":"2021-08-07","objectID":"/kubernetes-debug/:2:3","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"Liveness 与 Readiness 总结 介绍 Liveness 指针是存活指针，它用来判断容器是否存活、判断 pod 是否 running。如果 Liveness 指针判断容器不健康，此时会通过 kubelet 杀掉相应的 pod，并根据重启策略来判断是否重启这个容器。如果默认不配置 Liveness 指针，则默认情况下认为它这个探测默认返回是成功的。 Readiness 指针用来判断这个容器是否启动完成，即 pod 的 condition 是否 ready。如果探测的一个结果是不成功，那么此时它会从 pod 上 Endpoint 上移除，也就是说从接入层上面会把前一个 pod 进行摘除，直到下一次判断成功，这个 pod 才会再次挂到相应的 endpoint 之上。 检测失败 对于检测失败上面来讲 Liveness 指针是直接杀掉这个 pod，而 Readiness 指针是切掉 endpoint 到这个 pod 之间的关联关系，也就是说它把这个流量从这个 pod 上面进行切掉。 适用场景 Liveness 指针适用场景是支持那些可以重新拉起的应用，而 Readiness 指针主要应对的是启动之后无法立即对外提供服务的这些应用。 注意事项 在使用 Liveness 指针和 Readiness 指针的时候有一些注意事项。因为不论是 Liveness 指针还是 Readiness 指针都需要配置合适的探测方式，以免被误操作。 第一个是调大超时的阈值，因为在容器里面执行一个 shell 脚本，它的执行时长是非常长的，平时在一台 ecs 或者在一台 vm 上执行，可能 3 秒钟返回的一个脚本在容器里面需要 30 秒钟。所以这个时间是需要在容器里面事先进行一个判断的，那如果可以调大超时阈值的方式，来防止由于容器压力比较大的时候出现偶发的超时； 第二个是调整判断的一个次数，3 次的默认值其实在比较短周期的判断周期之下，不一定是最佳实践，适当调整一下判断的次数也是一个比较好的方式； 第三个是 exec，如果是使用 shell 脚本的这个判断，调用时间会比较长，比较建议大家可以使用类似像一些编译性的脚本 Golang 或者一些 C 语言、C++ 编译出来的这个二进制的 binary 进行判断，那这种通常会比 shell 脚本的执行效率高 30% 到 50%； 第四个是如果使用 tcpSocket 方式进行判断的时候，如果遇到了 TLS 的服务，那可能会造成后边 TLS 里面有很多这种未健全的 tcp connection，那这个时候需要自己对业务场景上来判断，这种的链接是否会对业务造成影响。 ","date":"2021-08-07","objectID":"/kubernetes-debug/:2:4","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"问题诊断","date":"2021-08-07","objectID":"/kubernetes-debug/:3:0","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"了解状态机制首先要了解一下 K8s 中的一个设计理念，就是这个状态机制。因为 K8s 是整个的一个设计是面向状态机的，它里面通过 yaml 的方式来定义的是一个期望到达的一个状态，而真正这个 yaml 在执行过程中会由各种各样的 controller来负责整体的状态之间的一个转换。 比如说上面的图，实际上是一个 Pod 的一个生命周期。刚开始它处在一个 pending 的状态，那接下来可能会转换到类似像 running，也可能转换到 Unknown，甚至可以转换到 failed。然后，当 running 执行了一段时间之后，它可以转换到类似像 successded 或者是 failed，然后当出现在 unknown 这个状态时，可能由于一些状态的恢复，它会重新恢复到 running 或者 successded 或者是 failed。 其实 K8s 整体的一个状态就是基于这种类似像状态机的一个机制进行转换的，而不同状态之间的转化都会在相应的 K8s对象上面留下来类似像 Status 或者像 Conditions 的一些字段来进行表示。 一个 pod 里面有多个 container，每个 container 上面又会有一个字段叫 State，然后 State 的状态表示当前这个 container 的一个聚合状态。 ","date":"2021-08-07","objectID":"/kubernetes-debug/:3:1","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"常见异常Pod 停留在 Pending第一个就是 pending 状态，pending 表示调度器没有进行介入。此时可以通过 kubectl describe pod 来查看相应的事件，如果由于资源或者说端口占用，或者是由于 node selector 造成 pod 无法调度的时候，可以在相应的事件里面看到相应的结果，这个结果里面会表示说有多少个不满足的 node，有多少是因为 CPU 不满足，有多少是由于 node 不满足，有多少是由于 tag 打标造成的不满足。 Pod 停留在 waiting那第二个状态就是 pod 可能会停留在 waiting 的状态，pod 的 states 处在 waiting 的时候，通常表示说这个 pod 的镜像没有正常拉取，原因可能是由于这个镜像是私有镜像，但是没有配置 Pod secret；那第二种是说可能由于这个镜像地址是不存在的，造成这个镜像拉取不下来；还有一个是说这个镜像可能是一个公网的镜像，造成镜像的拉取失败。 Pod 不断被拉取并且可以看到 crashing第三种是 pod 不断被拉起，而且可以看到类似像 backoff。这个通常表示说 pod 已经被调度完成了，但是启动失败，那这个时候通常要关注的应该是这个应用自身的一个状态，并不是说配置是否正确、权限是否正确，此时需要查看的应该是 pod 的具体日志。 Pod 处在 Runing 但是没有正常工作第四种 pod 处在 running 状态，但是没有正常对外服务。那此时比较常见的一个点就可能是由于一些非常细碎的配置，类似像有一些字段可能拼写错误，造成了 yaml 下发下去了，但是有一段没有正常地生效，从而使得这个 pod 处在 running 的状态没有对外服务，那此时可以通过 apply-validate-f pod.yaml 的方式来进行判断当前 yaml 是否是正常的，如果 yaml 没有问题，那么接下来可能要诊断配置的端口是否是正常的，以及 Liveness 或 Readiness 是否已经配置正确。 Service 无法正常的工作最后一种就是 service 无法正常工作的时候，该怎么去判断呢？那比较常见的 service 出现问题的时候，是自己的使用上面出现了问题。因为 service 和底层的 pod 之间的关联关系是通过 selector 的方式来匹配的，也就是说 pod 上面配置了一些 label，然后 service 通过 match label 的方式和这个 pod 进行相互关联。如果这个 label 配置的有问题，可能会造成这个 service 无法找到后面的 endpoint，从而造成相应的 service 没有办法对外提供服务，那如果 service 出现异常的时候，第一个要看的是这个 service 后面是不是有一个真正的 endpoint，其次来看这个 endpoint 是否可以对外提供正常的服务。 ","date":"2021-08-07","objectID":"/kubernetes-debug/:3:2","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"远程调试","date":"2021-08-07","objectID":"/kubernetes-debug/:4:0","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"Pod 远程调试首先把一个应用部署到集群里面的时候，发现问题的时候，需要进行快速验证，或者说修改的时候，可能需要类似像登陆进这个容器来进行一些诊断。 进入一个正在运行的Pod kubectl exec -it pod-name /bin/bash 进入一个正在运行包含多容器的Pod kubectl exec -it pod-name -c container-name /bin/bash 比如说可以通过 exec 的方式进入一个 pod。像这条命令里面，通过 kubectl exec-it pod-name 后面再填写一个相应的命令，比如说 /bin/bash，表示希望到这个 pod 里面进入一个交互式的一个 bash。然后在 bash 里面可以做一些相应的命令，比如说修改一些配置，通过 supervisor 去重新拉起这个应用，都是可以的。 那如果指定这一个 pod 里面可能包含着多个 container，这个时候该怎么办呢？怎么通过 pod 来指定 container 呢？其实这个时候有一个参数叫做 -c，如上图下方的命令所示。-c 后面是一个 container-name，可以通过 pod 在指定 -c 到这个 container-name，具体指定要进入哪个 container，后面再跟上相应的具体的命令，通过这种方式来实现一个多容器的命令的一个进入，从而实现多容器的一个远程调试。 ","date":"2021-08-07","objectID":"/kubernetes-debug/:4:1","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"Service 远程调试那么 service 的远程调试该怎么做呢？service 的远程调试其实分为两个部分： 第一个部分是说我想将一个服务暴露到远程的一个集群之内，让远程集群内的一些应用来去调用本地的一个服务，这是一条反向的一个链路； 还有一种方式是我想让这个本地服务能够去调远程的服务，那么这是一条正向的链路。 在反向列入上面有这样一个开源组件，叫做 Telepresence，它可以将本地的应用代理到远程集群中的一个 service 上面，使用它的方式非常简单。 首先先将 Telepresence 的一个 Proxy 应用部署到远程的 K8s 集群里面。然后将远程单一个 deployment swap 到本地的一个 application，使用的命令就是 Telepresence-swap-deployment $DEPLOYMENT_NAME 然后以及远程的 DEPLOYMENT_NAME。通过这种方式就可以将本地一个 application 代理到远程的 service 之上、可以将应用在远程集群里面进行本地调试. 第二个是如果本地应用需要调用远程集群的服务时候，可以通过 port-forward 的方式将远程的应用调用到本地的端口之上。比如说现在远程的里面有一个 API server，这个 API server 提供了一些端口，本地在调试 Code 时候，想要直接调用这个 API server，那么这时，比较简单的一个方式就是通过 port-forward 的方式。 kubectl port-forward svc/app -n app-namespace 它的使用方式是 kubectl port-forward，然后 service 加上远程的 service name，再加上相应的 namespace，后面还可以加上一些额外的参数，比如说端口的一个映射，通过这种机制就可以把远程的一个应用代理到本地的端口之上，此时通过访问本地端口就可以访问远程的服务。 ","date":"2021-08-07","objectID":"/kubernetes-debug/:4:2","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"kubectl-debug最后再给大家介绍一个开源的调试工具，它也是 kubectl 的一个插件，叫 kubectl-debug。我们知道在 K8s 里面，底层的容器 runtime 比较常见的就是类似像 docker 或者是 containerd，不论是 docker 还是 containerd，它们使用的一个机制都是基于 Linux namespace 的一个方式进行虚拟化和隔离的。 通常情况下 ，并不会在镜像里面带特别多的调试工具，类似像 netstat telnet 等等这些 ，因为这个会造成应用整体非常冗余。那么如果想要调试的时候该怎么做呢？其实这个时候就可以依赖类似于像 kubectl-debug 这样一个工具。 kubectl-debug 这个工具是依赖于 Linux namespace 的方式来去做的，它可以 datash 一个 Linux namespace 到一个额外的 container，然后在这个 container 里面执行任何的 debug 动作，其实和直接去 debug 这个 Linux namespace 是一致的。这里有一个简单的操作，给大家来介绍一下： 这个地方其实已经安装好了 kubectl-debug，它是 kubectl 的一个插件。所以这个时候，你可以直接通过 kubectl-debug 这条命令来去诊断远程的一个 pod。像这个例子里面，当执行 debug 的时候，实际上它首先会先拉取一些镜像，这个镜像里面实际上会默认带一些诊断的工具。当这个镜像启用的时候，它会把这个 debug container 进行启动。与此同时会把这个 container 和相应的你要诊断的这个 container 的 namespace 进行挂靠，也就说此时这个 container 和你是同 namespace 的，类似像网络站，或者是类似像内核的一些参数，其实都可以在这个 debug container 里面实时地进行查看。 ","date":"2021-08-07","objectID":"/kubernetes-debug/:4:3","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"本节总结 关于 Liveness 和 Readiness 的指针。Liveness probe 就是保活指针，它是用来看 pod 是否存活的，而 Readiness probe 是就绪指针，它是判断这个 pod 是否就绪的，如果就绪了，就可以对外提供服务。这个就是 Liveness 和 Readiness 需要记住的部分； 应用诊断的三个步骤：首先 describe 相应的一个状态；然后提供状态来排查具体的一个诊断方向；最后来查看相应对象的一个 event 获取更详细的一个信息； 提供 pod 一个日志来定位应用的自身的一个状态； 远程调试的一个策略，如果想把本地的应用代理到远程集群，此时可以通过 Telepresence 这样的工具来实现，如果想把远程的应用代理到本地，然后在本地进行调用或者是调试，可以用类似像 port-forward 这种机制来实现。 ","date":"2021-08-07","objectID":"/kubernetes-debug/:5:0","tags":["云原生","Kubernetes","问题诊断","debug"],"title":"Kubernetes 问题诊断","uri":"/kubernetes-debug/"},{"categories":["云原生"],"content":"本文介绍了存储快照概念、使用与工作原理、存储拓扑调度背景、概念、使用与工作原理","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"基础知识","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:1:0","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"存储快照产生背景在使用存储时，为了提高数据操作的容错性，我们通常有需要对线上数据进行snapshot，以及能快速restore的能力。另外，当需要对线上数据进行快速的复制以及迁移等动作，如进行环境的复制、数据开发等功能时，都可以通过存储快照来满足需求，而 K8s 中通过 CSI Snapshotter controller 来实现存储快照的功能。 ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:1:1","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"存储快照用户接口-Snapshot我们知道，K8s 中通过 pvc 以及 pv 的设计体系来简化用户对存储的使用，而存储快照的设计其实是仿照 pvc \u0026 pv 体系的设计思想。当用户需要存储快照的功能时，可以通过 VolumeSnapshot 对象来声明，并指定相应的 VolumeSnapshotClass 对象，之后由集群中的相关组件动态生成存储快照以及存储快照对应的对象 VolumeSnapshotContent。如下对比图所示，动态生成 VolumeSnapshotContent 和动态生成 pv 的流程是非常相似的。 Snapshot ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:1:2","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"存储快照用户接口-Restore有了存储快照之后，如何将快照数据快速恢复过来呢？ Restore 如上所示的流程，可以借助 PVC 对象将其的 dataSource 字段指定为 VolumeSnapshot 对象。这样当 PVC 提交之后，会由集群中的相关组件找到 dataSource 所指向的存储快照数据，然后新创建对应的存储以及 pv 对象，将存储快照数据恢复到新的 pv 中，这样数据就恢复回来了，这就是存储快照的restore用法。 ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:1:3","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"Topolopy-含义首先了解一下拓扑是什么意思：这里所说的拓扑是 K8s 集群中为管理的 nodes 划分的一种“位置”关系，意思为：可以通过在 node 的 labels 信息里面填写某一个 node 属于某一个拓扑。 常见的有三种，这三种在使用时经常会遇到的： 第一种，在使用云存储服务的时候，经常会遇到 region，也就是地区的概念，在 K8s 中常通过 label failure-domain.beta.kubernetes.io/region 来标识。这个是为了标识单个 K8s 集群管理的跨 region 的 nodes 到底属于哪个地区； 第二种，比较常用的是可用区，也就是 available zone，在 K8s 中常通过 label failure-domain.beta.kubernetes.io/zone 来标识。这个是为了标识单个 K8s 集群管理的跨 zone 的 nodes 到底属于哪个可用区； 第三种，是 hostname，就是单机维度，是拓扑域为 node 范围，在 K8s 中常通过 label kubernetes.io/hostname 来标识，这个在文章的最后讲 local pv 的时候，会再详细描述。 上面讲到的三个拓扑是比较常用的，而拓扑其实是可以自己定义的。可以定义一个字符串来表示一个拓扑域，这个 key 所对应的值其实就是拓扑域下不同的拓扑位置。 举个例子：可以用 rack，也就是机房中的机架这个纬度来做一个拓扑域。这样就可以将不同机架 (rack) 上面的机器标记为不同的拓扑位置，也就是说可以将不同机架上机器的位置关系通过 rack 这个纬度来标识。属于 rack1 上的机器，node label 中都添加 rack 的标识，它的 value 就标识成 rack1，即 rack=rack1；另外一组机架上的机器可以标识为 rack=rack2，这样就可以通过机架的纬度就来区分来 K8s 中的 node 所处的位置。 接下来就一起来看看拓扑在 K8s 存储中的使用。 ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:1:4","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"存储拓扑调度产生背景上一节课我们说过，K8s 中通过 PV 的 PVC 体系将存储资源和计算资源分开管理了。如果创建出来的 PV有\"访问位置\"的限制，也就是说，它通过 nodeAffinity 来指定哪些 node 可以访问这个 PV。为什么会有这个访问位置的限制？ 因为在 K8s 中创建 pod 的流程和创建 PV 的流程，其实可以认为是并行进行的，这样的话，就没有办法来保证 pod 最终运行的 node 是能访问到 有位置限制的 PV 对应的存储，最终导致 pod 没法正常运行。这里来举两个经典的例子： 首先来看一下 Local PV 的例子，Local PV 是将一个 node 上的本地存储封装为 PV，通过使用 PV 的方式来访问本地存储。为什么会有 Local PV 的需求呢？简单来说，刚开始使用 PV 或 PVC 体系的时候，主要是用来针对分布式存储的，分布式存储依赖于网络，如果某些业务对 I/O 的性能要求非常高，可能通过网络访问分布式存储没办法满足它的性能需求。这个时候需要使用本地存储，刨除了网络的 overhead，性能往往会比较高。但是用本地存储也是有坏处的！分布式存储可以通过多副本来保证高可用，但本地存储就需要业务自己用类似 Raft 协议来实现多副本高可用。 接下来看一下 Local PV 场景可能如果没有对PV做“访问位置”的限制会遇到什么问题？ No Visit Restriction 当用户在提交完 PVC 的时候，K8s PV controller可能绑定的是 node2 上面的 PV。但是，真正使用这个 PV 的 pod，在被调度的时候，有可能调度在 node1 上，最终导致这个 pod 在起来的时候没办法去使用这块存储，因为 pod 真实情况是要使用 node2 上面的存储。 第二个(如果不对 PV 做“访问位置”的限制会出问题的)场景： No Visit Restriction 如果搭建的 K8s 集群管理的 nodes 分布在单个区域多个可用区内。在创建动态存储的时候，创建出来的存储属于可用区 2，但之后在提交使用该存储的 pod，它可能会被调度到可用区 1 了，那就可能没办法使用这块存储。因此像阿里云的云盘，也就是块存储，当前不能跨可用区使用，如果创建的存储其实属于可用区 2，但是 pod 运行在可用区 1，就没办法使用这块存储，这是第二个常见的问题场景。 接下来我们来看看 K8s 中如何通过存储拓扑调度来解决上面的问题的。 ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:1:5","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"存储拓扑调度首先总结一下之前的两个问题，它们都是 PV 在给 PVC 绑定或者动态生成 PV 的时候，我并不知道后面将使用它的 pod 将调度在哪些 node 上。但 PV 本身的使用，是对 pod 所在的 node 有拓扑位置的限制的，如 Local PV 场景是我要调度在指定的 node 上我才能使用那块 PV，而对第二个问题场景就是说跨可用区的话，必须要在将使用该 PV 的 pod 调度到同一个可用区的 node 上才能使用阿里云云盘服务，那 K8s 中怎样去解决这个问题呢？ 简单来说，在 K8s 中将 PV 和 PVC 的 binding 操作和动态创建 PV 的操作做了 delay，delay 到 pod 调度结果出来之后，再去做这两个操作。这样的话有什么好处？ 首先，如果要是所要使用的 PV 是预分配的，如 Local PV，其实使用这块 PV 的 pod 它对应的 PVC 其实还没有做绑定，就可以通过调度器在调度的过程中，结合 pod 的计算资源需求(如 cpu/mem) 以及 pod 的 PVC 需求，选择的 node 既要满足计算资源的需求又要 pod 使用的 pvc 要能 binding 的 pv 的 nodeaffinity 限制; 其次对动态生成 PV 的场景其实就相当于是如果知道 pod 运行的 node 之后，就可以根据 node 上记录的拓扑信息来动态的创建这个 PV，也就是保证新创建出来的 PV 的拓扑位置与运行的 node 所在的拓扑位置是一致的，如上面所述的阿里云云盘的例子，既然知道 pod 要运行到可用区 1，那之后创建存储时指定在可用区 1 创建即可。 为了实现上面所说的延迟绑定和延迟创建 PV，需要在 K8s 中的改动涉及到的相关组件有三个： PV Controller 也就是 persistent volume controller，它需要支持延迟 Binding 这个操作。 另一个是动态生成 PV 的组件，如果 pod 调度结果出来之后，它要根据 pod 的拓扑信息来去动态的创建 PV。 第三组件，也是最重要的一个改动点就是 kube-scheduler。在为 pod 选择 node 节点的时候，它不仅要考虑 pod 对 CPU/MEM 的计算资源的需求，它还要考虑这个 pod 对存储的需求，也就是根据它的 PVC，它要先去看一下当前要选择的 node，能否满足能和这个 PVC 能匹配的 PV 的 nodeAffinity；或者是动态生成 PV 的过程，它要根据 StorageClass 中指定的拓扑限制来 check 当前的 node 是不是满足这个拓扑限制，这样就能保证调度器最终选择出来的 node 就能满足存储本身对拓扑的限制。 ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:1:6","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"用例解读","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:2:0","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"Volume Snapshot/Restore示例 # 创建 VolumeSnapshotClass 对象apiVersion:snapshot.storage.k8s.io/v1alpha1kind:VolumeSnapshotClassmetadata:name:disk-snapshotclasssnapshotter:diskplugin.csi.aliyun.com# 指定VolumeSnapshot时使用的Volume Plugin # 创建VolumeSnapshot对象apiVersion:snapshot.storage.k8s.io/v1alpha1kind:VolumeSnapshotmetadata:name:disk-snapshotspec:snapshotClassName:disk-snapshotclasssource:name:disk-pvc # Snapshot的数据源kind:PersistentVolumeClaim # 从snapshot中恢复数据到新生成的PV对象中apiVersion:v1kind:PersistentVolumeClaimmetadata:name:restore-pvcspec:dataSource:name:disk-snapshotkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.io 下面来看一下存储快照如何使用：首先需要集群管理员，在集群中创建 VolumeSnapshotClass 对象，VolumeSnapshotClass 中一个重要字段就是 Snapshot，它是指定真正创建存储快照所使用的卷插件，这个卷插件是需要提前部署的，稍后再说这个卷插件。 接下来用户他如果要做真正的存储快照，需要声明一个 VolumeSnapshotClass，VolumeSnapshotClass 首先它要指定的是 VolumeSnapshotClassName，接着它要指定的一个非常重要的字段就是 source，这个 source 其实就是指定快照的数据源是啥。这个地方指定 name 为 disk-pvc，也就是说通过这个 pvc 对象来创建存储快照。提交这个 VolumeSnapshot 对象之后，集群中的相关组件它会找到这个 PVC 对应的 PV 存储，对这个 PV 存储做一次快照。 有了存储快照之后，那接下来怎么去用存储快照恢复数据呢？这个其实也很简单，通过声明一个新的 PVC 对象并在它的 spec 下面的 DataSource 中来声明我的数据源来自于哪个 VolumeSnapshot，这里指定的是 disk-snapshot 对象，当我这个 PVC 提交之后，集群中的相关组件，它会动态生成新的 PV 存储，这个新的 PV 存储中的数据就来源于这个 Snapshot 之前做的存储快照。 ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:2:1","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"Local PV 示例 # 创建一个no-provisioner StorageClass对象，用的是告诉PV# Controller遇到 .spec.storageClassName 为 local-storage 的# PVC 暂不做Binding操作kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:local-storageprovisioner:kubernetes.io/no-provisionervolumeBindingMode:WaitForFirstConsumer# deley binding apiVersion:v1kind:PersistentVolumeClaimmetadata:name:local-pvcspec:storageClassName:local-storageaccessModes:- ReadWriteOnce # 创建Local PV对象apiVersion:v1kind:PersistentVolumemetadata:name:local-pvspec:storageClassname:local-storagelocal:path:/sharenodeAffinity:# 限制该PV只能在node1上被使用required:nodeSelectorTerms:- matchExpressions:- key:kubernetes.io/hostname# 拓扑域限制： 单node可访问operator:Invalues:- node1 Local PV 大部分使用的时候都是通过静态创建的方式，也就是要先去声明 PV 对象，既然 Local PV 只能是本地访问，就需要在声明 PV 对象的，在 PV 对象中通过 nodeAffinity 来限制我这个 PV 只能在单 node 上访问，也就是给这个 PV 加上拓扑限制。如上图拓扑的 key 用 kubernetes.io/hostname 来做标记，也就是只能在 node1 访问。如果想用这个 PV，你的 pod 必须要调度到 node1 上。 既然是静态创建 PV 的方式，这里为什么还需要 storageClassname 呢？前面也说了，在 Local PV 中，如果要想让它正常工作，需要用到延迟绑定特性才行，那既然是延迟绑定，当用户在写完 PVC 提交之后，即使集群中有相关的 PV 能跟它匹配，它也暂时不能做匹配，也就是说 PV controller 不能马上去做 binding，这个时候你就要通过一种手段来告诉 PV controller，什么情况下是不能立即做 binding。这里的 storageClass 就是为了起到这个副作用，我们可以看到 storageClass 里面的 provisioner 指定的是 no-provisioner，其实就是相当于告诉 K8s 它不会去动态创建 PV，它主要用到 storageclass 的 VolumeBindingMode 字段，叫 WaitForFirstConsumer，可以先简单地认为它是延迟绑定。 当用户开始提交 PVC 的时候，pv controller 在看到这个 pvc 的时候，它会找到相应的 storageClass，发现这个 BindingMode 是延迟绑定，它就不会做任何事情。 之后当真正使用这个 pvc 的 pod，在调度的时候，当它恰好调度在符合 pv nodeaffinity 的 node 的上面后，这个 pod 里面所使用的 PVC 才会真正地与 PV 做绑定，这样就保证我 pod 调度到这台 node 上之后，这个 PVC 才与这个 PV 绑定，最终保证的是创建出来的 pod 能访问这块 Local PV，也就是静态 Provisioning 场景下怎么去满足 PV 的拓扑限制。 ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:2:2","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"限制 Dynamic Provisioning PV 拓扑示例 apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:csi-diskprovisioner:diskplugin.csi.aliyun.comparameters:regionId:cn-hangzhoufsType:ext4type:cloud_ssdvolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:# 拓扑域限制：动态创建的PV只能在可用区cn-hangzhou-d被使用- key:topology.diskplugin.csi.aliyun.com/zonevalues:- cn-hanghzou-dreclaimPolicy:Delete apiVersion:v1kind:PersistentVolumeClaimmetadata:name:disk-pvcspec:accessMode:- ReadWriteOncestorageClassName:csi-disk 动态就是指动态创建 PV 就有拓扑位置的限制，那怎么去指定？ 首先在 storageclass 还是需要指定 BindingMode，就是 WaitForFirstConsumer，就是延迟绑定。 其次特别重要的一个字段就是 allowedTopologies，限制就在这个地方。上图中可以看到拓扑限制是可用区的级别，这里其实有两层意思： 第一层意思就是说我在动态创建 PV 的时候，创建出来的 PV 必须是在这个可用区可以访问的; 第二层含义是因为声明的是延迟绑定，那调度器在发现使用它的 PVC 正好对应的是该 storageclass 的时候，调度 pod 就要选择位于该可用区的 nodes。 总之，就是要从两方面保证，一是动态创建出来的存储时要能被这个可用区访问的，二是我调度器在选择 node 的时候，要落在这个可用区内的，这样的话就保证我的存储和我要使用存储的这个 pod 它所对应的 node，它们之间的拓扑域是在同一个拓扑域，用户在写 PVC 文件的时候，写法是跟以前的写法是一样的，主要是在 storageclass 中要做一些拓扑限制。 ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:2:3","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"处理流程","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:3:0","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"kubernetes 对 Volume Snapshot/Restore 处理流程 Volume Snapshot/Restore Flow 首先来看一下存储快照的处理流程，这里来首先解释一下 csi 部分。K8s 中对存储的扩展功能都是推荐通过 csi out-of-tree 的方式来实现的。 csi 实现存储扩展主要包含两部分： 第一部分是由 K8s 社区推动实现的 csi controller 部分，也就是这里的 csi-snapshottor controller 以及 csi-provisioner controller，这些主要是通用的 controller 部分; 另外一部分是由特定的云存储厂商用自身 OpenAPI 实现的不同的 csi-plugin 部分，也叫存储的 driver 部分。 两部分部件通过 unix domain socket 通信连接到一起。有这两部分，才能形成一个真正的存储扩展功能。 如上图所示，当用户提交 VolumeSnapshot 对象之后，会被 csi-snapshottor controller watch 到。之后它会通过 GPPC 调用到 csi-plugin，csi-plugin 通过 OpenAPI 来真正实现存储快照的动作，等存储快照已经生成之后，会返回到 csi-snapshottor controller 中，csi-snapshottor controller 会将存储快照生成的相关信息放到 VolumeSnapshotContent 对象中并将用户提交的 VolumeSnapshot 做 bound。这个 bound 其实就有点类似 PV 和 PVC 的 bound 一样。 有了存储快照，如何去使用存储快照恢复之前的数据呢？前面也说过，通过声明一个新的 PVC 对象，并且指定他的 dataSource 为 Snapshot 对象，当提交 PVC 的时候会被 csi-provisioner watch 到，之后会通过 GRPC 去创建存储。这里创建存储跟之前讲解的 csi-provisioner 有一个不太一样的地方，就是它里面还指定了 Snapshot 的 ID，当去云厂商创建存储时，需要多做一步操作，即将之前的快照数据恢复到新创建的存储中。之后流程返回到 csi-provisioner，它会将新创建的存储的相关信息写到一个新的 PV 对象中，新的 PV 对象被 PV controller watch 到它会将用户提交的 PVC 与 PV 做一个 bound，之后 pod 就可以通过 PVC 来使用 Restore 出来的数据了。这是 K8s 中对存储快照的处理流程。 ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:3:1","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"kubernetes 对 Volume Topology-aware Scheduling 处理流程 Volume Topolopy-Aware Scheduling Flow 第一个步骤其实就是要去声明延迟绑定，这个通过 StorageClass 来做的，上面已经阐述过，这里就不做详细描述了。 接下来看一下调度器，上图中红色部分就是调度器新加的存储拓扑调度逻辑，我们先来看一下不加红色部分时调度器的为一个 pod 选择 node 时，它的大概流程： 首先用户提交完 pod 之后，会被调度器 watch 到，它就会去首先做预选，预选就是说它会将集群中的所有 node 都来与这个 pod 它需要的资源做匹配； 如果匹配上，就相当于这个 node 可以使用，当然可能不止一个 node 可以使用，最终会选出来一批 node； 然后再经过第二个阶段优选，优选就相当于我要对这些 node 做一个打分的过程，通过打分找到最匹配的一个 node； 之后调度器将调度结果写到 pod 里面的 spec.nodeName 字段里面，然后会被相应的 node 上面的 kubelet watch 到，最后就开始创建 pod 的整个流程。 那现在看一下加上卷相关的调度的时候，筛选 node(第二个步骤)又是怎么做的？ 先就要找到 pod 中使用的所有 PVC，找到已经 bound 的 PVC，以及需要延迟绑定的这些 PVC； 对于已经 bound 的 PVC，要 check 一下它对应的 PV 里面的 nodeAffinity 与当前 node 的拓扑是否匹配 。如果不匹配， 就说明这个 node 不能被调度。如果匹配，继续往下走，就要去看一下需要延迟绑定的 PVC； 对于需要延迟绑定的 PVC。先去获取集群中存量的 PV，满足 PVC 需求的，先把它全部捞出来，然后再将它们一一与当前的 node labels 上的拓扑做匹配，如果它们(存量的 PV)都不匹配，那就说明当前的存量的 PV 不能满足需求，就要进一步去看一下如果要动态创建 PV 当前 node 是否满足拓扑限制，也就是还要进一步去 check StorageClass 中的拓扑限制，如果 StorageClass 中声明的拓扑限制与当前的 node 上面已经有的 labels 里面的拓扑是相匹配的，那其实这个 node 就可以使用，如果不匹配，说明该 node 就不能被调度。 经过这上面步骤之后，就找到了所有即满足 pod 计算资源需求又满足 pod 存储资源需求的所有 nodes。 当 node 选出来之后，第三个步骤就是调度器内部做的一个优化。这里简单过一下，就是更新经过预选和优选之后，pod 的 node 信息，以及 PV 和 PVC 在 scheduler 中做的一些 cache 信息。 第四个步骤也是重要的一步，已经选择出来 node 的 Pod，不管其使用的 PVC 是要 binding 已经存在的 PV，还是要做动态创建 PV，这时就可以开始做。由调度器来触发，调度器它就会去更新 PVC 对象和 PV 对象里面的相关信息，然后去触发 PV controller 去做 binding 操作，或者是由 csi-provisioner 去做动态创建流程。 ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:3:2","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"总结 通过对比 PVC\u0026PV 体系讲解了存储快照的相关 K8s 资源对象以及使用方法； 通过两个实际场景遇到的问题引出存储拓扑调度功能必要性，以及 K8s 中如何通过拓扑调度来解决这些问题； 通过剖析 K8s 中存储快照和存储拓扑调度内部运行机制，深入理解该部分功能的工作原理。 ","date":"2021-07-31","objectID":"/application-storage-and-persistent-data-volume-storage-snapshot/:4:0","tags":["云原生","Kubernetes","存储","存储快照","拓扑调度"],"title":"应用存储和持久化数据卷 - 存储快照与拓扑调度","uri":"/application-storage-and-persistent-data-volume-storage-snapshot/"},{"categories":["云原生"],"content":"本文介绍了 K8s Volume 使用场景、PVC/PV/StorageClass 基本操作和概念解析、PVC+PV 体系的设计与实现原理","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"Volumes 介绍","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:1:0","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"Pod Volumes首先来看一下 Pod Volumes 的使用场景： 场景一：如果 pod 中的某一个容器在运行时异常退出，被 kubelet 重新拉起之后，如何保证之前容器产生的重要数据没有丢失？ 场景二：如果同一个 pod 中的多个容器想要共享数据，应该如何去做？ 以上两个场景，其实都可以借助 Volumes 来很好地解决，接下来首先看一下 Pod Volumes 的常见类型： 本地存储，常用的有 emptydir/hostpath； 网络存储：网络存储当前的实现方式有两种，一种是 in-tree，它的实现的代码是放在 K8s 代码仓库中的，随着k8s对存储类型支持的增多，这种方式会给k8s本身的维护和发展带来很大的负担；而第二种实现方式是 out-of-tree，它的实现其实是给 K8s 本身解耦的，通过抽象接口将不同存储的driver实现从k8s代码仓库中剥离，因此out-of-tree 是后面社区主推的一种实现网络存储插件的方式； Projected Volumes：它其实是将一些配置信息，如 secret/configmap 用卷的形式挂载在容器中，让容器中的程序可以通过POSIX接口来访问配置数据； PV 与 PVC 就是本文要重点介绍的内容。 ","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:1:1","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"Persistent Volumes接下来看一下 PV（Persistent Volumes）。既然已经有了 Pod Volumes，为什么又要引入 PV 呢？我们知道 pod 中声明的 volume 生命周期与 pod 是相同的，以下有几种常见的场景： 场景一：pod 重建销毁，如用 Deployment 管理的 pod，在做镜像升级的过程中，会产生新的 pod并且删除旧的 pod ，那新旧 pod 之间如何复用数据？ 场景二：宿主机宕机的时候，要把上面的 pod 迁移，这个时候 StatefulSet 管理的 pod，其实已经实现了带卷迁移的语义。这时通过 Pod Volumes 显然是做不到的； 场景三：多个 pod 之间，如果想要共享数据，应该如何去声明呢？我们知道，同一个 pod 中多个容器想共享数据，可以借助 Pod Volumes 来解决；当多个 pod 想共享数据时，Pod Volumes 就很难去表达这种语义； 场景四：如果要想对数据卷做一些功能扩展性，如：snapshot、resize 这些功能，又应该如何去做呢？ 以上场景中，通过 Pod Volumes 很难准确地表达它的复用/共享语义，对它的扩展也比较困难。因此 K8s 中又引入了 Persistent Volumes 概念，它可以将存储和计算分离，通过不同的组件来管理存储资源和计算资源，然后解耦 pod 和 Volume 之间生命周期的关联。这样，当把 pod 删除之后，它使用的PV仍然存在，还可以被新建的 pod 复用。 ","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:1:2","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"PVC设计意图 职责分离，PVC中只用声明自己需要的存储size、access node等业务真正关心的存储需求，PV和其对应的后端存储信息则由交给cluster admin统一运维和管控，安全访问策略更容易控制。 PVC简化了User对存储的需求，PV才是存储的实际信息的承载体，通过kube-controller-manager中的PersisentVolumeController将PVC与合适的PV bound到一起，从而满足User对存储的实际需求。 PVC像是面向对象编程中抽象出来的接口，PV是接口对应的实现。 access node是什么？其实就是：我要使用的存储是可以被多个node共享还是只能单node独占访问(注意是node level而不是pod level)？只读还是读写访问？用户只用关心这些东西，与存储相关的实现细节是不需要关心的。 ","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:1:3","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"Static Volume Provisioning静态 Provisioning：由集群管理员事先去规划这个集群中的用户会怎样使用存储，它会先预分配一些存储，也就是预先创建一些 PV；然后用户在提交自己的存储需求（也就是 PVC）的时候，K8s 内部相关组件会帮助它把 PVC 和 PV 做绑定；之后用户再通过 pod 去使用存储的时候，就可以通过 PVC 找到相应的 PV，它就可以使用了。 Static Volume Provisioning 静态产生方式有什么不足呢？可以看到，首先需要集群管理员预分配，预分配其实是很难预测用户真实需求的。举一个最简单的例子：如果用户需要的是 20G，然而集群管理员在分配的时候可能有 80G 、100G 的，但没有 20G 的，这样就很难满足用户的真实需求，也会造成资源浪费。有没有更好的方式呢？ ","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:1:4","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"Dynamic Volume Provisioning动态供给是什么意思呢？就是说现在集群管理员不预分配 PV，他写了一个模板文件，这个模板文件是用来表示创建某一类型存储（块存储，文件存储等）所需的一些参数，这些参数是用户不关心的，给存储本身实现有关的参数。用户只需要提交自身的存储需求，也就是PVC文件，并在 PVC 中指定使用的存储模板（StorageClass）。 Dynamic Volume Provisioning K8s 集群中的管控组件，会结合 PVC 和 StorageClass 的信息动态，生成用户所需要的存储（PV），将 PVC 和 PV 进行绑定后，pod 就可以使用 PV 了。通过 StorageClass 配置生成存储所需要的存储模板，再结合用户的需求动态创建 PV 对象，做到按需分配，在没有增加用户使用难度的同时也解放了集群管理员的运维工作。 ","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:1:5","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"用例解读接下来看一下 Pod Volumes、PV、PVC 及 StorageClass 具体是如何使用的。 apiVersion:v1kind:Podmetadata:name:test-podspec:containers:- name:container-1Image:ubuntu:18.04volumeMounts:- name:cache-volumemountPath:/cache # 容器内挂载路径subPath:cache1 # 在cache-volume建立子目录cache1- name:hostpath-volumemountPath:/datareadOnly:true# 只读挂载- name:container-2image:ubuntu:18.04volumeMounts:- mountPath:/cachename:cache-volumesubPath:cache2volumes:- name:cache-volume# 宿主机上路径：/var/lib/kubelet/pods/\u003cPodUID\u003e/volumes/kubernetes.io-empty-dir/cache-volumeemptyDir:{}- name:hostpath-volumehostPath:path:/tmp/data # 宿主上路径，pod删除之后该目录仍然存在 声明的两个卷，一个是用的是 emptyDir，另外一个用的是 hostPath，这两种都是本地卷。在容器中应该怎么去使用这个卷呢？它其实可以通过 volumeMounts 这个字段，volumeMounts 字段里面指定的 name 其实就是它使用的哪个卷，mountPath 就是容器中的挂载路径。 这里还有个 subPath，subPath 是什么？ 先看一下，这两个容器都指定使用了同一个卷，就是这个 cache-volume。那么，在多个容器共享同一个卷的时候，为了隔离数据，我们可以通过 subPath 来完成这个操作。它会在卷里面建立两个子目录，然后容器 1 往 cache 下面写的数据其实都写在子目录 cache1 了，容器 2 往 cache 写的目录，其数据最终会落在这个卷里子目录下面的 cache2 下。 还有一个 readOnly 字段，readOnly 的意思其实就是只读挂载，这个挂载你往挂载点下面实际上是没有办法去写数据的。 另外emptyDir、hostPath 都是本地存储，它们之间有什么细微的差别呢？emptyDir 其实是在 pod 创建的过程中会临时创建的一个目录，这个目录随着 pod 删除也会被删除，里面的数据会被清空掉；hostPath 顾名思义，其实就是宿主机上的一个路径，在 pod 删除之后，这个目录还是存在的，它的数据也不会被丢失。这就是它们两者之间一个细微的差别。 ","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:2:0","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"PV Spec 重要字段解析 Capacity：这个很好理解，就是存储对象的大小； AccessModes：也是用户需要关心的，就是说我使用这个 PV 的方式。它有三种使用方式。 一种是单 node 读写访问； 第二种是多个 node 只读访问，是常见的一种数据的共享方式； 第三种是多个 node 上读写访问。 用户在提交 PVC 的时候，最重要的两个字段 —— Capacity 和 AccessModes。在提交 PVC 后，k8s 集群中的相关组件是如何去找到合适的 PV 呢？首先它是通过为 PV 建立的 AccessModes 索引找到所有能够满足用户的 PVC 里面的 AccessModes 要求的 PV list，然后根据PVC的 Capacity，StorageClassName, Label Selector 进一步筛选 PV，如果满足条件的 PV 有多个，选择 PV 的 size 最小的，accessmodes 列表最短的 PV，也即最小适合原则。 ReclaimPolicy：这个就是刚才提到的，我的用户方 PV 的 PVC 在删除之后，我的 PV 应该做如何处理？常见的有三种方式。 第一种方式我们就不说了，现在 K8s 中已经不推荐使用了； 第二种方式 delete，也就是说 PVC 被删除之后，PV 也会被删除； 第三种方式 Retain，就是保留，保留之后，后面这个 PV 需要管理员来手动处理。 StorageClassName：StorageClassName 这个我们刚才说了，我们动态 Provisioning 时必须指定的一个字段，就是说我们要指定到底用哪一个模板文件来生成 PV ； NodeAffinity：就是说我创建出来的 PV，它能被哪些 node 去挂载使用，其实是有限制的。然后通过 NodeAffinity 来声明对node的限制，这样其实对 使用该PV的pod调度也有限制，就是说 pod 必须要调度到这些能访问 PV 的 node 上，才能使用这块 PV，这个字段在我们下一讲讲解存储拓扑调度时在细说。 ","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:2:1","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"PV状态流转 PV State Flow 首先在创建 PV 对象后，它会处在短暂的pending 状态；等真正的 PV 创建好之后，它就处在 available 状态。 available 状态意思就是可以使用的状态，用户在提交 PVC 之后，被 K8s 相关组件做完 bound（即：找到相应的 PV），这个时候 PV 和 PVC 就结合到一起了，此时两者都处在 bound 状态。当用户在使用完 PVC，将其删除后，这个 PV 就处在 released 状态，之后它应该被删除还是被保留呢？这个就会依赖我们刚才说的 ReclaimPolicy。 这里有一个点需要特别说明一下：当 PV 已经处在 released 状态下，它是没有办法直接回到 available 状态，也就是说接下来无法被一个新的 PVC 去做绑定。如果我们想把已经 released 的 PV 复用，我们这个时候通常应该怎么去做呢？ 第一种方式：我们可以新建一个 PV 对象，然后把之前的 released 的 PV 的相关字段的信息填到新的 PV 对象里面，这样的话，这个 PV 就可以结合新的 PVC 了；第二种是我们在删除 pod 之后，不要去删除 PVC 对象，这样给 PV 绑定的 PVC 还是存在的，下次 pod 使用的时候，就可以直接通过 PVC 去复用。K8s中的 StatefulSet 管理的 Pod 带存储的迁移就是通过这种方式。 ","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:2:2","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"架构设计","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:3:0","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"PV和PVC的处理流程 PV/PVC Handle Flow csi 是什么？csi 的全称是 container storage interface，它是K8s社区后面对存储插件实现(out of tree)的官方推荐方式。csi 的实现大体可以分为两部分： 第一部分是由k8s社区驱动实现的通用的部分，像我们这张图中的 csi-provisioner和 csi-attacher controller； 另外一种是由云存储厂商实践的，对接云存储厂商的 OpenApi，主要是实现真正的 create/delete/mount/unmount 存储的相关操作，对应到上图中的csi-controller-server和csi-node-server。 接下来看一下，当用户提交 yaml 之后，k8s内部的处理流程。用户在提交 PVCyaml 的时候，首先会在集群中生成一个 PVC 对象，然后 PVC 对象会被 csi-provisioner controller watch到，csi-provisioner 会结合 PVC 对象以及 PVC 对象中声明的 storageClass，通过 GRPC 调用 csi-controller-server，然后，到云存储服务这边去创建真正的存储，并最终创建出来 PV 对象。最后，由集群中的 PV controller 将 PVC 和 PV 对象做 bound 之后，这个 PV 就可以被使用了。 用户在提交 pod 之后，首先会被调度器调度选中某一个合适的node，之后该 node 上面的 kubelet 在创建 pod 流程中会通过首先 csi-node-server 将我们之前创建的 PV 挂载到我们 pod 可以使用的路径，然后 kubelet 开始 create \u0026\u0026 start pod 中的所有 container。 ","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:3:1","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"PV、PVC以及通过csi使用存储流程 PV/PVC Usage Flow 主要分为三个阶段： 第一个阶段(Create阶段)是用户提交完 PVC，由 csi-provisioner 创建存储，并生成 PV 对象，之后 PV controller 将 PVC 及生成的 PV 对象做 bound，bound 之后，create 阶段就完成了； 之后用户在提交 pod yaml 的时候，首先会被调度选中某一个 合适的node，等 pod 的运行 node 被选出来之后，会被 AD Controller watch 到 pod 选中的 node，它会去查找 pod 中使用了哪些 PV。然后它会生成一个内部的对象叫 VolumeAttachment 对象，从而去触发 csi-attacher去调用csi-controller-server 去做真正的 attache 操作，attach操作调到云存储厂商OpenAPI。这个 attach 操作就是将存储 attach到 pod 将会运行的 node 上面。第二个阶段 —— attach阶段完成； 然后我们接下来看第三个阶段。第三个阶段 发生在kubelet 创建 pod的过程中，它在创建 pod 的过程中，首先要去做一个 mount，这里的 mount 操作是为了将已经attach到这个 node 上面那块盘，进一步 mount 到 pod 可以使用的一个具体路径，之后 kubelet 才开始创建并启动容器。这就是 PV 加 PVC 创建存储以及使用存储的第三个阶段 —— mount 阶段。 总的来说，有三个阶段：第一个 create 阶段，主要是创建存储；第二个 attach 阶段，就是将那块存储挂载到 node 上面(通常为将存储load到node的/dev下面)；第三个 mount 阶段，将对应的存储进一步挂载到 pod 可以使用的路径。这就是我们的 PVC、PV、已经通过CSI实现的卷从创建到使用的完整流程。 ","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:3:2","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"总结 介绍了 K8s Volume 的使用场景，以及本身局限性； 通过介绍 K8s 的 PVC 和 PV 体系，说明 K8s 通过 PVC 和 PV 体系增强了 K8s Volumes 在多 Pod 共享/迁移/存储扩展等场景下的能力的必要性以及设计思想； 通过介绍 PV（存储）的不同供给模式 (static and dynamic)，学习了如何通过不同方式为集群中的 Pod 供给所需的存储； 通过 PVC\u0026PV 在 K8s 中完整的处理流程，深入理解 PVC\u0026PV 的工作原理。 ","date":"2021-07-25","objectID":"/application-storage-and-persistent-data-volume-core-knowledge/:4:0","tags":["云原生","Kubernetes","存储","PV","PVC"],"title":"应用存储和持久化数据卷 - 核心知识","uri":"/application-storage-and-persistent-data-volume-core-knowledge/"},{"categories":["云原生"],"content":"本文系统介绍了 ConfigMap、Secret、Pod 身份认证、容器资源和安全、InitContainer","date":"2021-07-17","objectID":"/application-configuration-management/","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"需求来源","date":"2021-07-17","objectID":"/application-configuration-management/:1:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"背景问题首先一起来看一下需求来源。大家应该都有过这样的经验，用一个容器镜像来启动一个 container。要启动这个容器，其实有很多需要配套的问题待解决： 第一，比如说一些可变的配置。因为我们不可能把一些可变的配置写到镜像里面，当这个配置需要变化的时候，可能需要我们重新编译一次镜像，这个肯定是不能接受的； 第二就是一些敏感信息的存储和使用。比如说应用需要使用一些密码，或者用一些 token； 第三就是我们容器要访问集群自身。比如我要访问 kube-apiserver，那么本身就有一个身份认证的问题； 第四就是容器在节点上运行之后，它的资源需求； 第五个就是容器在节点上，它们是共享内核的，那么它的一个安全管控怎么办？ 最后一点，容器启动之前的一个前置条件检验。比如说，一个容器启动之前，可能要确认一下 DNS 服务是不是好用？又或者确认一下网络是不是联通的？那么这些其实就是一些前置的校验。 ","date":"2021-07-17","objectID":"/application-configuration-management/:1:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Pod 的配置管理在 Kubernetes 里面，它是怎么做这些配置管理的呢？如下表所示： 配置 说明 ConfigMap 可变配置 Secret 敏感信息 ServiceAccount 身份认证 Spec.Containers[].Resources.limits/requests 资源配置 Spec.Containers[].Resources.SecrityContext 安全管控 Spec.InitContainers 前置校验 ","date":"2021-07-17","objectID":"/application-configuration-management/:1:2","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ConfigMap","date":"2021-07-17","objectID":"/application-configuration-management/:2:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ConfigMap 介绍ConfigMap 它是用来做什么的、以及它带来的一个好处。它其实主要是管理一些可变配置信息，比如说我们应用的一些配置文件，或者说它里面的一些环境变量，或者一些命令行参数。 它的好处在于它可以让一些可变配置和容器镜像进行解耦，这样也保证了容器的可移植性。 apiVersion:v1kind:ConfigMapmetadata:lables:app:flanneltier:nodename:kube-flannnel-cfgnamespace:kube-systemdata:cni-conf.json:|{ \"name\": \"cbr0\", \"type\": \"\"flannel\", \"delegate\": { \"isDefaultGateway\": true } }net-conf.json:|{ \"Network\": \"172.27.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } } 这是 ConfigMap 本身的一个定义，它包括两个部分：一个是 ConfigMap 元信息，我们关注 name 和 namespace 这两个信息。接下来这个 data 里面，可以看到它管理了两个配置文件。它的结构其实是这样的：从名字看ConfigMap中包含Map单词，Map 其实就是 key:value，key 是一个文件名，value 是这个文件的内容。 ","date":"2021-07-17","objectID":"/application-configuration-management/:2:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ConfigMap 创建推荐用 kubectl 这个命令来创建，它带的参数主要有两个：一个是指定 name，第二个是 DATA。其中 DATA 可以通过指定文件或者指定目录，以及直接指定键值对。 kubectl create configmap [NAME] [DATA] 指定文件的话，文件名就是 Map 中的 key，文件内容就是 Map 中的 value。然后指定键值对就是指定数据键值对，即：key:value 形式，直接映射到 Map 的key:value。 ","date":"2021-07-17","objectID":"/application-configuration-management/:2:2","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ConfigMap 使用创建完了之后，应该怎么使用呢？ 第一种是环境变量。环境变量的话通过 valueFrom，然后 ConfigMapKeyRef 这个字段，下面的 name 是指定 ConfigMap 名，key 是 ConfigMap.data 里面的 key。这样的话，在 busybox 容器启动后容器中执行 env 将看到一个 SPECIAL_LEVEL_KEY 环境变量； 第二个是命令行参数。命令行参数其实是第一行的环境变量直接拿到 cmd 这个字段里面来用； 最后一个是通过 volume 挂载的方式直接挂到容器的某一个目录下面去。上面的例子是把 special-config 这个 ConfigMap 里面的内容挂到容器里面的 /etc/config 目录下，这个也是使用的一种方式。 ","date":"2021-07-17","objectID":"/application-configuration-management/:2:3","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ConfigMap 注意要点现在对 ConfigMap 的使用做一个总结，以及它的一些注意点，注意点一共列了以下五条： ConfigMap 文件的大小。虽然说 ConfigMap 文件没有大小限制，但是在 ETCD 里面，数据的写入是有大小限制的，现在是限制在 1MB 以内； 第二个注意点是 pod 引入 ConfigMap 的时候，必须是相同的 Namespace 中的 ConfigMap，前面其实可以看到，ConfigMap.metadata 里面是有 namespace 字段的； 第三个是 pod 引用的 ConfigMap。假如这个 ConfigMap 不存在，那么这个 pod 是无法创建成功的，其实这也表示在创建 pod 前，必须先把要引用的 ConfigMap 创建好； 第四点就是使用 envFrom 的方式。把 ConfigMap 里面所有的信息导入成环境变量时，如果 ConfigMap 里有些 key 是无效的，比如 key 的名字里面带有数字，那么这个环境变量其实是不会注入容器的，它会被忽略。但是这个 pod 本身是可以创建的。这个和第三点是不一样的方式，是 ConfigMap 文件存在基础上，整体导入成环境变量的一种形式； 最后一点是：什么样的 pod 才能使用 ConfigMap？这里只有通过 K8s api 创建的 pod 才能使用 ConfigMap，比如说通过用命令行 kubectl 来创建的 pod，肯定是可以使用 ConfigMap 的，但其他方式创建的 pod，比如说 kubelet 通过 manifest 创建的 static pod，它是不能使用 ConfigMap 的。 ","date":"2021-07-17","objectID":"/application-configuration-management/:2:4","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Secret","date":"2021-07-17","objectID":"/application-configuration-management/:3:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Secret 介绍Secret 是一个主要用来存储密码 token 等一些敏感信息的资源对象。其中，敏感信息是采用 base-64 编码保存起来的。 Secret 类型种类比较多，下面列了常用的四种类型： 第一种是 Opaque，它是普通的 Secret 文件； 第二种是 service-account-token，是用于 service-account 身份认证用的 Secret； 第三种是 dockerconfigjson，这是拉取私有仓库镜像的用的一种 Secret； 第四种是 bootstrap.token，是用于节点接入集群校验用的 Secret。 ","date":"2021-07-17","objectID":"/application-configuration-management/:3:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Secret 创建Secret 有两种创建方式： 系统创建：比如 K8s 为每一个 namespace 的默认用户（default ServiceAccount）创建 Secret； 用户手动创建：手动创建命令，推荐 kubectl 这个命令行工具，它相对 ConfigMap 会多一个 type 参数。其中 data 也是一样，它也是可以指定文件和键值对的。type 的话，要是你不指定的话，默认是 Opaque 类型。 ","date":"2021-07-17","objectID":"/application-configuration-management/:3:2","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Secret 使用创建完 Secret 之后，再来看一下如何使用它。它主要是被 pod 来使用，一般是通过 volume 形式挂载到容器里指定的目录，然后容器里的业务进程再到目录下读取 Secret 来进行使用。另外在需要访问私有镜像仓库时，也是通过引用 Secret 来实现。 挂载到用户指定目录的方式： 第一种方式：用户直接指定 第二种方式：系统自动生成，过程中会生成两个文件，一个是 ca.crt，一个是 token。 ","date":"2021-07-17","objectID":"/application-configuration-management/:3:3","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Secret 使用注意要点 Secret 的文件大小限制。这个跟 ConfigMap 一样，也是 1MB。 Secret 采用了 base-64 编码，但是它跟明文也没有太大区别。所以说，如果有一些机密信息要用 Secret 来存储的话，还是要很慎重考虑。也就是说谁会来访问你这个集群，谁会来用你这个 Secret，还是要慎重考虑，因为它如果能够访问这个集群，就能拿到这个 Secret。如果是对 Secret 敏感信息要求很高，对加密这块有很强的需求，推荐可以使用 Kubernetes 和开源的 vault做一个解决方案，来解决敏感信息的加密和权限管理。 Secret 读取的最佳实践，建议不要用 list/watch，如果用 list/watch 操作的话，会把 namespace 下的所有 Secret 全部拉取下来，这样其实暴露了更多的信息。推荐使用 GET 的方法，这样只获取你自己需要的那个 Secret。 ","date":"2021-07-17","objectID":"/application-configuration-management/:3:4","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ServiceAccount","date":"2021-07-17","objectID":"/application-configuration-management/:4:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ServiceAccount 介绍ServiceAccount 首先是用于解决 pod 在集群里面的身份认证问题，身份认证信息是存在于 Secret 里面。 ","date":"2021-07-17","objectID":"/application-configuration-management/:4:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Resource","date":"2021-07-17","objectID":"/application-configuration-management/:5:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"容器资源配合管理目前内部支持类型有三种：CPU、内存以及临时存储。如果用户觉得这三种不够，可以自己定义所需的资源，如 GPU。配置资源时，资源指定数量必须为整数。目前资源配置主要分成 request 和 limit 两种类型，一个是需要的数量，一个是资源的界限。CPU、内存以及临时存储都是在 container 下的 Resource 字段里进行一个声明。 ","date":"2021-07-17","objectID":"/application-configuration-management/:5:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Pod 服务质量 (QoS) 配置根据 CPU 对容器内存资源的需求，我们对 pod 的服务质量进行一个分类，分别是 Guaranteed、Burstable 和 BestEffort。 Guaranteed：pod 里面每个容器都必须有内存和 CPU 的 request 以及 limit 的一个声明，且 request 和 limit 必须是一样的，这就是 Guaranteed； Burstable：Burstable 至少有一个容器存在内存和 CPU 的一个 request； BestEffort：只要不是 Guaranteed 和 Burstable，那就是 BestEffort。 服务质量是什么样的呢？资源配置好后，当这个节点上 pod 容器运行，比如说节点上 memory 配额资源不足，kubelet会把一些低优先级的，或者说服务质量要求不高的（如：BestEffort、Burstable）pod 驱逐掉。它们是按照先去除 BestEffort，再去除 Burstable 的一个顺序来驱逐 pod 的。 ","date":"2021-07-17","objectID":"/application-configuration-management/:5:2","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"SecurityContext","date":"2021-07-17","objectID":"/application-configuration-management/:6:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"SecurityContext 介绍SecurityContext 主要是用于限制容器的一个行为，它能保证系统和其他容器的安全。这一块的能力不是 Kubernetes 或者容器 runtime 本身的能力，而是 Kubernetes 和 runtime 通过用户的配置，最后下传到内核里，再通过内核的机制让 SecurityContext 来生效。 SecurityContext 主要分为三个级别： 第一个是容器级别，仅对容器生效； 第二个是 pod 级别，对 pod 里所有容器生效； 第三个是集群级别，就是 PSP，对集群内所有 pod 生效。 权限和访问控制设置项，现在一共列有七项（这个数量后续可能会变化）： 第一个就是通过用户 ID 和组 ID 来控制文件访问权限； 第二个是 SELinux，它是通过策略配置来控制用户或者进程对文件的访问控制； 第三个是特权容器； 第四个是 Capabilities，它也是给特定进程来配置一个 privileged 能力； 第五个是 AppArmor，它也是通过一些配置文件来控制可执行文件的一个访问控制权限，比如说一些端口的读写； 第六个是一个对系统调用的控制； 第七个是对子进程能否获取比父亲更多的权限的一个限制。 ","date":"2021-07-17","objectID":"/application-configuration-management/:6:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"InitContainer","date":"2021-07-17","objectID":"/application-configuration-management/:7:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"InitContainer 介绍首先介绍 InitContainer 和普通 container 的区别，有以下三点内容： InitContainer 首先会比普通 container 先启动，并且直到所有的 InitContainer 执行成功后，普通 container 才会被启动 InitContainer 之间是按定义的次序去启动执行的，执行成功一个之后再执行第二个，而普通的 container 是并发启动的； InitContainer 执行成功后就结束退出，而普通容器可能会一直在执行。它可能是一个 longtime 的，或者说失败了会重启，这个也是 InitContainer 和普通 container 不同的地方 InitContainer 其实主要为普通 container 服务，比如说它可以为普通 container 启动之前做一个初始化，或者为它准备一些配置文件， 配置文件可能是一些变化的东西。再比如做一些前置条件的校验，如网络是否联通。 ","date":"2021-07-17","objectID":"/application-configuration-management/:7:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"结束语 ConfigMap 和 Secret: 首先介绍了 ConfigMap 和 Secret 的创建方法和使用场景，然后对 ConfigMap 和 Secret 的常见使用注意点进行了分类和整理。最后介绍了私有仓库镜像的使用和配置； Pod 身份认证: 首先介绍了 ServiceAccount 和 Secret 的关联关系，然后从源码角度对 Pod 身份认证流程和实现细节进行剖析，同时引出了 Pod 的权限管理(即 RBAC 的配置管理)； 容器资源和安全： 首先介绍了容器常见资源类型 (CPU/Memory) 的配置，然后对 Pod 服务质量分类进行详细的介绍。同时对 SecurityContext 有效层级和权限配置项进行简要说明； InitContainer: 首先介绍了 InitContainer 和普通 container 的区别以及 InitContainer 的用途。然后基于实际用例对 InitContainer 的用途进行了说明。 ","date":"2021-07-17","objectID":"/application-configuration-management/:8:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"通过类比 Deployment 控制器，我们理解了一下DaemonSet 控制器的工作流程与方式，并且通过对 DaemonSet 的更新了解了滚动更新的概念和相对应的操作方式。","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"需求来源如果我们没有 DaemonSet 会怎么样？下面有几个需求： 首先如果希望每个节点都运行同样一个 pod 怎么办？ 如果新节点加入集群的时候，想要立刻感知到它，然后去部署一个 pod，帮助我们初始化一些东西，这个需求如何做？ 如果有节点退出的时候，希望对应的 pod 会被删除掉，应该怎么操作？ 如果 pod 状态异常的时候，我们需要及时地监控这个节点异常，然后做一些监控或者汇报的一些动作，那么这些东西运用什么控制器来做？ ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:1:0","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"DaemonSet：守护进程控制器DaemonSet 也是 Kubernetes 提供的一个 default controller，它实际是做一个守护进程的控制器，它能帮我们做到以下几件事情： 首先能保证集群内的每一个节点都运行一组相同的 pod； 同时还能根据节点的状态保证新加入的节点自动创建对应的 pod； 在移除节点的时候，能删除对应的 pod； 而且它会跟踪每个 pod 的状态，当这个 pod 出现异常、Crash 掉了，会及时地去 recovery 这个状态。 ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:2:0","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"用例解读","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:3:0","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"DaemonSet 语法 apiVersion:apps/v1kind:DaemonSetmetadata:name:fluentd-elasticsearchnamespace:kube-systemlables:k8s-app:fluentd-loggingspec:selector:matchLabels:name:fluentd-elasticsearchtemplate:metadata:labels:name:fluentd-elasticsearchspec:containers:- name:fluentd-elasticsearchimage:fluent/fluentd:v1.4-1 首先是 kind:DaemonSet。它会有 matchLabel，通过 matchLabel 去管理对应所属的 pod，这个 pod.label 也要和这个 DaemonSet.controller.label 想匹配，它才能去根据 label.selector 去找到对应的管理 Pod。下面 spec.container 里面的东西都是一致的。 DaemonSet 适用场景： 集群存储进程： GlusterFS 或者 Ceph 之类的东西，需要每台节点上都运行一个类似于 Agent 的东西，DaemonSet 就能很好地满足这个诉求； 日志收集进程：logstash 或者 fluentd，这些都是同样的需求，需要每台节点都运行一个 Agent，这样的话，我们可以很容易搜集到它的状态，把各个节点里面的信息及时地汇报到上面； 需要在每个节点运行的监控收集器，例如 Promethues。 ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:3:1","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"查看 DaemonSet 状态创建完 DaemonSet 之后，我们可以使用 kubectl get DaemonSet（DaemonSet 缩写为 ds）。 kubectl get ds NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluentd-elasticsearch 4 4 4 4 4 \u003cnone\u003e 4s 状态描述： DESIRED：需要的 pod 个数 CURRENT：当前已存在的 pod 个数 READY：就绪的个数 UP-TO-DATE：最新创建的个数 AVAILABLE：可用 pod 个数 NODE SELECTOR：节点选择标签，可用于选择部分节点去运行 pod，而不是全部节点运行 pod ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:3:2","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"更新 DaemonSet其实 DaemonSet 和 deployment 特别像，它也有两种更新策略：一个是 RollingUpdate，另一个是 OnDelete。 RollingUpdate 其实比较好理解，就是会一个一个的更新。先更新第一个 pod，然后老的 pod 被移除，通过健康检查之后再去见第二个 pod，这样对于业务上来说会比较平滑地升级，不会中断； OnDelete 其实也是一个很好的更新策略，就是模板更新之后，pod 不会有任何变化，需要我们手动控制。我们去删除某一个节点对应的 pod，它就会重建，不删除的话它就不会重建，这样的话对于一些我们需要手动控制的特殊需求也会有特别好的作用。 ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:3:3","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"架构设计","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:4:0","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"DaemonSet 管理模式 DaemonSet Architecture DaemonSet Controller 负责根据配置创建 Pod DaemonSet Controller 跟踪 Job 状态，根据配置及时重试 Pod 或者继续创建 DaemonSet Controller 会自动添加 affinity\u0026label 来跟踪对应的 pod，并根据配置在每个节点或者适应的部分节点创建 Pod ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:4:1","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"DaemonSet 控制器 DaemonSet Controller DaemonSet 其实和 Job controller 做的差不多：两者都需要根据 watch 这个 API Server 的状态。现在 DaemonSet 和 Job controller 唯一的不同点在于，DaemonsetSet Controller需要去 watch node 的状态，但其实这个 node 的状态还是通过 API Server 传递到 ETCD 上。 当有 node 状态节点发生变化时，它会通过一个内存消息队列发进来，然后DaemonSet controller 会去 watch 这个状态，看一下各个节点上是都有对应的 Pod，如果没有的话就去创建。当然它会去做一个对比，如果有的话，它会比较一下版本，然后加上刚才提到的是否去做 RollingUpdate？如果没有的话就会重新创建，Ondelete 删除 pod 的时候也会去做 check 它做一遍检查，是否去更新，或者去创建对应的 pod。 当然最后的时候，如果全部更新完了之后，它会把整个 DaemonSet 的状态去更新到 API Server 上，完成最后全部的更新。 ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:4:2","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"本节总结DaemonSet 基础操作与概念解析：通过类比 Deployment 控制器，我们理解了一下DaemonSet 控制器的工作流程与方式，并且通过对 DaemonSet 的更新了解了滚动更新的概念和相对应的操作方式。 ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:5:0","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"如何管理 Pod 中的进程？一切秘密都在这里","date":"2021-07-04","objectID":"/application-orchestration-and-management-job/","tags":["云原生","Kubernetes","应用编排","Job","CronJob"],"title":"应用编排与管理：Job","uri":"/application-orchestration-and-management-job/"},{"categories":["云原生"],"content":"Job","date":"2021-07-04","objectID":"/application-orchestration-and-management-job/:1:0","tags":["云原生","Kubernetes","应用编排","Job","CronJob"],"title":"应用编排与管理：Job","uri":"/application-orchestration-and-management-job/"},{"categories":["云原生"],"content":"需求来源Job 背景问题 在 K8s 里，最小的调度单元是 Pod，可以直接通过 Pod 来运行任务进程。这样做会产生以下几种问题： 如何保证 Pod 内进程正确的结束？ 如何保证进程运行失败后重试？ 如何管理多个任务，且任务之间有依赖关系？ 如何并行地运行任务，并管理任务的队列大小？ Job：管理任务的控制器 Job 提供了以下功能： 首先 K8s 的 Job 是一个管理任务的控制器，可以创建一个或多个 Pod 来指定 Pod 的数量，并可以监控它是否成功地运行或终止； 可以根据 Pod 的状态来给 Job 设置重置的方式及重试的次数； 可以根据依赖关系，保证上一个任务运行完成之后再运行下一个任务； 可以控制任务的并行度，根据并行度来确保 Pod 运行过程中的并行次数和总体完成大小。 ","date":"2021-07-04","objectID":"/application-orchestration-and-management-job/:1:1","tags":["云原生","Kubernetes","应用编排","Job","CronJob"],"title":"应用编排与管理：Job","uri":"/application-orchestration-and-management-job/"},{"categories":["云原生"],"content":"用例解读Job 语法 apiVersion:batch/v1kind:Jobmetadata:name:pispec:template:spec:containers:- name:piimage:perlcommand:[\"perl\",\"-Mbignum=bpi\",\"-wle\",\"print bpi(2000)\"]restartPolicy:NeverbackoffLimit:4 restartPolicy：可以设置 Never、OnFailure、Always 这三种重试策略。在希望 Job 需要重新运行的时候，可以用 Never；希望在失败的时候再运行，再重试可以用 OnFailure；或者不论什么情况下都重新运行时 Always； backoffLimit：控制 Job 重试的次数。 并行运行 Job apiVersion:batch/v1kind:Jobmetadata:name:paral-1spec:completions:8parallelism:2template:spec:containers:- name:paralimage:ubuntucommand:[\"/bin/sh\"]args:[\"-c\",\"sleep 30; date\"]restartPolicy:OnFailure 有时会有并行运行的需求：希望 Job 运行的时候可以最大化的并行，并行出 n 个 Pod 去快速地执行。同时，也不希望同时并行的 Pod 数过多。 completions：指定本 Pod 队列执行次数，或者理解为指定可以运行的总次数； parallelism：并行执行的个数。 CronJob 语法 apiVersion:batch/v1beta1kind:CronJobmetadata:name:hellospec:schedule:\"*/1 * * * *\"jobTemplate:spec:template:spec:containers:- name:helloimage:busyboxargs:- /bin/sh- -c- date; echo Hello from the Kubernetes clusterrestartPolicy:OnFailurestartingDeadlineSeconds:10concurrencyPolicy:AllowsuccessfulJobsHistoryLimit:3 CronJob，也叫做定时运行 Job，可以设定一个事件去执行； schedule：设置时间格式，和 Linux 的 crontime 是一样的，直接根据 Linux 的 crontime 书写格式来写就可以了 startDeadlineSeconds：每次运行 Job 的时候，最长可以等多长时间，有时 Job 可能运行很长时间也不会启动。如果超过较长时间的话，CronJob 就会停止这个 Job； concurrencyPolicy：是否允许并行运行，如果设置允许，则不管前面的 Job 是否运行完成，后面的 Job 每分钟都会执行，如果是 false，则等待上个结束再运行下一个； JobsHistoryLimit：留存历史版本数量。 ","date":"2021-07-04","objectID":"/application-orchestration-and-management-job/:1:2","tags":["云原生","Kubernetes","应用编排","Job","CronJob"],"title":"应用编排与管理：Job","uri":"/application-orchestration-and-management-job/"},{"categories":["云原生"],"content":"架构设计 Job Architecture Job Controller 创建相对应的 pod； Job Controller 跟踪 Job 状态，根据配置及时重试 Pod 或者继续创建； Job Controller 会自动添加 label 来跟踪对应的 pod，并根据配置并行或串行创建 Pod。 Job 控制器 Job Controller 所有 Job 都是一个 controller，它会 watch 这个 API Server，每次提交一个 Job 的 yaml 都会经过 api-server 传到 etcd 里面去，然后 Job Controller 会注册几个 Handler，每当有添加、更新、删除等操作的时候，会通过一个内存级的消息队列，发到 controller 里面。 通过 Job Controller 检查当前是否有运行的 Pod，如果没有的话，通过 Scale up 把这个 Pod 创建出来；如果有的话，或者大于这个数，对它进行 Scale down，如果这时 Pod 发生变化，需要及时更新状态。 同时要去检查它是否是并行的 job，或者是串行的 job，根据设置的配置并行度、串行度，及时地把 pod 的数量给创建出来。最后，它会把 job 的整个的状态更新到 API Server 里面去，这样就能看到呈现出来的最终效果了。 ","date":"2021-07-04","objectID":"/application-orchestration-and-management-job/:1:3","tags":["云原生","Kubernetes","应用编排","Job","CronJob"],"title":"应用编排与管理：Job","uri":"/application-orchestration-and-management-job/"},{"categories":["云原生"],"content":"Deployment 保证集群里可用 Pod 数量，为所有 Pod 更新镜像版本，保证更新过程中的服务可用性，实现快速回滚","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"需求来源","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:1:0","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"背景问题现在存在三个应用，分别对应一些 Pod，那么我们可以直接管理集群中所有的 Pod 吗？ 如果管理所有的 Pod，那么如何保证集群里可用 Pod 数量？如何为所有 Pod 更新镜像版本？更新的过程中，如何保证服务可用性？更新的过程中，发现问题如何快速回滚？ ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:1:1","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"Deployment：管理部署发布的控制器可以通过 Deployment 将三个应用分别规划到不同的 Deployment，每个 Deployment 管理一组相同的应用 Pod，这组 Pod 是相同的一个副本。 Deployment Deployment 可以实现以下功能： Deployment 定义了 Pod 期望数量； 配置 Pod 发布方式，即 controller 会按照用户给定的策略来更新 Pod，而且更新过程中，可以设定不可用 Pod 数量在多少范围内； 更新过程中发现问题可以回滚。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:1:2","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"架构设计","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:0","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"管理模式 Management Model Deployment 只管理不同版本的 ReplicaSet，由 ReplicaSet 来管理具体的 Pod 副本数，每个 ReplicaSet 对应 Deployment template 的一个版本。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:1","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"Deployment 控制器 Deployment Controller 控制器通过 Informer 中的 Event 做一些 Handler 和 Watch，收到事件后会加入到队列中。而 Deployment controller 从队列中取出来之后，它的逻辑会判断 Check Paused，如果 Paused 设置为 true 的话，就表示这个 Deployment 只会做一个数量上的维持，不会做新的发布，如果为 false 的话，就会做 Rollout。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:2","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"ReplicaSet 控制器 Replicaset Controller 当 Deployment 分配 ReplicaSet 之后，ReplicaSet 控制器本身也是从 Informer 中 watch 一些事件，这些事件包含了 ReplicaSet 和 Pod 的事件。从队列中取出之后，ReplicaSet controller 的逻辑很简单，就只管理副本数。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:3","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"发布模拟 Deployment Process Deployment 当前初始版本为 template1，底下有三个 Pod：Pod1、Pod2、Pod3。 这时修改 template 中一个容器的 image，Deployment controller 就会新建一个对应 template2 的 ReplicaSet。创建出来之后 ReplicaSet 会逐渐修改两个 ReplicaSet 的数量，比如逐渐增加 ReplicaSet2 中 replicas 的期望数量，逐渐减少 ReplicaSet1 中的 Pod 数量。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:4","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"spec 字段解析 MinReadySeconds：Deployment 会根据 Pod ready 来看 Pod 是否可用，但是如果我们设置了 MinReadySeconds 之后，比如设置为 30 秒，那 Deployment 就一定会等到 Pod ready 超过 30 秒之后才认为 Pod 是 available 的。Pod available 的前提条件是 Pod ready，但是 ready 的 Pod 不一定是 available 的，它一定要超过 MinReadySeconds 之后，才会判断为 available revisionHistoryLimit：保留历史 revision，即保留历史 ReplicaSet 的数量，默认值为 10 个。这里可以设置为一个或两个，如果回滚可能性比较大的话，可以设置数量超过 10； paused：paused 是标识，Deployment 只做数量维持，不做新的发布，这里在 Debug 场景可能会用到； progressDeadlineSeconds：前面提到当 Deployment 处于扩容或者发布状态时，它的 condition 会处于一个 processing 的状态，processing 可以设置一个超时时间。如果超过超时时间还处于 processing，那么 controller 将认为这个 Pod 会进入 failed 的状态。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:5","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"升级策略字段解析Deployment 在 RollingUpdate 中主要提供了两个策略，一个是 MaxUnavailable，另一个是 MaxSurge。 MaxUnavailable：滚动过程中最多有多少个 Pod 不可用； MaxSurge：滚动过程中最多存在多少个 Pod 超过预期 replicas 数量。 上文提到，ReplicaSet 为 3 的 Deployment 在发布的时候可能存在一种情况：新版本的 ReplicaSet 和旧版本的 ReplicaSet 都可能有两个 replicas，加在一起就是 4 个，超过了我们期望的数量三个。这是因为我们默认的 MaxUnavailable 和 MaxSurge 都是 25%，默认 Deployment 在发布的过程中，可能有 25% 的 replica 是不可用的，也可能超过 replica 数量 25% 是可用的，最高可以达到 125% 的 replica 数量。 这里其实可以根据用户实际场景来做设置。比如当用户的资源足够，且更注重发布过程中的可用性，可设置 MaxUnavailable 较小、MaxSurge 较大。但如果用户的资源比较紧张，可以设置 MaxSurge 较小，甚至设置为 0，这里要注意的是 MaxSurge 和 MaxUnavailable 不能同时为 0。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:6","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["LCTT"],"content":"可拓展性与灵活性、安全性和高可信的协作、不受供应商限制、顶尖人才和社区","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":" 使用开源技术可以帮助组织获得更好的业务结果。 构建技术栈是每个组织的主要决策。选择合适的工具将让团队获得成功，选择错误的解决方案或平台会对生产率和利润率产生毁灭性影响。为了在当今快节奏的世界中脱颖而出，组织必须明智地选择数字解决方案，好的数字解决方案可以提升团队行动力与运营敏捷性。 这就是为什么越来越多的组织都采用开源解决方案的原因，这些组织来自各行各业，规模有大有小。根据 麦肯锡 最近的报告，高绩效组织的最大区别是采用不同的开源方案。 采用开源技术可以帮助组织提高竞争优势、获得更好业务成果的原因有以下四点。 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:0:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":"1、可拓展性和灵活性可以说，技术世界发展很快。例如，在 2014 年之前，Kubernetes 并不存在，但今天，它却令人印象深刻，无处不在。根据 CNCF 2020 云原生调查，91% 的团队正在以某种形式使用 Kubernetes。 组织投资开源的一个主要原因是因为开源赋予组织行动敏捷性，组织可以迅速地将新技术集成到技术栈中。这与传统方法不同，在传统方法中，团队需要几个季度甚至几年来审查、实施、采用软件，这导致团队不可能实现火速转变。 开源解决方案完整地提供源代码，团队可以轻松将软件与他们每天使用的工具连接起来。 简而言之，开源让开发团队能够为手头的东西构建完美的工具，而不是被迫改变工作方式来适应不灵活的专有工具。 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:1:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":"2、安全性和高可信的协作在数据泄露备受瞩目的时代，组织需要高度安全的工具来保护敏感数据的安全。 专有解决方案中的漏洞不易被发现，被发现时为时已晚。不幸的是，使用这些平台的团队无法看到源代码，本质上是他们将安全性外包给特定供应商，并希望得到最好的结果。 采用开源的另一个主要原因是开源工具使组织能够自己把控安全。例如，开源项目——尤其是拥有大型开源社区的项目——往往会收到更负责任的漏洞披露，因为每个人在使用过程中都可以彻底检查源代码。 由于源代码是免费提供的，因此披露通常伴随着修复缺陷的详细建议解决方案。这些方案使得开发团队能够快速解决问题，不断增强软件。 在远程办公时代，对于分布式团队来说，在知道敏感数据受到保护的情况下进行协作比以往任何时候都更重要。开源解决方案允许组织审核安全性、完全掌控自己数据，因此开源方案可以促进远程环境下高可信协作方式的成长。 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:2:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":"3、不受供应商限制根据 最近的一项研究，68% 的 CIO 担心受供应商限制。当你受限于一项技术中，你会被迫接受别人的结论，而不是自己做结论。 当组织更换供应商时，专有解决方案通常会 给你带走数据带来挑战。另一方面，开源工具提供了组织需要的自由度和灵活性，以避免受供应商限制，开源工具可以让组织把数据带去任意地方。 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:3:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":"4、顶尖人才和社区随着越来越多的公司 接受远程办公，人才争夺战变得愈发激烈。 在软件开发领域，获得顶尖人才始于赋予工程师先进工具，让工程师在工作中充分发挥潜力。开发人员 越来越喜欢开源解决方案 而不是专有解决方案，组织应该强烈考虑用开源替代商业解决方案，以吸引市场上最好的开发人员。 除了雇佣、留住顶尖人才更容易，公司能够通过开源平台利用贡献者社区，得到解决问题的建议，从平台中得到最大收益。此外，社区成员还可以 直接为开源项目做贡献。 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:4:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":"开源带来自由开源软件在企业团队中越来越受到欢迎——这是有原因的。它帮助团队灵活地构建完美的工作工具，同时使团队可以维护高度安全的环境。同时，开源允许团队掌控未来方向，而不是局限于供应商的路线图。开源还帮助公司接触才华横溢的工程师和开源社区成员。 via: https://opensource.com/article/21/4/open-source-competitive-advantage 作者：Jason Blais 选题：lujun9972 译者：DCOLIVERSUN 校对：wxy 本文由 LCTT 原创编译，Linux中国 荣誉推出 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:5:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["云原生"],"content":"本文介绍了K8s的控制器模式与工作逻辑，解释了为何命令式API不适用控制器模式的原因","date":"2021-06-11","objectID":"/application-orchestration-and-management/","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"资源元信息","date":"2021-06-11","objectID":"/application-orchestration-and-management/:1:0","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"K8s 资源对象K8s 的资源对象组成：主要包括了 Spec、Status 两部分。其中 Spec 部分用来描述期望的状态，Status 部分用来描述观测到的状态。 其实，K8s 还有另外一部分，即元数据部分。该部分主要包括了用来识别资源的标签：Labels，用来描述资源的注解：Annotations，用来描述多个资源之间相互关系的 OwnerReference。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:1:1","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"labels第一个元数据，也是最重要的元数据是：资源标签。资源标签是一种具有标识型的 Key：Value 元数据。 标签主要用来筛选资源和组合资源，可以使用类似于 SQL 查询 select，来根据 Label 查询相关的资源。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:1:2","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"AnnotationsAnnotations 是系统或者工具用来存储资源的非标志性信息，可以用来扩展资源的 spec/status 的描述。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:1:3","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"Ownereference所谓所有者，一般就是指集合类的资源，比如说 Pod 集合，就有 replicaset、statefulset。 集合类资源的控制器会创建对应的归属资源。比如：replicaset 控制器在操作中会创建 Pod，被创建 Pod 的 Ownereference 就指向了创建 Pod 的 replicaset。Ownereference 使得用户可以方便地查找一个创建资源的对象，另外，还可以用来实现级联删除的效果。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:1:4","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"控制器模式","date":"2021-06-11","objectID":"/application-orchestration-and-management/:2:0","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"控制循环控制型模式最核心的就是控制循环的概念。在控制循环中包括了控制器，被控制的系统，以及能够观测系统的传感器。 Control Operation 这些组件都是逻辑的，外界通过修改资源 spec 来控制资源，控制器比较资源 spec 和 status，从而计算一个 diff，diff 最后会用来决定对系统的控制操作，控制操作会使得系统产生新的输出，并被传感器以资源 status 形式上报，控制器的各个组件将都会是独立自主地运行，不断使系统向 spec 表示终态趋近。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:2:1","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"Sensor循环控制中逻辑的传感器主要由 Reflector、Informer、Indexer 三个组件构成。 Sensor Reflector 通过 List 和 Watch K8s server 来获取资源的数据。List 用来在 Controller 重启以及 Watch 中断的情况下，进行系统资源的全量更新；而 Watch 则在多次 List 之间进行增量的资源更新；Reflector 在获取新的资源数据后，会在 Delta 队列中塞入一个包括资源对象信息本身以及资源对象事件类型的 Delta 记录，Delta 队列中可以保证同一个对象在队列中仅有一条记录，从而避免 Reflector 重新 List 和 Watch 的时候产生重复的记录。 Informer 组件不断从 Delta 队列中弹出 delta 记录，然后把资源对象交给 indexer，让 indexer 把资源记录在一个缓存中，缓存在默认设置下是用资源的命名空间来做索引的，并且可以被 Controller Manager 或多个 Controller 所共享。之后，再把这个事件交给事件的回调函数。 控制循环中的控制器组件主要由事件处理函数以及 worker 组成，事件处理函数会相互关注资源的新增、更新、删除的事件，并根据控制器的逻辑去决定是否需要处理。对需要处理的事件，会把事件关联资源的命名空间以及名字塞入一个工作队列中，并且由后续的 worker 池中的一个 worker 来处理，工作队列会对存储的对象进行去重，从而避免多个 worker 处理同一个资源的情况。 worker 在处理资源对象时，一般需要用资源的名字来重新获得最新的资源数据，用来创建或更新资源对象，或者调用其他的外部服务，worker 如果处理失败的时候，一般情况下会把资源的名字重新加入到工作队列中，从而方便之后进行重试。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:2:2","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"控制循环例子-扩容 apiVersion:apps/v1kind:ReplicaSetmetadata:name:rsAnamespace:nsAspec:replicas:3selector:matchLabels:env:prodtemplate:metadata:labels:env:prodspec:containers:- images:registry.cn-hangzhou.aliyuncs.com/acs/nginxname:nginxstatus:replicas:2 更新 Reflector会 watch 到 ReplicaSet 和 Pod 两种资源的变换； 向 delta 队列中塞入 rsA 对象，类型为更新； Informer 取出记录； 将新的 ReplicaSet 更新到缓存中，并与 Namespace nsA 作为索引； 调用 Update 回调函数，ReplicaSet 控制器发现 ReplicaSet 发生变化后把字符串 nsA/rsA 字符串塞入到工作队列中； worker 从工作队列中取到 nsA/rsA 这个字符串的 key； 从缓存中取到了最新的 ReplicaSet 数据； worker 通过比较 ReplicaSet 中 spec 和 status 里的数值，发现需要进行扩容操作，因此创建一个 Pod，这个 Pod 中的 Ownereference 指向了 ReplicaSet rsA； 扩容 Reflector Watch 到 Pod 新增事件； Reflector 在 Delta 队列中额外加入 Add 类型的 delta 记录； Informer 取出记录传送给 Indexer，调用 ReplicaSet 控制器的 Add 回调函数； 将记录存储在缓存中； Add 回调函数通过检查 pod ownerReferences 找到了对应的 ReplicaSet，并把包括 ReplicaSet 命名空间和字符串塞入到了工作队列中； ReplicaSet 的 worker 在得到新工作项之后，从缓存中取到了新的 ReplicaSet 记录，并得到了其所有创建的 Pod； ReplicaSet 更新 status 使得 spec 和 status 达成一致。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:2:3","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"控制器模式总结","date":"2021-06-11","objectID":"/application-orchestration-and-management/:3:0","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"两种 API 设计方法K8s 控制器模式依赖声明式 API，另一种常见 API 类型是命令式 API。 声明式 API：只说目标决定，具体操作交给 worker 命令式 API：告诉 worker 具体操作 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:3:1","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"命令式 API 的问题 错误处理：在大规模分布式系统中，错误是无处不在的。一旦发出的命令没有响应，调用方只能通过反复重试的方式来试图恢复错误，然而盲目的重试可能会带来更大的问题。 命令式 API 后台往往还有一个巡检系统，用来修正命令处理超时、重试等一些场景造成数据不一致的问题。 容易在并发访问时出现问题：假如有多方并发的对一个资源请求进行操作，一旦其中有操作出现错误，就要重试。那么很难确认是哪个操作生效。很多命令式系统往往在操作前会对系统进行加锁，从而保证整个系统最后生效行为的可预见性，但是加锁行为会降低整个系统的操作执行效率。 声明式 API 系统天然记录了系统现在和最终的状态。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:3:2","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"总结 K8s 所采用的控制器模式，是由声明式 API 驱动的。确切来说，是基于对 K8s 资源对象的修改来驱动的； K8s 资源之后，是关注该资源的控制器。这些控制器将异步的控制系统向设置的终态驱近； 控制器是自主运行的，使得系统的自动化和无人值守成为可能； 因为 K8s 的控制器和资源都是可以自定义的，因此可以方便的扩展控制器模式。特别是对于有状态应用，往往通过自定义资源和控制器的方式，来自动化运维操作。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:3:3","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"本文介绍为什么需要Pod、Pod的实现机制与设计模式","date":"2021-05-31","objectID":"/pod-notebook/","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"为什么需要 Pod","date":"2021-05-31","objectID":"/pod-notebook/:1:0","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"容器的基本概念容器的本质实际上是一个进程，是一个视图被隔离，资源受限的进程。 容器里面 PID = 1 的进程就是应用本身，这意味着管理虚拟机等于管理基础设施，因为我们是在管理机器，但管理容器等于直接管理应用本身。 ","date":"2021-05-31","objectID":"/pod-notebook/:1:1","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"Pod 的类比概念我们说 K8s 就是云时代的操作系统，我们举个真实操作系统的例子。 例如 Helloworld 程序，这个程序实际上是由一组进程组成的 ({api、main、log、compute})，这些进程等同于 Linux 中的线程。 K8s 类比为一个操作系统，同时容器类比为进程，那么 Pod 就可以类比为进程组。 ","date":"2021-05-31","objectID":"/pod-notebook/:1:2","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"进程组概念先提出一个问题：假如用容器把 Helloworld 程序跑起来，会怎么做呢？ 最自然的解法，启动一个 Docker 容器，里面运行四个进程。这里会引出一个问题，这个容器里面 PID = 1 的进程该是谁？如果该进程是 main 进程，那么由谁去管理剩下的三个进程？ 问题的核心在于，容器设计本身是一种“单进程”模型，不是说容器里只能起一个进程，由于容器的应用等于进程，所以只能去管理 PID = 1 进程，其他再起来的进程其实是一个托管状态。所以说服务应用进程本身具有“进程管理”的能力。 比如说 Helloworld 的程序有 system 的能力，或者直接把容器里 PID = 1 的进程直接改为 systemd，否则这个应用，或者说容器没有办法去管理很多个进程。因为 PID = 1 进程是应用本身，如果现在把这个 PID = 1 的进程给 kill 了，或者它自己运行过程中死掉了，那么剩下三个进程资源无法回收。 反过来，真的把应用本身改为 systemd，或者在容器里运行一个 systemd，这会导致另一个问题：管理容器不是管理应用本身，而是管理 systemd。如果应用退出或者 fail，容器是没有办法知道的。 在 K8s 中，四个进程共同组成的应用 Helloworld，会被定义为一个拥有四个容器的 Pod，四个容器共享 Pod 内资源。需要注意的是，Pod 是一个逻辑单位，没有真实的东西对应 Pod。 Pod ","date":"2021-05-31","objectID":"/pod-notebook/:1:3","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"为什么 Pod 必须是原子调度单位？因为存在调度失败Task co-scheduling，调度失败指的是分配容器到 Node 时，因为不知道全局分配信息，导致有紧密协作的容器不能被分配到同一个 Node。 调度失败存在如下解决办法： Mesos 的资源囤积：当所有设置了 Affinity 约束的任务都达到时，才开始统一调度； Omega 系统的乐观调度：不管冲突的异常情况，先调度，同时设置一个回滚机制，冲突后利用回滚解决问题； K8s 的 Pod：将紧密协作的容器视为一个 Pod，以 Pod 为单位进行调度。 ","date":"2021-05-31","objectID":"/pod-notebook/:1:4","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"再次理解 Pod首先，Pod 里面的容器是 “超亲密关系”。与亲密关系不同的是，亲密关系是可以通过调度解决的，例如两个 Pod 运行在同一台宿主机上，而超亲密关系大致分为如下几类： 文件交换； 需要通过 localhost 或者本地 socket 进行通信； 需要非常频繁的 RPC 调用； 需要共享某些 Linux Namespace。 ","date":"2021-05-31","objectID":"/pod-notebook/:1:5","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"Pod 的实现机制Pod 要打破容器间的 Linux Namespace 和 cgroups 隔离，具体解法分为两部分：网络和存储。 共享网络 在每个 Pod 里，额外起一个 Infra container 小容器来共享整个 Pod 的 Network Namespace。 Infra container 是一个非常小的镜像，大概 100~200KB 左右，是一个汇编语言写的、永远处于“暂停”状态的容器。由于有了这样一个 Infra container 之后，其他所有容器都会通过 Join Namespace 的方式加入到 Infra container 的 Network Namespace 中。 所以说一个 Pod 里面的所有容器，它们看到的网络视图是完全一样的。即：它们看到的网络设备、IP地址、MAC地址等等，跟网络相关的信息，其实全是一份，这一份都来自于 Pod 第一次创建的这个 Infra container。 共享存储 比如说现在有两个容器，一个是 Nginx，另外一个是非常普通的容器，在 Nginx 里放一些文件，让我能通过 Nginx 访问到。所以它需要去 share 这个目录。share 文件或者是 share 目录在 Pod 里面是非常简单的，实际上就是把 volume 变成了 Pod level。然后所有容器，就是所有同属于一个 Pod 的容器，他们共享所有的 volume。 ","date":"2021-05-31","objectID":"/pod-notebook/:2:0","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"详解容器设计模式","date":"2021-05-31","objectID":"/pod-notebook/:3:0","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"InitContainer可以在 yaml 里首先定义一个 Init Container，完成操作后就退出。这个容器会比用户容器先启动，并且严格按照定义顺序来依次执行。 ","date":"2021-05-31","objectID":"/pod-notebook/:3:1","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"容器设计模式：SidecarSidecar 的含义是可以定义一些专门的容器，来执行主业务容器所需要的一些辅助工作，例如： 原本需要在容器里面执行 SSH 需要干的一些事情，可以写脚本、一些前置的条件，其实都可以通过像 Init Container 或者另外像 Sidecar 的方式去解决； 日志收集，日志收集本身是一个进程，是一个小容器，那么就可以把它打包进 Pod 里面去做这个收集工作； Debug 应用，实际上现在 Debug 整个应用都可以在应用 Pod 里面再次定义一个额外的小的 Container，它可以去 exec 应用 pod 的 namespace； 查看其他容器的工作状态。不再需要去 SSH 登陆到容器里去看，只要把监控组件装到额外的小容器里面就可以了，然后把它作为一个 Sidecar 启动起来，跟主业务容器进行协作，所以同样业务监控也都可以通过 Sidecar 方式来去做。 这种做法一个非常明显的优势就是在于其实将辅助功能从业务容器解耦了，所以我就能够独立发布 Sidecar 容器，并且更重要的是这个能力是可以重用的，即同样的一个监控 Sidecar 或者日志 Sidecar。 ","date":"2021-05-31","objectID":"/pod-notebook/:3:2","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"本文详细介绍了 K8s 的概念、核心功能、架构、核心概念与API，非常全面的 K8s 整理笔记","date":"2021-05-26","objectID":"/kubernetes-notebook/","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"什么是 KubernetesKubernetes 是一个自动化的容器编排平台，负责应用的部署、弹性以及管理，这些都是基于容器的。可以简称为 k8s，是将中间 8 个字母 “ubernete” 替换为 “8” 而导致的一个缩写。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:1:0","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"Kubernetes 的核心功能 服务发现与负载均衡； 容器的自动装箱，也就是调度scheduling，把一个容器放到一个集群的某一个机器上，K8s 会自动做存储的编排，让存储的生命周期和容器的生命周期能有一个连接； K8s 会做自动化的容器恢复。在一个集群中，经常会出现宿主机的问题或者说是 OS 的问题，导致容器本身的不可用，K8s 会自动恢复不可用的容器； K8s 会做应用的自动发布与应用的回滚，以及与应用相关的配置密文的管理； 对于 Job 类型任务，K8s 可以做批量执行； 为了让这个集群、这个应用更富有弹性，K8s 也支撑水平伸缩。 举个例子 调度 K8s 可以把用户提交的容器放到 K8s 管理的集群的某一台节点上去。K8s 的调度器是执行这项工作的组件，它会观察正在被调度的这个容器的大小、规格。 根据容器所需要的 CPU 和需要的 memory，在集群中找一台相对比较空闲的机器来放置。 自动修复 K8s 有一个节点健康检查的功能，会监测这个集群中所有的宿主机，当宿主机本身出现故障，或软件出现故障的时候，K8s 会发现这个情况。 下面，K8s 将运行在失败节点上的容器进行自动迁移，迁移到一个正在健康运行的宿主机上，来完成集群内容器的一个自动恢复。 水平伸缩 K8s 有业务负载检查能力，会监测业务上所承担的负载，如果这个业务本身的 CPU 利用率过高，或者响应时间过长，它可以对这个业务进行一次扩容。 比如，将这个业务分为三份，分布到不同的节点上，以此来提高响应的时间。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:2:0","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"Kubernetes 的架构K8s 架构是一个比较典型的二层架构和 server-client 架构。Master 作为中央的管控节点，会去与 Node 进行一个连接。 所有 UI 的 clients 和 user 侧的组件，只会和 Master 进行连接，把希望的状态或者想执行的命令下发给 Master，Master 会把这些命令或状态下发给相应的节点，执行最终的执行。 Kubernetes 架构 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:3:0","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的架构：MasterK8s 的 Master 包含四个主要的组件：API Server、Controller、Scheduler 以及 etcd。如下图所示： Kubernetes Master API Server： 顾名思义是用来处理 API 操作的，K8s 中所有组件都会和 API Server 进行连接，组件与组件之间一般不进行独立的连接，都依赖于 API Server 进行消息的传送； Controller： 控制器，用来完成对集群状态的一些管理； Scheduler： 调度器，依据用户提交 Container 所需的 CPU 和 memory 请求大小，找到合适的节点进行放置； etcd： 分布式存储系统，API Server 中所需的原信息放置在 etcd 中，etcd 本身也是一个高可用系统，通过 etcd 保证整个 K8s 的 Master 组件的高可用性。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:3:1","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的架构：NodeK8s 的 Node 是真正运行业务负载的，每个业务负载会以 Pod 的形式运行。一个 Pod 中运行的一个或者多个容器，真正去运行这些 Pod 的组件的是叫做 kubelet，也就是 Node 上最为关键的组件，它通过 API Server 接收到所需要 Pod 运行的状态，然后提交到 Container Runtime 组件中。 Kubernetes Node 在 OS 上去创建容器所需要运行的环境，最终把容器或者 Pod 运行起来，也需要对存储和网络进行管理。K8s 并不会直接进行网络存储的操作，他们会靠 Storage Plugin 或者是网络的 Plugin 来进行操作。用户自己或者云厂商都会去写相应的 Storage Plugin 或者 Network Plugin，去完成存储操作或网络操作。 在 K8s 自己的环境中，也会有 K8s 的 Network，它是为了提供 Service network 来进行搭网组网的。真正完成 service 组网的组件的是 Kube-proxy，它是利用了 iptable 的能力来进行组建 K8s 的 Network，就是 cluster network，以上就是 Node 上面的四个组件。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:3:2","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"组件之间的通信 Component Communication 用户可以通过 UI 或 CLI 提交一个 Pod 给 K8s 进行部署，这个 Pod 请求首先会通过 CLI 或者 UI 提交给 K8s API Server； API Server 会把这个信息写入到它的存储系统 etcd； 之后 Scheduler 会通过 API Server 的 watch 或者叫做 notification 机制得到这个信息：有一个 Pod 需要被调度； Scheduler 会根据内存状态进行一次调度决策，完成调度后通知 API Server； API Server 收到这次操作后会把结果再次写到 etcd 中； API Server 会通知相应节点进行这次 Pod 真正的执行启动； 相应节点的 kubelet 得到通知，调 Container runtime 去启动容器和环境，调度 Storage Plugin 去配置存储，network Plugin 去配置网络。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:3:3","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念与 API","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:0","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念：PodPod 是 K8s 的一个最小调度以及资源单元。用户可以通过 K8s 的 Pod API 生产一个 Pod，让 K8s 对这个 Pod 进行调度，也就是把它放在某一个 K8s 管理的节点上运行起来。一个 Pod 简单来说是对一组容器的抽象，它里面会包含一个或多个容器。 在 Pod 里面，可以去定义容器所需要运行的方式。比如说运行容器的 Command，以及运行容器的环境变量等等。Pod 这个抽象也给这些容器提供了一个共享的运行环境，它们会共享同一个网络环境，这些容器可以用 localhost 来进行直接的连接。而 Pod 与 Pod 之间，是互相有 isolation 隔离的。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:1","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念：VolumeVolume 就是卷的概念，它是用来管理 K8s 存储的，是用来声明在 Pod 中的容器可以访问文件目录的，一个卷可以被挂载在 Pod 中一个或者多个容器的指定路径下面。 Kubernetes Conception: Volume 而 Volume 本身是一个抽象的概念，一个 Volume 可以去支持多种的后端的存储。比如说 K8s 的 Volume 就支持了很多存储插件，它可以支持本地的存储，可以支持分布式的存储，比如说像 ceph，GlusterFS ；它也可以支持云存储，比如说阿里云上的云盘、AWS 上的云盘、Google 上的云盘等等。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:2","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念：DeploymentDeployment 是在 Pod 这个抽象上更为上层的一个抽象，它可以定义一组 Pod 的副本数目、以及这个 Pod 的版本。一般大家用 Deployment 这个抽象来做应用的真正的管理，而 Pod 是组成 Deployment 最小的单元。 Kubernetes Conception: Deployment Kubernetes 是通过 Controller，也就是我们刚才提到的控制器去维护 Deployment 中 Pod 的数目，它也会去帮助 Deployment 自动恢复失败的 Pod。 比如说我可以定义一个 Deployment，这个 Deployment 里面需要两个 Pod，当一个 Pod 失败的时候，控制器就会监测到，它重新把 Deployment 中的 Pod 数目从一个恢复到两个，通过再去新生成一个 Pod。通过控制器，我们也会帮助完成发布的策略。比如说进行滚动升级，进行重新生成的升级，或者进行版本的回滚。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:3","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念：ServiceService 提供了一个或者多个 Pod 实例的稳定访问地址。 比如在上面的例子中，我们看到：一个 Deployment 可能有两个甚至更多个完全相同的 Pod。对于一个外部的用户来讲，访问哪个 Pod 其实都是一样的，所以它希望做一次负载均衡，在做负载均衡的同时，我只想访问某一个固定的 VIP，也就是 Virtual IP 地址，而不希望得知每一个具体的 Pod 的 IP 地址。 我们刚才提到，这个 pod 本身可能 terminal go（终止），如果一个 Pod 失败了，可能会换成另外一个新的。 对一个外部用户来讲，提供了多个具体的 Pod 地址，这个用户要不停地去更新 Pod 地址，当这个 Pod 再失败重启之后，我们希望有一个抽象，把所有 Pod 的访问能力抽象成一个第三方的一个 IP 地址，实现这个的 K8s 的抽象就叫 Service。 实现 Service 有多种方式，K8s 支持 Cluster IP，上面我们讲过的 kuber-proxy 的组网，它也支持 nodePort、 LoadBalancer 等其他的一些访问的能力。 Kubernetes Conception: Service ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:4","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念：NamespaceNamespace 是用来做一个集群内部的逻辑隔离的，它包括鉴权、资源管理等。K8s 的每个资源，比如刚才讲的 Pod、Deployment、Service 都属于一个 Namespace，同一个 Namespace 中的资源需要命名的唯一性，不同的 Namespace 中的资源可以重名。 Kubernetes Conception: Namespace ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:5","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的 API从 high-level 上看，K8s API 是由 HTTP+JSON 组成的：用户访问的方式是 HTTP，访问的 API 中 content 的内容是 JSON 格式的。 K8s 的 kubectl 也就是 command tool，Kubernetes UI，或者有时候用 curl，直接与 K8s 进行沟通，都是使用 HTTP + JSON 这种形式。 Kubernetes API 如图所示，对于这个 Pod 类型的资源，它的 HTTP 访问路径，就是 API，然后是 apiVersion: V1，之后是相应的 Namespace，以及 Pods 资源，最终是 Podname。 如果我们去提交一个 Pod，或者 get 一个 Pod 的时候，它的 content 内容都是用 JSON 或者是 YAML 表达的。上图中右侧是 yaml 的例子，在这个 yaml file 中，对 Pod 资源的描述也分为几个部分。 第一个部分，一般来讲会是 API 的 version。比如在这个例子中是 V1，它也会描述我在操作哪个资源；比如说我的 kind 如果是 pod，在 Metadata 中，就写上这个 Pod 的名字；比如说 nginx，我们也会给它打一些 label，我们等下会讲到 label 的概念。在 Metadata 中，有时候也会去写 annotation，也就是对资源的额外的一些用户层次的描述。 比较重要的一个部分叫做 Spec，Spec 也就是我们希望 Pod 达到的一个预期的状态。比如说它内部需要有哪些 container 被运行；比如说这里面有一个 nginx 的 container，它的 image 是什么？它暴露的 port 是什么？ 当我们从 Kubernetes API 中去获取这个资源的时候，一般来讲在 Spec 下面会有一个项目叫 status，它表达了这个资源当前的状态；比如说一个 Pod 的状态可能是正在被调度、或者是已经 running、或者是已经被 terminates，就是被执行完毕了。 刚刚在 API 之中，我们讲了一个比较有意思的 metadata 叫做“label”，这个 label 可以是一组 KeyValuePair。 这些 label 是可以被 selector，也就是选择器所查询的。这个能力实际上跟我们的 sql 类型的 select 语句是非常相似的。 通过 label，k8s 的 API 层就可以对这些资源进行一个筛选，那这些筛选也是 kubernetes 对资源的集合所表达默认的一种方式。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:6","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"什么是容器与镜像？容器的生命周期如何？容器与虚拟机的区别？","date":"2021-05-11","objectID":"/docker-notebook/","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"容器与镜像","date":"2021-05-11","objectID":"/docker-notebook/:1:0","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"什么是容器？在介绍容器的具体概念之前，先简单回顾一下操作系统是如何管理进程的。 操作系统与容器对比 首先，当我们登陆到操作系统之后，可以通过 ps 等操作看到各式各样的进程，这些进程包括系统自带的服务和用户的应用进程。那么，这些进程都有什么样的特点？ 这些进程可以相互看到、相互通信； 它们使用的是同一个文件系统，可以对同一个文件进行读写操作； 这些进程会使用相同的系统资源。 这样的三个特点会带来什么问题呢？ 因为这些进程相互看到并且进行通信，高级权限的进程可以攻击其他进程； 因为它们使用同一个文件系统，因此会带来两个问题：这些进程可以对于已有的数据进行增删改查，具有高级权限的进程可能会将其他进程的数据删除掉，破坏掉其他进程的正常运行；此外，进程与进程之间的依赖可能会存在冲突，如此一来就会给运维带来很大的压力； 因为这些进程使用同一个宿主机的资源，应用之间可能会存在资源抢占的问题，当一个应用需要消耗大量 CPU 和内存资源的时候，就可能破坏其他应用的运行，导致其他应用无法正常地提供服务。 针对上述的三个问题，如何为进程提供一个独立的运行环境呢？ 针对不同进程使用同一个文件系统所造成的问题而言，Linux 和 Unix 操作系统可以通过 chroot 系统调用将子目录变成根目录，达到视图级别的隔离；进程在 chroot 的帮助下可以具有独立的文件系统，对于这样的文件系统进行增删改查不会影响到其他进程； 因为进程之间相互可见并且可以相互通信，使用 Namespace 技术来实现进程在资源的视图上进行隔离。在 chroot 和 Namespace 的帮助下，进程就能够运行在一个独立的环境下了； 在独立的环境下，进程所使用的还是同一个操作系统的资源，一些进程可能会侵蚀掉整个系统的资源。为了减少进程彼此之间的影响，可以通过 Cgroup 来限制其资源使用率，设置其能够使用的 CPU 以及内存量。 那么，应该如何定义这样的进程集合呢？ 其实，容器就是一个视图隔离、资源可限制、独立文件系统的进程集合。所谓“视图隔离”就是能够看到部分进程以及具有独立的主机名等；控制资源使用率则是可以对于内存大小以及 CPU 使用个数等进行限制。容器就是一个进程集合，它将系统的其他资源隔离开来，具有自己独立的资源视图。 容器具有一个独立的文件系统，因为使用的是系统的资源，所以在独立的文件系统内不需要具备内核相关的代码或工具，我们只需要提供容器所需的二进制文件、配置文件以及依赖即可。只要容器运行时所需的文件集合都能够具备，那么这个容器就能够运行起来。 ","date":"2021-05-11","objectID":"/docker-notebook/:1:1","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"什么是镜像？综上所述，我们将这些容器运行时所需要的所有文件集合称之为容器镜像。 那么，一般通过什么方式来构建镜像的呢？通常情况下，我们会采用 Dockerfile 来构建镜像，这是因为 Dockerfile 提供了非常便利的语法，能够帮助我们很好地描述构建的每个步骤。当然，每个构建步骤都会对已有的文件系统进行操作，这样就会带来文件系统内容的变化，我们将这些变化称之为 changeset。当我们把构建步骤所产生的变化依次作用到一个空文件夹上，就能得到一个完整的镜像。 changeset 分层示意 changeset 的分层以及复用特点能够带来几点优势： 提高分发效率，简单试想一下，对于大镜像而言，如果将其拆分成各个小块就能够提高镜像的分发效率，这是因为镜像拆分之后就可以并行下载。 因为数据是相互共享的，也就意味着当本地存储上包含了一些数据的时候，只需要下载本地没有的数据即可。 因为镜像数据是共享的，因此可以节约大量的磁盘空间，简单设想一下，当本地存储具有了 alpine 镜像和 golang 镜像，在没有复用能力之前，alpine 镜像具有 5M 大小，Golang 镜像有 300M 大小，因此就会占用 305M 空间；而当具有复用能力之后，只需要 300 M 空间即可，这是因为 Golang 镜像是基于 alpine 镜像构建的。 ","date":"2021-05-11","objectID":"/docker-notebook/:1:2","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"如何构建镜像？如下所示的 Dockerfile 适用于描述如何构建 Golang 应用的。 # base on golang:1.12-alpine imageFROMgolang:1.12-alpine# setting current working dir (PWD -\u003e /go/src/app)WORKDIR/go/src/app# copy local files into /go/src/appCOPY ..# get all the dependenciesRUN go get -d -v ./...# build the application and install itRUN go install -v ./...# by default, run the appCMD[\"app\"] 如上代码所示： FROM 行表示以下的构建步骤基于什么镜像进行构建，正如前述，镜像是可以复用的； WORKDIR 行表示会把接下来的构建步骤都在哪一个相应的具体目录下进行，其起到的作用类似于 shell 里面的 cd； COPY 行表示的是可以将宿主机上的文件拷贝到容器镜像内； RUN 行表示在具体的文件系统内执行相应的动作。当我们运行完毕后就可以得到一个应用了； CMD 行表示使用镜像时的默认程序名字。 当有了 Dockerfile 之后，就可以通过 docker build 命令构建出所需要的应用。构建出的结果存储在本地，一般情况下，镜像构建会在打包机或者其他的隔离环境下完成。 ","date":"2021-05-11","objectID":"/docker-notebook/:1:3","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"如何运行容器 容器运行过程 运行一个容器一般情况下分为三步： 第一步：从镜像仓库中将相应的镜像下载下来； 第二步：当镜像下载完成之后，就可以通过 docker images 来查看本地镜像； 第三步：当选中镜像之后，就可以通过 docker run 来运行这个镜像得到想要的容器，当然可以通过多次运行得到多个容器。一个镜像就相当于是一个模板，一个容器就像是一个具体的运行实例，因此镜像就具有了一次构建、到处运行的特点。 ","date":"2021-05-11","objectID":"/docker-notebook/:1:4","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"容器的生命周期容器是一组具有隔离特性的进程集合，在使用 docker run 的时候会选择一个镜像来提供独立的文件系统并指定相应的运行程序。这里指定的运行程序称之为 initial 进程，这个 initial 进程启动的时候，容器也会随之启动，当 initial 进程退出的时候，容器也会随之退出。 因此，可以认为容器的生命周期和 initial 进程的生命周期是一致的。当然，因为容器内不只有这样的一个 initial 进程，initial 进程本身也可以产生其他的子进程或者通过 docker exec 产生出来的运维操作，也属于 initial 进程管理的范围内。当 initial 进程退出的时候，所有的子进程也会随之退出，这样也是为了防止资源的泄漏。 但是这样的做法也会存在一些问题，首先应用里面的程序往往是有状态的，其可能会产生一些重要的数据，当一个容器退出被删除之后，数据也就会丢失了，这对于应用方而言是不能接受的，所以需要将容器所产生出来的重要数据持久化下来。容器能够直接将数据持久化到指定的目录上，这个目录就称之为数据卷。 数据卷有一些特点，其中非常明显的就是数据卷的生命周期是独立于容器的生命周期的，也就是说容器的创建、运行、停止、删除等操作都和数据卷没有任何关系，因为它是一个特殊的目录，是用于帮助容器进行持久化的。简单而言，我们会将数据卷挂载到容器内，这样一来容器就能够将数据写入到相应的目录里面了，而且容器的退出并不会导致数据的丢失。 通常情况下，数据卷管理主要有两种方式： 第一种是通过 bind 的方式，直接将宿主机的目录直接挂载到容器内；这种方式比较简单，但是会带来运维成本，因为其依赖于宿主机的目录，需要对于所有的宿主机进行统一管理。 第二种是将目录管理交给运行引擎。 # bind host dir into container docker run -v /tmp:/tmp busybox:1.25 sh -c \"data \u003e /tmp/demo.log\" # check result cat /tmp/demo.log # let it handled by docker container engine docker create volume demo # demo is volume name docker run -v demo:/tmp busybox:1.25 sh -c \"date \u003e /tmp/demo.log\" # check result docker run -v demo:/tmp busybox:1.25 sh -c \"cat /tmp/demo.log\" ","date":"2021-05-11","objectID":"/docker-notebook/:2:0","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"容器项目架构：moby 容器引擎架构moby 是目前最流行的容器管理引擎，moby daemon 会对上提供有关于容器、镜像、网络以及 Volume的管理。moby daemon 所依赖的最重要的组件就是 containerd，containerd 是一个容器运行时管理引擎，其独立于 moby daemon ，可以对上提供容器、镜像的相关管理。 moby 引擎 containerd 底层有 containerd shim 模块，其类似于一个守护进程，这样设计的原因有几点： 首先，containerd 需要管理容器生命周期，而容器可能是由不同的容器运行时所创建出来的，因此需要提供一个灵活的插件化管理。而 shim 就是针对于不同的容器运行时所开发的，这样就能够从 containerd 中脱离出来，通过插件的形式进行管理。 其次，因为 shim 插件化的实现，使其能够被 containerd 动态接管。如果不具备这样的能力，当 moby daemon 或者 containerd daemon 意外退出的时候，容器就没人管理了，那么它也会随之消失、退出，这样就会影响到应用的运行。 最后，因为随时可能会对 moby 或者 containerd 进行升级，如果不提供 shim 机制，那么就无法做到原地升级，也无法做到不影响业务的升级，因此 containerd shim 非常重要，它实现了动态接管的能力。 ","date":"2021-05-11","objectID":"/docker-notebook/:3:0","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"容器 VS VM 容器与虚拟机对比示意 容器 VM 进程级的隔离效果比 VM 要差 因为每个 Guest OS 都有独立的内核，所以 VM 提供一个更好的隔离效果 所需磁盘空间小 所需磁盘空间大 响应时间快，因为文件隔离都是进程级别的 响应时间慢，因为一部分计算资源交给虚拟化 ","date":"2021-05-11","objectID":"/docker-notebook/:4:0","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"总结 容器是一个进程集合，具有自己独特的视图视角； 镜像是容器所需要的所有文件集合，其具备一次构建、到处运行的特点； 容器的生命周期和 initial 进程的生命周期是一样的； 容器和 VM 相比，各有优劣，容器技术在向着强隔离方向发展。 ","date":"2021-05-11","objectID":"/docker-notebook/:5:0","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"学好 Yaml 语法，打好 K8s 基础","date":"2021-05-08","objectID":"/yaml-notebook/","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"简单说明Yaml 是一个可读性高，用来表达数据序列的格式。Yaml 的意思是：仍是一种标记语言，强调这种语言以数据做为中心，而不是以标记语言为重点 ","date":"2021-05-08","objectID":"/yaml-notebook/:1:0","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"基本语法 缩进时不允许使用 Tab 键，只允许使用空格 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 标识注释，从这个字符一直到行尾，都会被解释器忽略 ","date":"2021-05-08","objectID":"/yaml-notebook/:2:0","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"Yaml 支持的数据结构 对象：键值对的集合，又称为映射mapping映射mapping/ 哈希hash哈希hash/ 字典dictionary字典dictionary 数组：一组按次序排列的值，又称为序列sequence序列sequence/ 列表list列表list 纯量scalars纯量scalars：单个的、不可再分的值 ","date":"2021-05-08","objectID":"/yaml-notebook/:3:0","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"对象类型：对象的一组键值对，使用冒号结构表示 name:Oliverage:24 Yaml 也允许另一种写法，将所有键值对写成一个行内对象 hash:{name: Oliver, age:24} Yaml 允许存在层级键值对 major:name:computername:law ","date":"2021-05-08","objectID":"/yaml-notebook/:3:1","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"数组类型：一组连词线开头的行，构成一个数组 Sport:- Swim- Run 数组也可以采用行内表示法 Sport:[Swim, Run] 数组的一个相对复杂的实现方式 products:-id:1name:eggprice:8-id:2name:meatprice:33 ","date":"2021-05-08","objectID":"/yaml-notebook/:3:2","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"复合结构：对象和数组可以结合使用，形成复合结构 sport:- swim- run- basketballcity:haidian:beijing.haidianjinan:shandong.jinanhefei:anhui.hefei ","date":"2021-05-08","objectID":"/yaml-notebook/:3:3","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"纯量：纯量是最基本的、不可再分的值 字符串、布尔值、整数、浮点数、Null 时间、日期 数值 数值直接以字面量的形式表示 number:12.30 布尔值 布尔值用 true 和 false 表示 isSet:true Null null 用～表示 parent:~ 时间 时间采用 ISO8601 格式 iso8601:2001-12-14t21:59:43.10-05:00 日期 日期采用 ISO8601 格式的年、月、日表示 date:1976-07-31 类型转换 Yaml 允许使用两个感叹号，强制转换数据类型 e:!!str123f:!!strtrue 字符串 字符串默认不使用引号表示 str:我是字符串 如果字符串之中包含空格或特殊字符，需要放在引号中 str:'内容： 字符串' 单引号之中如果还有单引号，必须连续使用两个单引号转义 str:'labor’‘s day' 字符串可以写成多行，从第二行开始，必须有一个单空格缩进，换行符会被转为空格 str:我是一段多行字符串 多行字符串可以使用 | 保留换行符，也可以使用 \u003e 折叠换行 this:|Foo Barthat:\u003eFoo Bar + 表示保留文字块末尾的换行，- 表示删除字符串末尾的换行 s1:|Foos2:|+Foos3:|-Foo ","date":"2021-05-08","objectID":"/yaml-notebook/:3:4","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"引用\u0026 锚点和 * 别名，可以用来引用: defaults:\u0026defaultsadapter:postgreshost:localhostdevelopment:database:myapp_development\u003c\u003c:*defaultstest:database:myapp_test\u003c\u003c:*defaults 相当于 defaults:adapter:postgreshost:localhostdevelopment:database:myapp_developmentadapter:postgreshost:localhosttest:database:myapp_testadapter:postgreshost:localhost \u0026 用来建立锚点（defaults），« 表示合并到当前数据，* 用来引用锚点。 ","date":"2021-05-08","objectID":"/yaml-notebook/:4:0","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["LCTT"],"content":"Git 提供了几种方式可以帮你快速查看提交中哪些文件被改变。","date":"2021-04-24","objectID":"/git-whatchanged/","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"如果你每天使用 Git，应该会提交不少改动。如果你每天和其他人在一个项目中使用 Git，假设 每个人 每天的提交都是安全的，你会意识到 Git 日志会变得多么混乱，似乎永恒地滚动着变化，却没有任何迹象表明修改了什么。 那么，你该怎样查看指定提交中文件发生哪些变化？这比你想的容易。 ","date":"2021-04-24","objectID":"/git-whatchanged/:0:0","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"查看提交中文件发生的变化要想知道指定提交中哪些文件发生变化，可以使用 git log --raw 命令。这是发现一个提交影响了哪些文件的最快速、最方便的方法。git log 命令一般都没有被充分利用，主要是因为它有太多的格式化选项，许多用户在面对很多选择以及在一些情况下不明所以的文档时，会望而却步。 然而，Git 的日志机制非常灵活，--raw 选项提供了当前分支中的提交日志，以及更改的文件列表。 以下是标准的 git log 输出： $ git log commit fbbbe083aed75b24f2c77b1825ecab10def0953c (HEAD -\u003e dev, origin/dev) Author: tux \u003ctux@example.com\u003e Date: Sun Nov 5 21:40:37 2020 +1300 exit immediately from failed download commit 094f9948cd995acfc331a6965032ea0d38e01f03 (origin/master, master) Author: Tux \u003ctux@example.com\u003e Date: Fri Aug 5 02:05:19 2020 +1200 export makeopts from etc/example.conf commit 76b7b46dc53ec13316abb49cc7b37914215acd47 Author: Tux \u003ctux@example.com\u003e Date: Sun Jul 31 21:45:24 2020 +1200 fix typo in help message 即使作者在提交消息中指定了哪些文件发生变化，日志也相当简洁。 以下是 git log --raw 输出： $ git log --raw commit fbbbe083aed75b24f2c77b1825ecab10def0953c (HEAD -\u003e dev, origin/dev) Author: tux \u003ctux@example.com\u003e Date: Sun Nov 5 21:40:37 2020 +1300 exit immediately from failed download :100755 100755 cbcf1f3 4cac92f M src/example.lua commit 094f9948cd995acfc331a6965032ea0d38e01f03 (origin/master, master) Author: Tux \u003ctux@example.com\u003e Date: Fri Aug 5 02:05:19 2020 +1200 export makeopts from etc/example.conf :100755 100755 4c815c0 cbcf1f3 M src/example.lua :100755 100755 71653e1 8f5d5a6 M src/example.spec :100644 100644 9d21a6f e33caba R100 etc/example.conf etc/example.conf-default commit 76b7b46dc53ec13316abb49cc7b37914215acd47 Author: Tux \u003ctux@example.com\u003e Date: Sun Jul 31 21:45:24 2020 +1200 fix typo in help message :100755 100755 e253aaf 4c815c0 M src/example.lua 这会准确告诉你哪个文件被添加到提交中，哪些文件发生改变（A 是添加，M 是修改，R 是重命名，D 是删除）。 ","date":"2021-04-24","objectID":"/git-whatchanged/:1:0","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"Git whatchangedgit whatchanged 命令是一个遗留命令，它的前身是日志功能。文档说用户不应该用该命令替代 git log --raw，并且暗示它实质上已经被废弃了。不过，我还是觉得它是一个很有用的捷径，可以得到同样的输出结果（尽管合并提交的内容不包括在内），如果它被删除的话，我打算为它创建一个别名。如果你只想查看已更改的文件，不想在日志中看到合并提交，可以尝试 git whatchanged 作为简单的助记符。 ","date":"2021-04-24","objectID":"/git-whatchanged/:2:0","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"查看变化你不仅可以看到哪些文件发生更改，还可以使用 git log 显示文件中发生了哪些变化。你的 Git 日志可以生成一个内联差异，用 --patch 选项可以逐行显示每个文件的所有更改： commit 62a2daf8411eccbec0af69e4736a0fcf0a469ab1 (HEAD -\u003e master) Author: Tux \u003cTux@example.com\u003e Date: Wed Mar 10 06:46:58 2021 +1300 commit diff --git a/hello.txt b/hello.txt index 65a56c3..36a0a7d 100644 --- a/hello.txt +++ b/hello.txt @@ -1,2 +1,2 @@ Hello -world +opensource.com 在这个例子中，“world” 这行字从 hello.txt 中删掉，“opensource.com” 这行字则添加进去。 如果你需要在其他地方手动进行相同的修改，这些补丁patch可以与常见的 Unix 命令一起使用，例如 diff 与 patch。补丁也是一个好方法，可以总结指定提交中引入新信息的重要部分内容。当你在冲刺阶段引入一个 bug 时，你会发现这里的内容就是非常有价值的概述。为了更快地找到错误的原因，你可以忽略文件中没有更改的部分，只检查新代码。 ","date":"2021-04-24","objectID":"/git-whatchanged/:3:0","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"用简单命令得到复杂的结果你不必理解引用、分支和提交哈希，就可以查看提交中更改了哪些文件。你的 Git 日志旨在向你报告 Git 的活动，如果你想以特定方式格式化它或者提取特定的信息，通常需要费力地浏览许多文档来组合出正确的命令。幸运的是，关于 Git 历史记录最常用的请求之一只需要一两个选项：--raw 与 --patch。如果你不记得 --raw，就想想“Git，什么改变了？”，然后输入 git whatchanged。 via: https://opensource.com/article/21/4/git-whatchanged 作者：Seth Kenlon 选题：lujun9972 译者：DCOLIVERSUN 校对：wxy 本文由 LCTT 原创编译，Linux中国 荣誉推出 ","date":"2021-04-24","objectID":"/git-whatchanged/:4:0","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"一款简洁、好用的 Feed 阅读器，让你及时跟进想看的内容","date":"2021-04-13","objectID":"/newsflash/","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":" 有些人可能认为 RSS 阅读器已经不再，但它们仍然坚持在这里，特别是当你不想让大科技算法来决定你应该阅读什么的时候。Feed 阅读器可以帮你自助选择阅读来源。 我最近遇到一个很棒的 RSS 阅读器 NewsFlash。它支持通过基于网页的 Feed 阅读器增加 feed，例如 Feedly 和 NewsBlur。这是一个很大的安慰，因为如果你已经使用这种服务，就不必人工导入 feed，这节省了你的工作。 NewsFlash 恰好是 FeedReadeer 的精神继承者，原来的 FeedReader 开发人员也参与其中。 如果你正在找适用的 RSS 阅读器，我们整理了 Linux Feed 阅读器 列表供您参考。 ","date":"2021-04-13","objectID":"/newsflash/:0:0","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":"NewsFlash: 一款补充网页 RSS 阅读器账户的 Feed 阅读器 请注意，NewsFlash 并不只是针对基于网页的 RSS feed 账户量身定做的，你也可以选择使用本地 RSS feed，而不必在多设备间同步。 不过，如果你在用是任何一款支持的基于网页的 feed 阅读器，那么 NewsFlash 特别有用。 这里，我将重点介绍 NewsFlash 提供的一些功能。 ","date":"2021-04-13","objectID":"/newsflash/:1:0","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":"NewsFlash 功能 支持桌面通知 快速搜索、过滤 支持标签 便捷、可重定义的键盘快捷键 本地 feed OPML 文件导入/导出 无需注册即可在 Feedly 库中轻松找到不同 RSS Feed 支持自定义字体 支持多主题（包括深色主题） 启动/禁止缩略图 细粒度调整定期同步间隔时间 支持基于网页的 Feed 账户，例如 Feedly、Fever、NewsBlur、feedbin、Miniflux 除上述功能外，当你调整窗口大小时，还可以打开阅读器视图，这是一个细腻的补充功能。 账户重新设置也很容易，这将删除所有本地数据。是的，你可以手动清除缓存并设置到期时间，并为你关注的所有 feed 设置一个用户数据存在本地的到期时间。 ","date":"2021-04-13","objectID":"/newsflash/:2:0","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":"在 Linux 上安装 NewsFlash你无法找到适用于各种 Linux 发行版的官方软件包，只有 Flatpak。 对于 Arch 用户，可以从 AUR 下载。 幸运的是，Flatpak 软件包可以让你轻松在 Linux 发行版上安装 NewsFlash。具体请参阅我们的 Flatpak 指南。 你可以参考 NewsFlash 的 GitLab 页面 去解决大部分问题。 ","date":"2021-04-13","objectID":"/newsflash/:3:0","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":"结束语我现在用 NewsFlash 作为桌面本地解决方案，不用基于网页的服务。你可以通过直接导出 OPML 文件在移动 feed 应用上得到相同的 feed。这已经被我验证过了。 用户界面易于使用，也提供了数一数二的新版 UX。虽然这个 RSS 阅读器看似简单，但提供了你可以找到的所有重要功能。 你怎么看 NewsFlash？你喜欢用其他类似产品吗？欢迎在评论区中分享你的想法。 via: https://itsfoss.com/newsflash-feedreader/ 作者：Ankush Das 选题：lujun9972 译者：DCOLIVERSUN 校对：wxy 本文由 LCTT 原创编译，Linux中国 荣誉推出 ","date":"2021-04-13","objectID":"/newsflash/:4:0","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":"没有提交原始 Dockerfile 的时候，可以通过逆向工程查看镜像构建过程。","date":"2021-04-05","objectID":"/reverse-engineering-a-docker-image/","tags":["LCTT","Docker","逆向工程"],"title":"一次 Docker 镜像的逆向工程","uri":"/reverse-engineering-a-docker-image/"},{"categories":["LCTT"],"content":"这要从一次咨询的失误说起：政府组织 A 让政府组织 B 开发一个 Web 应用程序。政府机构 B 把部分工作外包给某个人。后来，项目的托管和维护被外包给一家私人公司 C。C 公司发现，之前外包的人（已经离开很久了）构建了一个自定义的 Docker 镜像，并将其成为系统构建的依赖项，但这个人没有提交原始的 Dockerfile。C 公司有合同义务管理这个 Docker 镜像，可是他们他们没有源代码。C 公司偶尔叫我进去做各种工作，所以处理一些关于这个神秘 Docker 镜像的事情就成了我的工作。 幸运的是，Docker 镜像的格式比想象的透明多了。虽然还需要做一些侦查工作，但只要解剖一个镜像文件，就能发现很多东西。例如，这里有一个 Prettier 代码格式化 的镜像可供快速浏览。 首先，让 Docker 守护进程daemon拉取镜像，然后将镜像提取到文件中： docker pull tmknom/prettier:2.0.5 docker save tmknom/prettier:2.0.5 \u003e prettier.tar 是的，该文件只是一个典型 tarball 格式的归档文件： $ tar xvf prettier.tar 6c37da2ee7de579a0bf5495df32ba3e7807b0a42e2a02779206d165f55f1ba70/ 6c37da2ee7de579a0bf5495df32ba3e7807b0a42e2a02779206d165f55f1ba70/VERSION 6c37da2ee7de579a0bf5495df32ba3e7807b0a42e2a02779206d165f55f1ba70/json 6c37da2ee7de579a0bf5495df32ba3e7807b0a42e2a02779206d165f55f1ba70/layer.tar 88f38be28f05f38dba94ce0c1328ebe2b963b65848ab96594f8172a9c3b0f25b.json a9cc4ace48cd792ef888ade20810f82f6c24aaf2436f30337a2a712cd054dc97/ a9cc4ace48cd792ef888ade20810f82f6c24aaf2436f30337a2a712cd054dc97/VERSION a9cc4ace48cd792ef888ade20810f82f6c24aaf2436f30337a2a712cd054dc97/json a9cc4ace48cd792ef888ade20810f82f6c24aaf2436f30337a2a712cd054dc97/layer.tar d4f612de5397f1fc91272cfbad245b89eac8fa4ad9f0fc10a40ffbb54a356cb4/ d4f612de5397f1fc91272cfbad245b89eac8fa4ad9f0fc10a40ffbb54a356cb4/VERSION d4f612de5397f1fc91272cfbad245b89eac8fa4ad9f0fc10a40ffbb54a356cb4/json d4f612de5397f1fc91272cfbad245b89eac8fa4ad9f0fc10a40ffbb54a356cb4/layer.tar manifest.json repositories 如你所见，Docker 在命名时经常使用哈希hash。我们看看 manifest.json。它是以难以阅读的压缩 JSON 写的，不过 JSON 瑞士军刀 jq 可以很好地打印 JSON： $ jq . manifest.json [ { \"Config\": \"88f38be28f05f38dba94ce0c1328ebe2b963b65848ab96594f8172a9c3b0f25b.json\", \"RepoTags\": [ \"tmknom/prettier:2.0.5\" ], \"Layers\": [ \"a9cc4ace48cd792ef888ade20810f82f6c24aaf2436f30337a2a712cd054dc97/layer.tar\", \"d4f612de5397f1fc91272cfbad245b89eac8fa4ad9f0fc10a40ffbb54a356cb4/layer.tar\", \"6c37da2ee7de579a0bf5495df32ba3e7807b0a42e2a02779206d165f55f1ba70/layer.tar\" ] } ] 请注意，这三个层Layer对应三个以哈希命名的目录。我们以后再看。现在，让我们看看 Config 键指向的 JSON 文件。它有点长，所以我只在这里转储第一部分： $ jq . 88f38be28f05f38dba94ce0c1328ebe2b963b65848ab96594f8172a9c3b0f25b.json | head -n 20 { \"architecture\": \"amd64\", \"config\": { \"Hostname\": \"\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"--help\" ], \"ArgsEscaped\": true, \"Image\": \"sha256:93e72874b338c1e0734025e1d8ebe259d4f16265dc2840f88c4c754e1c01ba0a\", 最重要的是 history 列表，它列出了镜像中的每一层。Docker 镜像由这些层堆叠而成。Dockerfile 中几乎每条命令都会变成一个层，描述该命令对镜像所做的更改。如果你执行 RUN script.sh 命令创建了 really_big_file，然后用 RUN rm really_big_file 命令删除文件，Docker 镜像实际生成两层：一个包含 really_big_file，一个包含 .wh.really_big_file 记录来删除它。整个镜像文件大小不变。这就是为什么你会经常看到像 RUN script.sh \u0026\u0026 rm really_big_file 这样的 Dockerfile 命令链接在一起——它保障所有更改都合并到一层中。 以下是该 Docker 镜像中记录的所有层。注意，大多数层不改变文件系统镜像，并且 empty_layer 标记为 true。以下只有三个层是非空的，与我们之前描述的相符。 $ jq .history 88f38be28f05f38dba94ce0c1328ebe2b963b65848ab96594f8172a9c3b0f25b.json [ { \"created\": \"2020-04-24T01:05:03.608058404Z\", \"created_by\": \"/bin/sh -c #(nop) ADD file:b91adb67b670d3a6ff9463e48b7def903ed516be66fc4282d22c53e41512be49 in / \" }, { \"created\": \"2020-04-24T01:05:03.92860976Z\", \"created_by\": \"/bin/sh -c #(nop) CMD [\\\"/bin/sh\\\"]\", \"empty_layer\": true }, { \"created\": \"2020-04-29T06:34:06.617130538Z\", \"created_by\": \"/bin/sh -c #(nop) ARG BUILD_DATE\", \"empty_layer\": true }, { \"created\": \"2020-04-29T06:34:07.020521808Z\", \"created_by\": \"/bin/sh -c #(nop) ARG VCS_REF\", \"empty_layer\": true }, { \"created\": \"2020-04-29T06:34:07.36915054Z\", \"created_by\": \"/bin/sh -c #(nop) ARG VERSION\", \"empty_layer\": true }, { \"created\": \"2020-04-29T06:34:07.708820086Z\", \"created_by\": \"/bin/sh -c #(nop) ARG REPO_NAME\", \"empty_layer\": true }, { \"created\": \"2020-04-29T06:34:08.06429638Z\", \"created_by\": \"/bin/sh -c #(nop) LABEL org.label-schema.vendor=tmknom org.label-schema.name=tmknom/prettier org.label-schema.description=Pre","date":"2021-04-05","objectID":"/reverse-engineering-a-docker-image/:0:0","tags":["LCTT","Docker","逆向工程"],"title":"一次 Docker 镜像的逆向工程","uri":"/reverse-engineering-a-docker-image/"},{"categories":["LCTT"],"content":"字典数据结构可以帮助你快速访问信息。","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"字典是 Python 编程语言使用的数据结构。一个 Python 字典由多个键值对组成；每个键值对将键映射到其关联的值上。 例如你是一名老师，想把学生姓名与成绩对应起来。你可以使用 Python 字典，将学生姓名映射到他们关联的成绩上。此时，键值对中键是姓名，值是对应的成绩。 如果你想知道某个学生的考试成绩，你可以从字典中访问。这种快捷查询方式可以为你节省解析整个列表找到学生成绩的时间。 本文介绍了如何通过键访问对应的字典值。学习前，请确保你已经安装了 Anaconda 包管理器 和 Jupyter 笔记本。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:0:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"1、在 Jupyter 中打开一个新的笔记本首先在 Web 浏览器中打开并运行 Jupyter。然后， 转到左上角的 “File”。 选择 “New Notebook”，点击 “Python 3”。 Create Jupyter notebook 开始时，新建的笔记本是无标题的，你可以将其重命名为任何名称。我为我的笔记本取名为 “OpenSource.com Data Dictionary Tutorial”。 笔记本中标有行号的位置就是你写代码的区域，也是你输入的位置。 在 macOS 上，可以同时按 Shift + Return 键得到输出。在创建新的代码区域前，请确保完成上述动作；否则，你写的任何附加代码可能无法运行。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:1:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"2、新建一个键值对在字典中输入你希望访问的键与值。输入前，你需要在字典上下文中定义它们的含义： empty_dictionary = {} grades = { \"Kelsey\": 87, \"Finley\": 92 } one_line = {a: 1, b: 2} Code for defining key-value pairs in the dictionary 这段代码让字典将特定键与其各自的值关联起来。字典按名称存储数据，从而可以更快地查询。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:2:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"3、通过键访问字典值现在你想查询指定的字典值；在上述例子中，字典值指特定学生的成绩。首先，点击 “Insert” 后选择 “Insert Cell Below”。 Inserting a new cell in Jupyter 在新单元格中，定义字典中的键与值。 然后，告诉字典打印该值的键，找到需要的值。例如，查询名为 Kelsey 的学生的成绩： # 访问字典中的数据 grades = { \"Kelsey\": 87, \"Finley\": 92 } print(grades[\"Kelsey\"]) 87 Code to look for a specific value 当你查询 Kelsey 的成绩（也就是你想要查询的值）时，如果你用的是 macOS，只需要同时按 Shift+Return 键。 你会在单元格下方看到 Kelsey 的成绩。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:3:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"4、更新已有的键当把一位学生的错误成绩添加到字典时，你会怎么办？可以通过更新字典、存储新值来修正这类错误。 首先，选择你想更新的那个键。在上述例子中，假设你错误地输入了 Finley 的成绩，那么 Finley 就是你需要更新的键。 为了更新 Finley 的成绩，你需要在下方插入新的单元格，然后创建一个新的键值对。同时按 Shift+Return 键打印字典全部信息： grades[\"Finley\"] = 90 print(grades) {'Kelsey': 87; \"Finley\": 90} Code for updating a key 单元格下方输出带有 Finley 更新成绩的字典。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:4:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"5、添加新键假设你得到一位新学生的考试成绩。你可以用新键值对将那名学生的姓名与成绩补充到字典中。 插入新的单元格，以键值对形式添加新学生的姓名与成绩。当你完成这些后，同时按 Shift+Return 键打印字典全部信息： grades[\"Alex\"] = 88 print(grades) {'Kelsey': 87, 'Finley': 90, 'Alex': 88} Add a new key 所有的键值对输出在单元格下方。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:5:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"使用字典请记住，键与值可以是任意数据类型，但它们很少是扩展数据类型non-primitive types。此外，字典不能以指定的顺序存储、组织里面的数据。如果你想要数据有序，最好使用 Python 列表，而非字典。 如果你考虑使用字典，首先要确认你的数据结构是否是合适的，例如像电话簿的结构。如果不是，列表、元组、树或者其他数据结构可能是更好的选择。 via: https://opensource.com/article/21/3/dictionary-values-python 作者：Lauren Maffeo 选题：lujun9972 译者：DCOLIVERSUN 校对：wxy 本文由 LCTT 原创编译，Linux中国 荣誉推出 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:6:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":null,"content":" 各位看官，如果本站内容对您有帮助，欢迎赞助我一杯咖啡☕️ 毕竟，来都来了 😁 支付宝 微信支付 ","date":"2021-03-31","objectID":"/donate/:0:0","tags":null,"title":"Donate","uri":"/donate/"},{"categories":["LCTT"],"content":"开启技术翻译道路","date":"2021-03-31","objectID":"/lctt-foreword/","tags":["LCTT","开源"],"title":"LCTT项目序言","uri":"/lctt-foreword/"},{"categories":["LCTT"],"content":"什么是LCTTLCTT 是 “Linux中国” 的翻译组，负责从国外优秀媒体翻译 Linux 相关的技术、资讯、杂文等内容。 ","date":"2021-03-31","objectID":"/lctt-foreword/:1:0","tags":["LCTT","开源"],"title":"LCTT项目序言","uri":"/lctt-foreword/"},{"categories":["LCTT"],"content":"加入 LCTT 的初衷前不久我参与了《On Java 8》的翻译工作。在这个过程中，我需要查询很多专业名词的翻译，往往纠结句子中个别单词怎么翻译比较好。虽然有痛苦，但也为我带来欢乐。我热衷于揣摩作者的表达意图，常常想他是在怎样的技术储备下写出这本书。更重要的是，在翻译过程中我对 Java 有了新的认识，而这些认识也是我之前看中文文献时疑惑的点。 不难看出，这次翻译工作为我带来了技术上的成长，也让我看到个人的能力不足。读研期间，导师让我阅读原始文献，避免受到翻译软件、译者的干扰，直接学习作者表达的内容。我想，在未来的工作中，我的阅读侧重点也应该倾向于原始文献。于是，我开始查找国内有没有优秀的国外文献阅读、学习的社区，LCTT 就是这个时候映入我的眼帘。 LCTT 社区中的文章由专业的选题人员选择，一般为技术访谈、博客等。后期由译者翻译、校对人员审核后再发布到 Linux中国社区。我作为译者参与社区工作，选择自己感兴趣的文章，翻译后提交给社区。我的工作让我见识到国外优秀的技术文章，它们介绍了实用的代码工具、翔实的技术总结、完整的调试过程，这些都可以拓宽我的技术视野，提升我的技术实力。同时，我也希望我的工作能为国内的开发者提供一些帮助，让大家在技术分享中共同进步。 ","date":"2021-03-31","objectID":"/lctt-foreword/:2:0","tags":["LCTT","开源"],"title":"LCTT项目序言","uri":"/lctt-foreword/"},{"categories":["LCTT"],"content":"我的工作展示欢迎大家访问我的 LCTT 主页 👉 Qian.Sun 我负责的所有文章均在主页中列出 ","date":"2021-03-31","objectID":"/lctt-foreword/:3:0","tags":["LCTT","开源"],"title":"LCTT项目序言","uri":"/lctt-foreword/"},{"categories":["LCTT"],"content":"特别说明 本博客中 LCTT 专栏仅转载本人翻译的文章； 翻译工作和译文发表仅用于学习和交流目的； 翻译工作遵照 CC-BY-NC-SA 协议规定，如果我的工作有侵犯到您的权益，请及时联系我； 转载敬请在正文中标注并保留原文/译文链接和作者/译者等信息； LCTT 专栏内所有译文由 LCTT 原创翻译，Linux 中国首发。 ","date":"2021-03-31","objectID":"/lctt-foreword/:4:0","tags":["LCTT","开源"],"title":"LCTT项目序言","uri":"/lctt-foreword/"},{"categories":null,"content":"我是谁大家好！👏 我是Qian Sun。如今居住在北京。 现在在阿里云从事数据库开发工作。 研究生就读于中国科学院计算技术研究所，获得了硕士学位。本科就读大连海事大学，获得了工学学士学位。 ","date":"2021-03-30","objectID":"/about/:0:1","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"博客会有哪些内容博客中会分享我对计算机技术的整理与见解、我在职场中的成长与感悟。 还会有我对前沿论文的解读、对大型工程的源码剖析。 ","date":"2021-03-30","objectID":"/about/:0:2","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"沟通交流最重要我十分期待你的反馈！:) 如果你不认同我博客中的内容，一定要联系我！让我们在沟通中共同成长进步！ 可以通过首页中展示的任何渠道联系到我☎️ ","date":"2021-03-30","objectID":"/about/:0:3","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"我的分享我会列出我最近的十次分享(可能也没有十次😅) 标题 活动 地点 时间 链接 轻松进大厂的简单方法 我把经验留下来 中科院计算所 2021.2.27 Slide ","date":"2021-03-30","objectID":"/about/:0:4","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"推荐的博客我会列出优秀的技术博客📝，这些博主都是我学习的榜样。他们的技术实力与工作履历十分精彩。 Cizixs ice1000 io-meter ","date":"2021-03-30","objectID":"/about/:0:5","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"简历我会不定期更新我的简历，大家可以关注我的领英: Qian Sun ","date":"2021-03-30","objectID":"/about/:0:6","tags":null,"title":"About me","uri":"/about/"},{"categories":["Java"],"content":"本文介绍了 Java 线程池中所有参数配置项与要求，针对不同业务场景提供对应的配置建议","date":"2021-03-30","objectID":"/java-concurrency-3/","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"配置线程池大小线程池的理想大小取决于被提交任务的类型以及所部署系统的特性。在实际工程中，通常不会固定线程池大小，应该通过某种配置机制来提供，或者根据 Runtime.availableProcessors 来动态计算。 在配置线程池大小时，应该避免“过大”和“过小”这两种极端情况。如果线程池过大，那么大量的线程将在相对很少的 CPU 和内存资源上发生竞争，不仅消耗更高的内存，而且还可能耗尽资源。如果线程池过小，将导致许多空闲处理器无法执行工作，从而降低吞吐率。 对于计算密集型的任务，在拥有 $N_{cpu}$ 个处理器系统上，当线程池的大小为 $N_{cpu}+1$ 时，通常能实现最优的利用率。 对于包含 I/O 操作或者其他阻塞操作的任务，由于线程并不会一直执行，因此线程池的规模应该更大。要正确设置线程池大小，你必须估算出任务的等待时间与计算时间的比值。这种估算不需要很精确，并且可以通过一些分析或监控工具来获得。 要使处理器达到期望的使用率，线程池的最优大小等于： $$N_{threads}=N_{cpu}\\times U_{cpu}\\times \\left( 1+\\frac{W}{C} \\right)$$ 式中，$N_{cpu}$ 代表 CPU 的数量，$U_{cpu}$ 代表 CPU 目标利用率，$U_{cpu}\\in [0,1]$，$\\frac{W}{C}$ 代表等待时间与计算时间的比值。 可以通过 Runtime 来获得 CPU 的数目： int N_CPUS = Runtime.getRuntime().availableProcessors(); CPU 周期并不是唯一影响线程池大小的资源，还包括内存、文件句柄、套接字句柄和数据库连接等。计算这些资源对线程池的约束条件是更容易的： 计算每个任务对该资源的需求量； 用该资源的可用总量除以每个任务的需求量，所得结果就是线程池大小上限。 ","date":"2021-03-30","objectID":"/java-concurrency-3/:1:0","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"配置 ThreadPoolExecutor public ThreadPoolExecutor (int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u003cRunnable\u003e workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { ... } ","date":"2021-03-30","objectID":"/java-concurrency-3/:2:0","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"线程的创建与销毁以下三个参数主要负责线程的创建与销毁： corePoolSize，线程池基本大小，即在没有任务执行时线程池的大小，并且只有在工作队列满了的情况下才会创建超出这个数量的线程。 maximumPoolSize，线程池最大大小，表示可同时活动的线程数量的上限。 keepAliveTime，线程池存活时间。如果某个线程的空闲时间超过了存活时间，那么将被标记为可回收的。 线程池的基本大小、最大大小和存储时间等因素共同负责线程的创建与销毁。当线程池的当前大小超过了基本大小时，被标记为可回收的线程将被终止。通过调节基本大小和存活时间，可以帮助线程池回收空闲线程占有的资源，从而使得这些资源可以用于执行其他工作。 ","date":"2021-03-30","objectID":"/java-concurrency-3/:2:1","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"管理队列任务ThreadPoolExecutor 允许提供一个 BlockingQueue 来保存等待执行的任务。基本的任务排队方法有 3 种：无界队列、有界队列和同步移交（Synchronous Handoff）。 ArrayBlockingQueue：一个基于数组结构的有界阻塞队列，按照 FIFO 原则对任务进行排序； LinkedBlockingQueue：一个基于链表结构的无界阻塞队列，按照 FIFO 原则排序任务，吞吐量通常要高于 ArrayBlockingQueue。 SynchronousQueue：一个不存储元素的线程间同步移交，要将一个元素放入 SynchronousQueue 中，必须有另一个线程正在等待接受这个元素。如果没有线程正在等待，并且线程池的当前大小小于最大值，那么 ThreadPoolExecutor 将创建一个新的线程，否则根据饱和策略，这个任务被拒绝。 PriorityBlockingQueue：一个具有优先级的有界阻塞队列，这个队列根据优先级来安排任务，任务的优先级是通过自然顺序或 Comparator 来定义的。 ","date":"2021-03-30","objectID":"/java-concurrency-3/:2:2","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"饱和策略当有界队列被填满后，饱和策略开始发挥作用。ThreadPoolExecutor 的饱和策略可以通过调用 setRejectedExecutionHandler 来修改。JDK 提供了几种不同的实现，每种实现包含不同的饱和策略： ThreadPoolExecutor.AbortPolicy：中止策略是默认的饱和策略，该策略将在线程池数量等于最大线程数时，抛出未检查的 RejectedExecutionException。涉及到的任务将不会执行。调用者可以捕获这个异常，然后根据需求编写自己的处理代码。 ThreadPoolExecutor.CallerRunsPolicy：既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量。它不会在线程池的某个线程中执行新提交的任务，而是在一个调用了 execute 的线程中执行该任务。 ThreadPoolExecutor.DiscardPolicy：该策略在线程池中数量等于最大线程数时，会悄悄丢弃不能执行的新增任务，不报任何异常； ThreadPoolExecutor.DiscardOldestPolicy：该策略在线程池中数量等于最大线程数时，会抛弃线程池中工作队列头部的任务（即等待时间最久的任务），并执行当前任务。 ","date":"2021-03-30","objectID":"/java-concurrency-3/:2:3","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"线程工厂每当线程池需要创建一个线程时，都是通过线程工厂方法来完成的。默认的线程工厂方法将创建一个新的、非守护的线程，并且不包含特殊的配置信息。通过指定一个线程工厂方法，可以定制线程池的配置信息。 定制线程池配置信息在某些场景下有需求，例如希望为线程池中的线程指定一个 UncaughtExecptionHandler，或者实例化一个定制的 Thread 类用于执行调试信息的记录。 下面的例子展示了自定义线程工厂为每个创建的线程池设置更有意义的名字，在 Debug 和定位问题时非常有帮助。 public class MyThreadFactory implements ThreadFactory { private final String poolName; public MyThreadFactory(String poolName) { this.poolName = poolName; } public Thread newThread(Runnable runnable) { return new MyAppThread(runnable, poolName); } } ","date":"2021-03-30","objectID":"/java-concurrency-3/:2:4","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"Executor框架的执行策略对线程池性能造成的影响随着业务场景不同而变化，本文介绍了四类任务对执行策略、线程池性能的影响","date":"2021-03-19","objectID":"/java-concurrency-2/","tags":["Java","并发"],"title":"任务执行策略与线程池性能","uri":"/java-concurrency-2/"},{"categories":["Java"],"content":" ThreadPool Executor框架的执行策略可以将任务的提交与执行解耦开来，为任务的制定、执行提供了相当大的灵活性。但并非所有的任务都适用于Executor框架执行策略，有些任务需要明确的指定执行策略。 依赖性任务。依赖性任务注重任务之间的执行顺序。如果线程池执行依赖性任务，需要隐含为执行策略带来约束，避免产生活跃性问题。 使用线程封闭机制的任务。任务要求 Executor 是单线程的，如果将 Executor 从单线程环境改为线程池环境，将会失去线程安全性。 对响应时间敏感的任务。这类任务需要及时响应。如果将一个运行时间较长的任务提交到单线程的 Executor 中，或者将多个运行时间较长的任务提交到一个只包含少量线程的线程池中，那么将降低该 Executor 管理的服务的响应性。 使用ThreadLocal任务。ThreadLocal 使每个线程都拥有某个变量的一个私有“版本”。只要条件允许，Executor 可以自由地重用这些线程。只有当线程本地值的生命周期受限于任务生命周期时，在线程池的线程中使用 ThreadLocal 才有意义，而在线程池的线程中不应该使用 ThreadLocal 在任务之间传递值。 只有当任务都是同类型且相互独立时，线程池的性能才能达到最佳。 ","date":"2021-03-19","objectID":"/java-concurrency-2/:0:0","tags":["Java","并发"],"title":"任务执行策略与线程池性能","uri":"/java-concurrency-2/"},{"categories":["Java"],"content":"线程饥饿死锁依赖性任务可能造成线程池死锁。线程池中如果所有正在执行任务的线程都因等待其他仍处于工作队列中的任务而阻塞，就会引发线程饥饿死锁Thread Starvation Deadlock。 ","date":"2021-03-19","objectID":"/java-concurrency-2/:1:0","tags":["Java","并发"],"title":"任务执行策略与线程池性能","uri":"/java-concurrency-2/"},{"categories":["Java"],"content":"运行时间较长的任务如果任务阻塞的时间过长，线程池的响应性也会变得糟糕。此外，运行时间较长的任务也会增加短任务的服务时间。 有一项技术可以缓解执行时间较长任务造成的影响，即限定任务等待资源的时间。在平台类库的大多数可阻塞方法中，都同时定义了限时版本和无限时版本，例如 Thread.join、BlockingQueue.put、CountDownLatch.await 以及 Selector.select 等。如果等待超时，那么可以把任务标识为失败，然后中止任务或重新返回队列以便随后执行。 ","date":"2021-03-19","objectID":"/java-concurrency-2/:2:0","tags":["Java","并发"],"title":"任务执行策略与线程池性能","uri":"/java-concurrency-2/"},{"categories":["杂谈分享"],"content":"避免租房踩坑","date":"2021-03-15","objectID":"/house-renting/","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"毕业季总会为我们带来各种感受，既有步入新环境的兴奋，也有为生活琐事操心的烦忧。在生活琐事中，住房问题永远是大家最关注的点。大家希望在有所居的基础上，享受洁净、明亮、宽敞的居住空间。 本文是我将租房经验加以整理、抽象得出的租房注意事项，欢迎大家补充。如果您的补充被本文采纳，我会在下方列出您的名字表示您对本文的贡献👇 注意 感谢以下同学为本文的贡献： Qian Sun 张兵 Changhao Liu Rain ","date":"2021-03-15","objectID":"/house-renting/:0:0","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"房源信息总体来说，房源信息来源分为两类：房东直租与中介介绍。 ","date":"2021-03-15","objectID":"/house-renting/:1:0","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"房东直租房东可以将自己的出租信息挂在网上，例如高校论坛、豆瓣、闲鱼等，租客也可以直接去小区、小区物业去打听此类信息。房东直租可以省去中介费的负担，在租金上可能也有优惠。不过需要大家擦亮眼睛，明辨出租房是否为业主所有，检查房中的水电、家居情况，与房东协商一系列使用、维修注意事项。 网上的直租信息存在中介钓鱼的可能性，需自行判断对方身份。 ","date":"2021-03-15","objectID":"/house-renting/:1:1","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"中介介绍房东直接委托中介办理房屋出租，租客可以将自己的需求明确告诉中介，中介为你推荐合适的房子。这类方式比较省事，中介可以帮你过滤不合适的信息，替你与业主协商使用、维修注意事项，如有需要也会帮你谈价格。租客承担中介费一般为一个月租金，续签时候的中介费问题需要看各个中介公司的情况。这种方式可以快速匹配租客与房源，省心省力。 上海租房可以尝试“六六直租”APP，但需要筛选房源。 ","date":"2021-03-15","objectID":"/house-renting/:1:2","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"个人建议如果你没有租房经验，建议找一家大型中介为你找房子，中介费权当你进入租房、购房市场的学费了。原因如下： 中介手中掌握大量房源信息，可以根据你的需求帮你快速定位。毕业季租房市场火爆，快速找到适合自己的房子是关键的第一步。 中介可以为你争取到市场上绝大多数租客可以享受到的服务，例如维修责任方、必要的家具。 双方交付房子时，中介会帮助你检查房中一切情况并登记在案，退房退押金时可作为依据。 有正规的租赁合同，省去找人看合同的麻烦。 如果有租房经验，可以直接去小区或者网上寻找房东直租信息，省去中介费岂不美滋滋。 ","date":"2021-03-15","objectID":"/house-renting/:1:3","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"看房关注的点","date":"2021-03-15","objectID":"/house-renting/:2:0","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"风险信息 核实出租人身份，查看身份证与房产证信息页是否匹配 检查是否有权利瑕疵，例如抵押、查封、拍卖等等 检查房屋是否非法改造，主体结构是否安全，是否动了承重墙，具体方法为看房产证 检查房屋是否存在空气质量问题 约定好违约行为，一般是市场默认的违约行为 异地租房一定要现场确认房屋情况，不要轻易确定 定金最好不要交，直接签合同交租金。如果需要交定金，一定不要轻易交 ","date":"2021-03-15","objectID":"/house-renting/:2:1","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"房屋质量 房屋的布局是否合理，是否符合你的要求 房屋采光如何，能否保证你希望的足够采光时长、强度。更详细请看建筑采光设计标准 房屋是否漏水，楼上是否漏水 房屋取暖方式，优先集体供暖 房屋燃气管、燃气阀、燃表情况，是否需要更换 房屋窗户是否漏风 家具是否齐全，家具功能是否正常 房屋电路是否正常，虽然插座出问题概率小，但必要时仍需检查 长期不住人的房屋更要小心仔细检查 房屋的隔音条件，周边环境是否有噪音 ","date":"2021-03-15","objectID":"/house-renting/:2:2","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"安全问题 小区、住宅楼的安全管理情况如何 ","date":"2021-03-15","objectID":"/house-renting/:2:3","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"费用问题 付款频率，一般来说是押一付三 退房返押金的标准，需要交付时候与房东确认好房屋细节 维修费用的承担，租客原因租客承担，非租客原因需要商量好 物业费、车位费：物业费房东承担，车位费没了解过 水、电、燃气、宽带、取暖费用租客承担 约定好违约金 ","date":"2021-03-15","objectID":"/house-renting/:2:4","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"其他有用的点 把自己收拾的干净一些，让房东相信你可以照顾好他的房子 明确和房东、中介讲清自己的需求，一开始约定好比后面追加条件要容易多 房东喜欢稳定、可长租的租客，可以表现你的工作、收入稳定 情侣比朋友合租更有优势 房东希望尽快入住，避免房屋空闲，签约速度也是房东选择租客的要素之一 ","date":"2021-03-15","objectID":"/house-renting/:3:0","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"总结要是没有经验，推荐找中介。有后续风险问题都可以找中介解决，第三信任方是交易顺利完成的保障。自己在看房子的时候需要仔细、仔细、仔细检查风险、房屋质量、安全与费用等问题。谈好的条件一定要求落实在合同中。 最后祝大家找到自己满意的房子。如果你有其他问题，可以从首页中任意联系方式找到我。 如果你有其他建议，欢迎分享给我，方式如上👆 我会添加在文中，并留下你的姓名表示感谢。 ","date":"2021-03-15","objectID":"/house-renting/:4:0","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["Java"],"content":"本文介绍了Executor框架，框架采用了任务提交、执行的解耦方案。为了让该方案适配不同场景，需要将多种因素考虑进执行策略中。不同的执行策略也衍生出不同的线程池，我们在使用前需要分析真实环境去选择适当的线程池。线程池异步执行多个任务，导致任务可能处于不同的状态。为了管理整个线程池的生命周期，ExecutorService提供了多种方法，一般采取awaitTermination、shutdown组合使用的方式，达到同步关闭的效果。最后，本文介绍了ScheduledThreadPool在延迟任务、周期任务的优越性，如果构建调度服务，可以采用DelayQueue。","date":"2021-03-12","objectID":"/java-concurrency-1/","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"Executor框架我们将执行的逻辑工作单元抽象为任务，那么线程就是使任务异步执行的容器。如果把所有任务放在单个线程执行，将对应用的响应性和吞吐量造成灾难性影响。 为每个任务分配一个线程似乎是不错的解决方案，不过这对资源管理有着较高的要求。线程池缓解了这一压力，它承担了管理线程工作。java.util.concurrent 提供了一种灵活的线程池作为Executor框架的一部分。 在 Java 类库中，任务执行的主要抽象不是 Thread，而是 Executor。 public interface Executor { void execute (Runnable command); } Executor 为灵活且强大的异步任务执行框架提供了基础，为任务的提交与执行提供了标准的方法，将两个过程解耦开来。此外，Executor 实现了对生命周期的支持，以及监控管理等机制。 Executor 基于生产者-消费者模式，提交任务的操作相当于生产者，执行任务的操作类似于消费者。反之，如果想实现简单的生产消费模型，可以采用 Executor 实现。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:1:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"执行策略Executor 框架需要设计一套任务执行策略，以解决多任务执行混乱的问题。执行策略包括以下内容： 在什么线程中执行任务？ 任务按照什么顺序执行（FIFO、LIFO、优先级）？ 有多少个任务能并发执行？ 在队列中有多少个任务在等待执行？ 如果系统需要拒绝一个任务，应该选择哪个任务？如何通知应用有任务被拒绝？ 在执行一个任务前后，应该进行哪些动作？ ","date":"2021-03-12","objectID":"/java-concurrency-1/:2:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"线程池线程池是管理诸多线程的资源池，依靠任务保持所有等待执行的任务。工作者线程Work Thread的任务很简单：从工作队列获取一个任务，执行任务，然后返回线程池并等待下一个任务。 为每个任务分配一个线程可能引入线程新建、销毁的开销，不如让多个任务在线程池中执行。通过重用现有的线程可以分摊多个线程的开销。此外，任务不会因等待线程创建而延迟执行，提升整体响应性。用户可以通过配置线程池大小，创建足够多的线程使处理器保持忙碌状态，还可防止多线程相互竞争资源而使应用程序耗尽内存或失败。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:3:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"newFixedThreadPoolnewFixedThreadPool 是拥有固定数量线程的线程池，每当提交一个任务就创建一个线程，直到达到最大数量。如果某个线程因异常结束，那么线程池会补充一个新线程。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:3:1","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"newCachedThreadPoolnewCachedThreadPool 是可缓存的线程池，如果线程池当前规模超过处理需求，将回收空闲线程，而当需求增加时，可以添加新线程。线程池规模不存在任何限制。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:3:2","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"newSingleThreadExecutornewSingleThreadExecutor 是单线程的 Executor，如果线程因异常结束，newSingleThreadExecutor 会创建另一个线程来替代。任务按照队列顺序而执行。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:3:3","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"newScheduledThreadPoolnewScheduledThreadPool 的线程数量也是固定的，但可以以延迟或定时方式执行任务。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:3:4","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"Executor 的生命周期Executor 通常会创建线程来执行任务，只有当所有线程全部终止后才会退出，如果无法正确关闭 Executor，JVM 将无法结束。 因为 Executor 以异步方式执行任务，可能引发任务状态不同步的问题。同一时刻，不同任务可能处于执行、完成、等待执行三个状态。为了解决执行任务的生命周期问题，Executor 扩展了 ExecutorService 接口，添加了一些用于生命周期管理的方法。 public interface ExecutorService extends Executor { void shutdown(); List\u003cRunnable\u003e shutdownNow(); boolean isShutdown(); boolean isTerminated(); boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // ... } ExecutorService 的生命周期有三种状态：运行、关闭和已终止。shutdown 方法将平缓地关闭 ExecutorService：不再接受新任务，同时等待已经提交的任务执行完成——包括还未开始执行的任务。shutdownNow 方法将粗暴地关闭 ExecutorService：直接取消所有执行中的任务，并且不再启动队列中尚未开始执行的任务。 awaitTermination 方式可以等待 ExecutorService 到达终止状态，或者调用 isTerminated 来轮询 ExecutorService 是否已经终止。通常在调用 awaitTermination 方法后立即调用 shutdown，从而产生同步关闭的效果。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:4:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"延迟任务与周期任务Timer 类负责管理延迟任务以及周期任务。但 Timer 类支持的是基于绝对时间的调度机制，对系统时钟变化的容忍度很低，存在天然缺陷。ScheduledThreadPool 只支持基于相对时间的调度，可以通过 ScheduledThreadPool 的构造函数或 newScheduledThreadPool 工厂方法来创建该类的对象。 如果构建调度服务，可以使用 DelayQueue，它实现了 BlockingQueue，并为 ScheduledThreadPoolExecutor 提供调度功能。DelayQueue 管理着一组 Delayed 对象。每个 Delayed 对象都有一个相应的延迟时间。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:5:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"总结本文一开始介绍了 Executor 框架，框架采用了任务提交、执行的解耦方案。为了让该方案适配不同场景，需要将多种因素考虑进执行策略中。不同的执行策略也衍生出不同的线程池，我们在使用前需要分析真实环境去选择适当的线程池。线程池异步执行多个任务，导致任务可能处于不同的状态。为了管理整个线程池的生命周期，ExecutorService 提供了多种方法，一般采取 awaitTermination、shutdown 组合使用的方式，达到同步关闭的效果。最后，本文介绍了 ScheduledThreadPool 在延迟任务、周期任务的优越性，如果构建调度服务，可以采用 DelayQueue。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:6:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["读书笔记"],"content":"思维混乱常常导致我们决策失误，戴上六顶思考帽，专注思考方向，全面、快速、清晰地思考","date":"2021-02-14","objectID":"/six-thinking-hats/","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["读书笔记"],"content":" 封面 ","date":"2021-02-14","objectID":"/six-thinking-hats/:0:0","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["读书笔记"],"content":"关于本书 关于本书 ","date":"2021-02-14","objectID":"/six-thinking-hats/:1:0","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["读书笔记"],"content":"内容简介本书介绍了一种思考模式，可以应对探索性问题或者单类型问题。 六顶思考帽代表一种思维方式。每次思考时专注于一顶帽子，避免出现思维混乱的情况。 通过训练、使用，可以全面、客观、快速认清问题，并提出解决方案。 ","date":"2021-02-14","objectID":"/six-thinking-hats/:2:0","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["读书笔记"],"content":"书摘 思维导图总结 ","date":"2021-02-14","objectID":"/six-thinking-hats/:3:0","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["读书笔记"],"content":"读后感 思考是人类最大的宝藏，但非常容易出现思考混乱，情感、信息、逻辑、希望和创意搅在一起。 可以不必关注“是什么”，关注“能够怎样”，辩证法告诉我们角度不同、结论不同。 现代新事物涌现太快，需要更快速、全面地认清新事务，这套方法比较适用。 经验告诉我，使用这套方法应该是润物细无声的方式，生拉硬拽会适得其反。 ","date":"2021-02-14","objectID":"/six-thinking-hats/:4:0","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["论文学习"],"content":"自治DBMS可以降低DBA工作负担，为企业带来数据驱动决策的便利。该文提出Peloton DBMS自治架构，并认为在深度神经网络、新硬件和高性能数据库架构下，自治DBMS是可以实现的","date":"2021-01-22","objectID":"/self-driving-dbms/","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"注意 自治 DBMS 可以降低 DBA 工作负担，为企业带来数据驱动决策的便利。该文提出 Peloton DBMS 自治架构，并认为在深度神经网络、新硬件和高性能数据库架构下，自治 DBMS 是可以实现的。 原文在这里 👉 Self-Driving DBMS ","date":"2021-01-22","objectID":"/self-driving-dbms/:0:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"摘要过去，研究员和供应商都搭建了查询工具去帮助 DBA 实现系统调优、物理设计。然而，之前大部分工作是不完整的，因为 DBA 仍无法摆脱数据库更改的裁定工作，并且在问题发生后仍需要采取应对措施。 真正的“自治”数据库管理系统Database Manage System所需要的是一种为自治而设计的新架构。这与以前的工作不同，因为系统所有方面都受集成规划组件控制，该组件不仅优化系统以适应当前负载，也预测未来的负载趋势，以便系统能够相应地进行准备。这样，DBMS 就可以支持所有以前的调优技术，而不需要人力确定正确的方式和适当的时间去部署它们。它还支持一些对现代高性能 DBMS 很重要的优化，这点在今天是很难的，因为管理这些系统的复杂性已经突破专家的能力上限。 本文介绍了第一代 Self-Driving DBMS——Peloton 的架构。由于深度学习算法的进步以及硬件、自适应数据库架构的改进，Peloton 的自主能力现在有了可能性。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:1:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"介绍从 1970 年以来，关系模型和声明式查询语言就以消除数据管理负担为卖点。40年后，DBMS 变得更加复杂，功能越来越多。使用现有的自动调优工具是一项繁重的任务，因为它们需要费力准备工作负载样本、空闲的硬件来测试更新，最重要的原因是需要直观了解 DBMS 内部结构。如果 DBMS 可以自动完成这些事，那么它将消除部署数据库所涉及的许多复杂性和成本。 以前关于调优系统的工作关注点在针对数据库单个方面的独立工具上。例如，一些工具能够选择数据库的最佳逻辑或物理设计，如索引、分区方案、数据组织或物化视图。其他工具可以为应用选择调优参数。这些工具中的大多数都以相同的方式操作：DBA 为其提供样本数据库和工作负载跟踪，以指导搜索过程去找到最佳或接近最佳的配置。主要的 DBMS 供应商（包括 Oracle、Microsoft 和 IBM）都以这种方式操作。最近有一种推动集成组件支持自适应架构的趋势，但这同样只专注于解决一个问题。同样，云厂商使用动态资源分配服务，不会对单个数据库进行调优。 所有这些对一个完全自治的系统来说都是不够的，因为它们在 DBMS 之外，不能同时解决多个问题。也就是说，它们从系统外部观察 DBMS 的行为，并在问题发生后建议 DBA 如何修正问题。调优工具假定操作它们的人有足够的知识，可以在特定时间内更新 DBMS，不对应用产生大影响。然而，数据库领域在过去十年中发生了巨大的变化，我们不能假定 DBMS 是由一个了解数据库优化的专家部署而成。再说，即使这些这些工具可以实现自治， DBMS 架构也会在重大更新时给 DBMS 施加很大压力，也无法突破未来的瓶颈。 本文中，作者证明了自治数据库系统是可以实现的。下文首先会讨论该系统所面临的主要挑战。然后，作者提出了 Peloton 的架构，以及使用 Peloton 中集成深度学习框架的测量结果。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:2:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"问题概述自治 DBMS 面临的第一个挑战是理解应用的负载。最基本的级别是将查询定义为 OLTP 或 OLAP 应用。如果 DBMS 确定了应用属于二者中的哪一类，那么它就可以决定如何优化数据库。例如，如果是 OLTP，DBMS 应该将元组存储在面向行的布局中，并为写进行优化。如果是 OLAP，那么 DBMS 使用面向列的布局，这样更适应访问表列子集的只读查询 。处理这个问题的一种方法是部署专门用于 OLTP 和 OLAP 负载的独立 DBMS，然后在它们之间定期流更新。但是这不适合 HTAP，因为它在数据由 OLTP 写入时就执行 OLAP 查询，所以无法将 DBMS 独立部署开。更好的方法是部署一个支持混合 HTAP 负载的 DBMS 。这种系统会自动为不同数据库段选择适当的 OLTP 或 OLAP 优化。 除了要理解应用的负载，DMS 也需要预测资源利用趋势。这帮助它能够预测未来需求和部署优化，同时对性能影响最小。许多程序的使用模式密切契合人类的日常生活。这也是为什么 DBA 会在非高峰时间安排更新，以避免正常业务时间的服务中断。不可否认，有些工作负载异常是 DBMS 无法预料的。但这些模型可以预先警告，使得 DBMS 能够比外部监控系统更快地实施缓解措施。 现在， DBMS 可以依靠这些模型去确定数据库调优与优化操作，以更好应对预期的工作负载。自治 DBMS 不支持 DBA 的任务，这些任务需要系统外部信息，比如权限、数据清洗和版本控制。如下表所示，自治 DBMS 可以支持三种优化类型。第一个是数据库物理设计，第二个是数据组织的修改，最后是影响 DBMS 的运行时行为。对于每一个优化操作，DBMS 将需要评估它们对数据库的潜在影响。这些评估不仅包括行动部署后消耗的资源，还包括 DBMS 部署行动时消耗的资源。 Types Actions PHYSICAL Indexes AddIndex, DropIndex, Rebuild, Convert PHYSICAL Materialized Views AddMatView, DropMatView PHYSICAL Storage Layout Row👉Columnar, Columnar👉Row, Compress DATA Location MoveUpTier, MoveDownTier, Migrate DATA Partitioning RepartitionTable, ReplicateTable RUNTIME Resources AddNode, RemoveNode RUNTIME Configuration Tuning IncrementKnob, DecrementKnob, SetKnob RUNTIME Query Optimizations CostModelTune, Compilation, Prefetch 即使系统能够预测程序的工作负载，选择要使用的操作 ，并确定实施操作的最佳时间，仍然存在额外的挑战。如果 DBMS 不能有效地应用这些优化，没有带来较大的性能下降，那么系统将无法快速适应变化。这也是目前自治 DBMS 不可能实现的另一个原因。如果系统只能每周更新一次，那么它很难规划如何纠正系统。因此，论文认为需要一个灵活的基于内存的 DBMS 体系结构，可以在部署过程中逐步优化，而不会对应用程序产生可察觉的影响。 最后，一个自治 DBMS 有两个额外的约束，它必须关联如今的应用程序。首先，DBMS 不能要求开发人员重写应用代码以适应自治 DBMS。第二，不能依赖只支持某些编程环境的程序分析工具。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:3:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"自治架构研发发现现有 DBMS 对于自治操作过于笨重 ，因为它们需要在更改时重新启动，而且上表中的许多操作太慢了。因此，DBMS 需要一个崭新的架构，对集成的自治组件有更全面、更细致的控制 。 接下来描述 Peloton 架构中的组件。Peloton 架构是一种全新的架构，而不是改造现有的 DBMS（例如，Postgres/MySQL）。最重要的是，它使用了多版本并发控制，在不阻塞 OLAP 查询的前提下交叉 OLTP 事务和操作。另一个特点是，它使用了一个内存存储管理器，具有无锁的数据结构和灵活的布局，可以快速执行 HTAP 工作负载。这些设计已经使我们能够支持 Peloton 的优化操作。 Peloton架构 Peloton 的自治流程如上图所示。除了环境设置（比如内存阈值与目录路径），论文的目标是让Peloton 在没有任何人为提供的指导信息的情况下高效运行。系统自动学习如何提高应用程序查询和事物的延迟。延迟是 DBMS 中最重要的度量标准，因为它全面代表性能情况。本文的其余部分假设延迟是主要优化目标。可以为分布式环境中的其他重要指标添加额外的约束，例如服务成本和资源。 Peloton 包含一个嵌入式监视器，跟踪系统的内部事件流的执行查询。每个查询条目都标注了其资源利用率。流还定期被 DBMS/OS 遥测数据和优化操作的开始/结束事件打断。然后，DBMS 根据这些监控数据为应用程序的预期工作负载构建预测模型。它使用这些模型来识别瓶颈和其他问题（例如，缺少索引、超载节点），然后选择最佳操作。系统执行此操作的同时仍然处理应用程序的常规工作负载，并收集新的监控数据，以了解这些操作如何影响其性能。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:4:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"工作负载分类第一个组件是 DBMS 的集群器，它使用无监督学习方法对具有类似特征的应用程序查询进行分类。集群工作负载减少了 DBMS 维护的预测模型数量，从而使预测应用程序的行为更容易（也更准确）。Peloton 的初始实现使用 DBSCAN 算法。这种方法已被用于对静态 OLTP 工作负载进行聚集操作。 这种集群的一大问题是使用什么查询特征。这两种类型的特性是查询的时间度量和查询的逻辑语义。虽然前者使 DBMS 能够更好地聚集类似的查询，且不需要理解它们的含义，但它们对数据库内容或其物理设计设计的变化更敏感。即使数据库没有更改，在高并发工作负载中也可能发生类似的问题。另一个方法是根据逻辑执行计划的结构（例如表、谓词）对查询进行分类，这些特征独立于数据库的内容及其物理设计。这些特征是否会产生集群以生成良好的预测模型还有待观察，或者运行时指标的准确性是否超过了再训练的成本。运行时度量可能会使系统在更短的时间内收敛到稳定的状态，因此系统不必经常重新训练它的预测模型。或者，即使集群经常变化，硬件加速训练也能使系统以最小 的开销加速重建模型。 下一个问题是如何确定集群何时失效。当这种情况发生时，DBMS 必须重新构建集群，这可能打乱已有的分类，并要求它重新训练所有的预测模型。Peloton 使用标准交叉验证技术来确定集群的错误率何时超过阈值。DBMS 还可以利用操作对查询的影响来决定何时重建集群。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:4:1","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"负载预测下一步是训练预测模型，预测每个工作负载集群的查询抵达率。除了异常热点外，这种预测使系统能够识别工作负载周期性和数据增长趋势，为负载波动做好准备。在 DBMS 执行一个查询后，它用它的集群标识符标记每个查询，然后填充一个直方图📊，该直方图跟踪在一段时间内每个集群收到的查询数量。Peloton 使用这些数据来训练预测模型，预测应用程序未来将执行的每个集群的查询数。DBMS 还为事件流中的其他 DBMS/OS 指标构建了类似的模型。 以前在自治系统方面的尝试使用了 auto-regressive-moving average 模型（ARMA）来预测在云中自动伸缩的 web 服务工作负载。ARMA 可以捕获时间序列数据中的线性关系，但它们通常需要一个人来识别模型中的差分顺序和 term 的数量。此外，线性假设对于许多数据库工作负载可能并不有效，因为它们受到外部因素的影响。 RNN 是预测非线形系统时间序列模式的一种有效方法。LSTM 是 RNN 的一种变体，允许网络学习时间序列数据中的周期性和重复趋势，而常规的 RNN 无法做到这一点。LSTM 包含一些特殊的 block，用于决定是否保留旧的信息以及何时将其输出到网络中。尽管 RNN 被吹捧为能够解决许多以前难以解决的问题，但仍需要研究如何使其适用于自治 DBMS。 RNN 的准确性也依赖于它的训练集数据的大小。但是跟踪在 DBMS 中执行的每个查询都会增加模型构建的计算成本。幸运的是，我们没有必要知道将来查询的确切数量。相反，Peloton 为每个组维护多个 RNN，它们在不同的时间范围和间隔粒度上预测工作负载。尽管这些粗粒度的 RNN 不准确，但它们减少了 DBMS 必须维护的训练数据和运行时的预测成本。组合多个 RNN 使 DBMS 能够处理对精确度要求更高的即时问题，也能够适应评估范围更广的长期计划。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:4:2","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"行动计划与执行最后一个组件是 Peloton 的控制框架，能够持续监控系统和选择优化行动，以提高应用程序的性能。这就是自治组件和 DBMS 架构紧密耦合的最明显好处，因为它使不同的部分能够相互提供反馈。作者也相信有机会在系统更多部分中使用强化学习，包括并发控制和查询优化。 行动生成：系统搜索可能提高性能的操作。Peloton 将这些操作存储在一个目录中，并记录在操作调用的历史。这种搜索是由预测模型指导的。它还可以删除冗余操作，降低搜索复杂度。 在需求竞争低时，Peloton 可以用更多的核去完成动作部署，在竞争高时，只能用更少的核。 行动计划：现在有了行动目录，DBMS 根据预测、当前数据库配置和目标函数去选择部署哪个行动。滚动时域控制模型receding-horizon control model，RHCM的基本思想：在每个时间单元，系统使用预测来估计某个有限时域的工作负载。然后，它会搜索动作，以最小化目标函数。但它只能应用于第一个动作，在下个时间单元重复流程前只好等待部署完成。这就是高性能 DBMS 至关重要的原因。如果动作在几分钟内完成，则系统不必监视工作负载是否已经转移，并决定中止正在执行的动作。 在 RHCM 下，计算过程被建模为一棵树，其中每一层包含 DBMS 可以调用每个操作的时间。该系统通过估算行动的成本效益来搜索树，并选择最低成本的行动序列。也可以选择在一个时间单元内不执行任何操作。一种降低这个过程复杂性的方法是，在搜索树的更深层次随机选择要考虑的行动，而不是评估所有可能的行动。这种抽样会加上权重，以便更有可能考虑为数据库当前状态及其预期工作负载提供最优行动。它还避免了最近调用的行动，但后来系统取消这条规则。 一个行动的成本是对部署它所需的时间估计，以及在这段时间内 DBMS 性能会下降多少。由于以前没有部署过很多行动，因此不可能总是从以前的历史记录生成这些信息。系统使用分析模型来评估每个操作类型的成本，然后通过反馈机制自动改进它们。这样的好处是在部署行动后查询的延迟时间发生了变化。这个好处来自 DBMS 的内部查询计划器成本模型。它是加权计算的行动部署后查询样本延迟改进的总和，权重是预测模型预测的期望查询完成率。 除了上述的成本-收益分析之外，系统还必须评估一个行动如何随时间推移影响 Peloton 的内存使用。任何导致 DBMS 超过内存阈值的行动都需要舍弃。 重要的是，系统在考虑行动时应该考虑多长时间跨度。太短会阻止 DBMS 及时为即将到来的负载峰值做好准备，但太长会使 DBMS 无法缓解突然出现的问题，因为模型太慢。此外，由于计算每个时间纪元的成本-收益是昂贵的，它可能创建另一个深度学习网络来近似它们值函数。 部署：Peloton 支持非阻塞方法部署行动。例如，重新组织表的布局或将其移动到不同的位置并不会阻止查询该表。有些操作，如添加索引，需要特别考虑。这样 DBMS 就不会因为在操作进行时数据被修改，导致任何错误发生。 DBMS 还从其集成的机器学习组件处理资源调度和竞争问题。使用单独的协处理器或 GPU 来处理繁重的计算任务，可以避免降低 DBMS 的速度。否则，DBMS 将不得不使用一台单独的机器，专门用于所有的预测和计算组件。这将使系统的设计复杂化，并由于协调而增加额外的开销。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:4:3","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"其他注意事项要让自治 DBMS 得到广泛应用，还需要克服一些非技术挑战。最重要的是 DBA 不愿意将数据库控制权交给自动化系统。为了简化过渡，自治 DBMS 可以以可视化方式公开其决策过程。例如，如果它选择添加一个索引，可以向 DBA 解释，它的模型表明当前的工作负载与过去某个时间点相似，这个时间点使用这样的索引是有好处的。 还必须支持来自 DBA 的提示，说明系统是否应该更多地关注工作负载的 OLTP 或 OLAP 部分。类似的，对于多租户部署，系统需要知道是否应该对每个数据库进行相同的调优，或者一个数据库是否比其他数据库更重要。 最后，可能需要为 DBA 提供一个覆盖机制。人类发起的改变被当作其他一样的行动，Peloton 记录操作历史来决定该行动是否有益。唯一的区别是系统不允许逆转。为了防止 DBA 做出的错误决策被永久保留，DBMS 可以要求 DBA 手动设置生命周期。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:4:4","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"初步结果作者在 Peloton 中集成了 Tensorflow，使用从一个在线讨论网站一个月的流量数据中提取 5200 万个查询来训练两个 RNN。使用 75% 的数据去训练模型，25% 数据去验证模型。在输入上应用了两个堆叠的 LSTM 层，然后连接到一个线形回归层，对这些层使用 10% 的 Drop 以避免过拟合。 第一个模型以分钟为粒度去预测下一小时将到达的查询数量。该模型收入是一个向量，表示过去两小时内每分钟的工作负载，而输出是一个标量，表示一小时后预测的工作负载。第二个模型使用 24 小时时域，粒度为 1 小时。 RNN与真实负载数据的比较结果 两个RNN的训练时间分别为 11 和 18 分钟，从下图可看出，模型能够以 11.3% 的错误率预测 1 小时后的工作负载，以 13.2% 的错误率预测一天后的。对于计算开销，前者大约是 2MB，DBMS 探测每个模型以获得新预测需要 2ms，向其添加一个新的数据点需要 5ms。 使用这些模型，可以在 Peloton 中实现数据优化操作。根据访问这些表的查询类型，将表迁移到不同布局中。每个表的“热”元组存储在行布局中，已经针对 OLTP 优化，而同一张表中“冷”元组存储在列布局中，已经针对 OLAP 优化。使用 HTAP 工作负载，白天执行 OLTP 操作，晚上执行 OLAP 查询。当启动自动布局时，在 Peloton 中执行查询序列，并将其与静态的行和列布局进行比较。 自动交叉布局与静态布局在HTAP应用中的性能比较 从上图可看出，Peloton 随着时间推移收敛到一种适合工作负载的布局。在第一段后，DBMS 将行元组迁移到列布局，适应 OLAP。接下来，当工作负载转移到 OLTP 查询时，自治 DBMS 比静态系统更好，因为它执行的内存写操作更少。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:5:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"总结随着大数据运动兴起，对自治 DBMS 的需求越来越大。这类系统将降低人力成本在部署任何规模数据库上的浪费，并使各组织更容易享受数据驱动决策带来的便利。论文概述了 Peloton DBMS 的自治架构。作者认为，由于深度神经网络、新硬件和高性能数据库架构，自治 DBMS 系统是可以实现的。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:6:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"Lakehouse架构支持开放的数据格式、机器学习，提供卓越的性能，是第三代数据分析平台的代表","date":"2021-01-06","objectID":"/lakehouse/","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"注意 Lakehouse架构逐渐在工业界铺开，第三代数据分析平台进入大众视野！ 原文在这里 👉 Lakehouse ","date":"2021-01-06","objectID":"/lakehouse/:0:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"摘要论文认为数据仓库架构在未来一段时间内会逐渐消亡，取而代之的是一种新型 Lakehouse架构，该架构具有如下特特性： 基于开放的数据格式，例如 Apache Parquet 完全支持机器学习和数据科学 提供卓越的性能 Lakehouse 解决数据仓库面临的主要挑战——数据陈旧、可靠性不高、总成本大、数据格式受限、场景支持受限。论文下面会讨论 Lakehouse 架构为何取得工业界青睐以及如何影响数据管理。 ","date":"2021-01-06","objectID":"/lakehouse/:1:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"数据分析平台发展数据仓库将业务数据库的数据收集到集中式仓库，帮助企业领导分析数据，之后被用于决策支持和商业智能Business Intelligence。数据仓库使用写模式schema-on-write写入数据，优化下游BI消费的数据模型。这就是第一代数据分析平台。 第一代数据分析平台 后来第一代平台开始面临诸多挑战。首先是计算与存储耦合使得扩容成本增加，这迫使企业支付用户负载和数据管理峰值的成本，这个成本随着数据规模增加而迅速增加。其次，越来越多的数据集是非结构化的，例如视频、音频和文本文档，而数据仓库无法存储、查询这类数据。 为了解决这些问题，第二代数据分析平台将所有原始数据导入数据湖：具有文件 API 的低成本存储系统，该系统可存储开放数据格式，例如 Apache Parquet 和 ORC。这个方法源起于 Apache Hadoop，基于 HDFS 实现低成本存储。数据湖是一种读模式schema-on-read架构，可以灵活、低成本地存储数据，也解决了数据质量和下游管理的问题。该架构中的一小部分数据在进行 ETL 后注入下游数据仓库，再进行决策支持和 BI 分析。开放数据格式使得绝大多数分析引擎可以直接访问数据湖数据。 第二代数据分析平台（数据湖+数据仓库） 2015年起，云数据湖（S3、ADLS、GCS、OSS等）开始取代HDFS，它们具有超强的持久性、冗余可靠、超低存储成本。云上架构与第二代平台架构相同，例如Redshift、Snowflake。数据湖 + 数据仓库 两层架构当今在工业界中占主导地位。 如今，这种架构面临新的挑战。尽管存储和计算的分离使得云数据湖 + 数据仓库架构的成本降低，可增加了用户的使用成本。在第一代平台中，所有业务数据库中的数据经过ETL后直接注入数据仓库。第二代平台却在中间引进了数据湖，增加了额外的复杂性、延迟与故障率。同时，数据湖 + 数据仓库二层架构不能很好支持机器学习之类的高级分析。具体来看，可以归纳为四个问题： 可靠性。保持数据湖与数据仓库的一致性是成本高昂且困难的事情。需要对两个系统之间的 ETL 作业进行仔细设计，如此方可进行高性能决策支持与 BI 分析。每个ETL步骤还有发生故障或引入错误的风险，例如由于数据湖和数据仓库引擎之间的细微差别而导致数据质量降低的风险。 数据陈旧。数据仓库数据的时效性低于数据湖数据，新数据的加载通常要花费几天。与第一代分析系统相比，这是个倒退，第一代分析系统可以直接查询新的业务数据。根据 Dimensional Research 与 Fivetran 调查，86% 的分析使用过时数据，62% 的报告每月需要等待几次引擎资源。 对高级分析支持有限。企业希望使用数据进行预测，例如“我应该为哪些顾客提供折扣？”。 尽管许多研究关注机器学习与数据管理结合，但主流机器学习系统没有一个可以工作在数据仓库上，包括 TensorFlow、PyTorch 和 XGBoost。与 BI 查询少量数据不同，这些机器学习系统需要使用复杂的 No-SQL 代码处理大型数据集，但通过 ODBC/JDBC 读取数据效率很低，并且无法直接访问数据仓库内部专有格式的数据。对于这类场景，数据仓库供应商建议导出数据为文件，但这增加了复杂性和滞后性，因为添加了第三个 ETL。或者，用户可以在支持开发格式的数据湖上运行这些系统，这会抛弃数据仓库丰富的数据管理功能，例如 ACID 事务、数据版本控制与索引。 总成本。除了支付 ETL 作业费用外，用户还得为复制到数据仓库的数据支付两倍的存储成本，而数据仓库使用的内部格式额外引入数据或工作负载迁移到其他系统的成本。 一种被广泛采用的解决方案是不使用数据湖 ，将所有数据存储在计算、存储分离的数据仓库中。论文认为这种方案可行性有限，因为不支持视频、音频和文本数据或从机器学习和数据科学工作负载中直接访问。 论文作者提出了一个问题：是否可以将基于开放数据格式（Parquet 与 ORC）的数据湖转为一个高性能系统，该系统既拥有数据仓库强大的性能、管理功能，又可直接、快速访问高级分析工作负载？随着越来越多的业务应用开始依赖运营数据和高级分析，Lakehouse 架构可以消除数据仓库的上述挑战。 作者相信 Lakehouse 的时机已经到来！ Lakehouse架构 Lakehouse可解决以下问题： 数据湖上可靠的数据管理：Lakehouse 需要存储原始数据，同时支持 ETL/ELT 流程来提高数据分析质量。传统数据湖将半结构化数据以“一堆文件”形式进行管理，很难提供一些简化ETL/ELT的关键管理功能，例如事务、回滚、零拷贝。然而，以 Delta Lake 和 Apache Iceberg 为代表的新型数据湖框架提供了数据湖的事物视图，并提供了管理功能，减少 ETL 步骤，并且分析人员可以高效查询原始数据表，这与第一代分析平台很像。 支持机器学习和数据科学：机器学习系统支持直接读取数据湖数据格式，很多系统采用 DataFrames 作为操作数据的抽象，而声明式 DataFrame APIs 可以为机器学习工作负载中的数据访问进行查询优化，可以让机器学习工作负载直接享受 Lakehouse 的优化点。 SQL性能：Lakehouse 需要在海量 Parquet/ORC 数据集上提供很好的 SQL 性能，相比之下经典数据仓库对 SQL 优化更彻底。尽管如此，论文提出需要维护 Parquet/ORC 数据集的辅助数据，在现有格式内优化数据布局以实现更好的性能。 ","date":"2021-01-06","objectID":"/lakehouse/:2:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"出发点：数据仓库的挑战当前工业界对数据仓库不是很满意。首先是数据质量和可靠性不高，维护数据流分析的准确性是一件很困难的工作。其次，越来越多的商业分析需要最新的数据，但数据仓库不可避免地引入数据滞后性。第三，如今的非结构化数据比重大幅增加，但数据仓库并不能提供很好的非结构化数据分析。最后，现在工业界部署的机器学习与数据科学应用无法从数据仓库和数据湖中得到很好的支持。 当前工业界对数据湖 + 数据仓库的两层架构并不满意。首先是几乎所有的数据仓库近期都增加了对 Parquet 和 ORC 格式的外部表支持，允许数据仓库用户可以从相同的 SQL 引擎查询数据湖表，但这没有降低数据湖管理难度，也没有消除数据仓库 ETL 复杂度、滞后性和高级分析挑战。实际上，这些支持的性能通常较差，因为 SQL 引擎主要针对其内部数据格式进行了优化。其次，直接针对数据湖存储的 SQL 引擎也有广泛产品，例如 Spark SQL、Presto、Hive 和 AWS Athena。然而，这些引擎不能解决数据湖所有问题，也不能取代数据仓库，数据湖仍然缺少包括 ACID 事务的基础管理功能和有效访问方法，例如与数据仓库性能匹配的索引。 ","date":"2021-01-06","objectID":"/lakehouse/:3:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"Lakehouse 架构论文为 Lakehouse 提出一个定义：基于低成本、直接访问存储的数据管理系统，该系统具有传统分析型 DBMS 管理和性能，例如 ACID 事务、数据版本管理、数据审计、索引、缓存和查询优化。可以看出，Lakehouse 结合了数据湖和数据仓库的核心优势。问题的关键在于是否可以有效结合这些优势，特别是 Lakehouse 对直接访问的支持意味着其放弃了部分数据独立性。 Lakehouse 天然适合计算、存储分离的云环境：不同的计算应用程序按需分配在完全独立的计算节点（例如 ML 的 GPU 集群），同时直接访问相同的存储数据，但也可以在本地存储系统（如 HDFS）上实现 Lakehouse。 ","date":"2021-01-06","objectID":"/lakehouse/:4:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"实现 Lakehouse 系统实现 Lakehouse 的第一个关键思想是使用标准文件格式（如 Parquet）将数据存储在低成本的对象存储（如 Amazon S3、OSS）中，并在对象存储上实现元数据层，其定义了哪些对象是表版本一部分。这使系统可以在元数据层实现如 ACID 事务处理或版本控制之类的管理功能，同时将大量数据保存在低成本对象存储中，并允许客户端使用使用标准文件格式直接从该存储中读取对象。 尽管元数据层增加了管理功能，但不足以实现良好的 SQL 性能。数据仓库使用多种技术获得性能提升，比如将热数据存储在 SSD 等高速设备、维护统计信息、构建有效的访问方法（如索引）以及优化数据格式和计算引擎。基于现有存储格式的 Lakehouse 无法变更格式，但是也可以实现保持数据文件不变情况下的其他优化，包括缓存、辅助数据结构（例如索引和统计信息）和数据布局优化。 最终，Lakehouse 既可以加快高级分析负载，又可以为其提供更好的数据管理功能。许多机器学习库（如 Tensorflow 和 Spark MLlib）已经可以读取数据湖文件格式（如 Parquet）。因此将它们与 Lakeehouse 集成最简单方法是查询元数据层，查询哪些 Parquet 文件属于表，然后将它们传递给机器学习库。这些系统支持 DataFrame API，以便进行更好的优化。R 与 Pandas 推广了 DataFrames，为用户提供包含多种操作符的表抽象，其中大多数映射到关系代数。Spark SQL 等系统通过惰性计算转换与传递结果操作步骤到优化器实现该 API 声明式。因此，这些 API 可利用 Lakehouse 新优化特性实现机器学习加速，例如缓存和辅助数据。 APIs and Lakehouse ","date":"2021-01-06","objectID":"/lakehouse/:4:1","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"用于数据管理的元数据层Lakehouses 的第一个组件是元数据层，其可以实现 ACID 事务和其他管理功能。诸如 S3 或 HDFS 之类的数据湖存储系统仅提供了低级的对象存储或文件系统接口，在这些接口中，即使是简单的操作（如更新跨多个文件的表）也不是原子的，这个问题使得一些组织开始设计更丰富的数据管理层，从 Apache Hive ACID 开始，其使用 OLTP DBMS 跟踪给定表版本中哪些数据文件是 Hive 表的一部分，并允许操作以事务方式更新此集合。近年来一些新系统提供了更多功能和改进的可伸缩性，如 2016 年 Databricks 开发的 Delta Lake，其将有关哪些对象是表中一部分的信息存储在数据湖中，作为 Parquet 格式的事务日志，使其能够扩展到每张表数十亿个对象；Netflix 的 Apache Iceberg 也使用类似的设计，并支持 Parquet 和 ORC 存储；Apache Hudi 始于 Uber 也类似，尽管它不支持并发写入（正在支持中），该系统侧重于简化流式数据入数据湖。 这些系统的经验表明它们可以提供与原始 Parquet/ORC 数据湖类似或更好的性能，同时还增加了非常有用的管理功能，例如事务处理，零拷贝和回滚。 元数据层对数据质量非常重要，例如可以对 Schema 进行校验，使其不破坏数据质量。 另外元数据层可以实现诸如访问控制和审核日志记录之类的治理功能，例如元数据层可以在授予客户端凭据以从云对象存储读取表中的原始数据之前，检查是否允许客户端访问表，并且记录所有访问行为。 未来方向和替代设计。由于数据湖的元数据层非常新，因此存在许多悬而未决的问题和替代设计。例如 Delta Lake 设计为将事务日志存储在它运行的同一对象存储中（例如 S3）以简化管理（消除了运行单独存储系统的需要）并提供高可用性和高读取带宽，但对象存储的高延迟限制了它可以支持的每秒事务处理速率，在某些情况下将元数据使用更快的存储系统的设计可能更可取。同样Delta Lake、Iceberg 和 Hudi 仅支持单表事务，但也可以扩展以支持跨表事务，优化事务日志的格式和管理对象的大小也是未解决的问题。 ","date":"2021-01-06","objectID":"/lakehouse/:4:2","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"Lakehouse 中的 SQL 性能Lakehouse 方案的最大技术问题可能是如何提供最新的 SQL 性能，同时又放弃了传统 DBMS 设计中很大一部分的数据独立性，有很多解决方案，例如可以在对象存储上添加一个缓存层，以及是否可以更改数据对象存储格式而不使用现有的标准（例如 Parquet 和 ORC（不断改进这些格式的新设计不断涌现））。无论采用何种设计，核心挑战在于数据存储格式已成为系统公共 API 的一部分以允许快速直接访问，这与传统 DBMS 不同。 我们提出了几种技术可以在 Lakehouse 中优化 SQL 性能，并且与数据格式无关，因此可以将其与现有格式或未来数据格式一起使用，这些与格式无关的优化大致如下： 缓存：使用元数据层时，Lakehouse 系统可以安全地将云对象存储中的文件缓存在处理节点上更快的存储设备（例如 SSD 和 RAM）上，正在运行的事务可以确定读取缓存的文件是否还有效，此外缓存可以采用转码格式，其对于查询引擎运行效率更高，例如在 Databricks 的缓存会解压了部分它加载的 Parquet 数据。 辅助数据：即使 Lakehouse 为支持直接 I/O 访问需要开放表存储格式（如 Parquet），它也可以维护其他数据来帮助优化查询，如在 Parquet 文件中维护表中每个数据文件的列最小-最大统计信息，有助于跳过数据，以及基于 Bloom 过滤器的索引。可以实现各种各样的辅助数据结构，类似于为\"原始\"数据建立索引。 数据布局：数据布局在访问性能中起着重要作用。Lakehouse 系统也可以优化多个布局决策，最明显的是记录排序：哪些记录聚集在一起可以最容易被批量读取，Delta 中使用 Z-Order，Hudi 中使用基于哪些列进行 Clustering。 对于分析系统中的典型访问模式，这三个优化可以很好地协同工作。典型的工作负载中大多数查询倾向于集中在数据的\"热\"子集上，Lakehouse 可以使用与数据仓库相同的优化数据结构对其进行缓存，以提供相同的查询性能。对于云对象存储中的\"冷\"数据，性能的主要决定于每个查询读取的数据量，在该情况下数据布局优化（将共同访问的数据聚类）和辅助数据结构（如区域图，使引擎快速确定要读取的数据文件范围）的组合可以使Lakehouse系统与数仓一样最小化 I/O 开销，尽管使用标准的开放文件格式（相比于数仓内置文件格式）。 性能结果 TPC-DS比较 未来方向和替代设计。设计性能良好且可以直接访问的 Lakehouse 系统是未来工作的重点。一个尚待探索的方向是设计更好适应此类场景的数据湖存储格式，例如为 Lakehouse 系统实现数据布局优化或索引提供更大灵活性的存储格式，或者更适合现代硬件。 即使不改变数据格式，也有许多缓存策略、辅助数据结构和数据布局策略。哪一种对云对象存储中的海量数据集更有效是一个开放式问题。 最后，另一个值得研究的点是确定何时以及如何使用 serverless 计算系统来响应查询、优化存储、元数据层和查询引擎，以实现延迟最小化效果。 ","date":"2021-01-06","objectID":"/lakehouse/:4:3","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"高级分析高效访问高级分析库通常不是使用 SQL 命令编写，其需要访问大量数据。作者认为需要研究以下问题：如何设计数据访问层，最大程度地提高顶部运行代码的灵活性，并且可以从 Lakehouse 的优化中受益？ 优化方案 作者验证了如上图所示的优化方案：将缓存、数据筛选、数据布局优化同时应用，实现了机器学习算法加速。 机器学习 API 迅速发展，但是一些数据访问 API（例如 TensorFlow 的 tf.data）没有尝试将查询语义推入底层存储系统，一些 API 还专注于 CPU 到 GPU 的传输和 GPU 计算，这在数据仓库中并未引起太多关注。 未来方向和替代设计。论文提出需要多关注机器学习系统的数据访问接口。近期一些机器学习框架将算法逻辑融合进 SQL join 操作，其他应用在算法中的查询优化应用在 SQL 中。最后，作者认为需要标准机器学习接口以使数据科学家能够充分利用 Lakehouse（甚至数据仓库）中强大的数据管理功能，如事务，数据版本控制和回滚等。 ","date":"2021-01-06","objectID":"/lakehouse/:4:4","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"研究问题和启示Lakehouse 还提出了其他一些研究问题，功能日益丰富的数据湖的行业趋势也对数据系统研究的其他领域产生了影响。 还有其他方法可以实现 Lakehouse 目标吗？可以想像其他方法来实现 Lakehouse 的主要目标，例如构建用于数据仓库的大规模并行服务层，可以支持对高级分析工作负载的并行读取，但是与工作负载直接访问对象存储库相比成本将更高，难以管理，并且性能可能会降低。这种服务层并未得到广泛应用，例如 Hive LLAP。 除了在性能、可用性、成本和锁定方面的挑战外，还有一些重要的管理原因，如企业可能更喜欢将数据保留为开放格式。随着对数据管理的法规要求不断提高，组织可能需要在短时间内搜索旧数据集，删除各种数据或更改其数据处理基础结构，并且采用开放格式进行标准化意味着它们将始终可以直接访问数据，软件行业的长期趋势一直是开放数据格式，企业数据应该继续保持这种趋势。 什么是正确的存储格式和访问 API？Lakehouse 的访问接口包括原始存储格式以及直接读取此格式的客户端库（例如使用 TensorFlow 读取时）以及高级 SQL 接口。有很多不同的方法可以在这些层上放置丰富的功能，例如通过要求读者执行更复杂的“可编程”解码逻辑，可以为系统提供更大的灵活性的存储方案。有待观察哪种存储格式、元数据层设计和访问 API 的组合效果最佳。 Lakehouse 如何影响其他数据管理研究和趋势？数据湖的流行以及对丰富管理接口的使用不断增加，无论它们是元数据层还是完整的 Lakehouse 设计，都对数据管理研究的其他领域产生了影响。 Polystore 旨在解决跨不同存储引擎查询数据这一难题，该问题在企业中持续存在，但是在云数据湖中以开放格式提供的数据比例越来越高，也可以通过直接针对云对象存储运行许多 polystore 查询，即使基础数据文件是逻辑上分开的 Lakehouse 的一部分。 还可以在 Lakehouse 上设计数据集成和清理工具，并可以快速并行访问所有数据，这可以用于大型联接和聚类等新算法。 可以将 HTAP 系统构建为 Lakehouse 前面的\"附加\"层，通过使用其事务管理 API 将数据直接归档到 Lakehouse 系统中，Lakehouse 将能够查询数据的一致快照。 ML 的数据管理也会变得更加简单和强大，如今组织正在构建各种可重新实现标准 DBMS 功能的，特定于 ML 的数据版本控制和特征存储系统，使用带有内置 DBMS 管理功能的数据湖来实现特征存储功能可能会更简单。 Serverless 引擎之类的云原生 DBMS 设计将需要与更丰富的元数据层集成，而不是直接扫描数据湖中的原始文件，可以能够提高查询性能。 最后 Lakehouse 的设计易于分布式协作，因为可以从对象存储库直接访问所有数据集，这使得共享数据变得很简单。 ","date":"2021-01-06","objectID":"/lakehouse/:5:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"结论在开放的数据湖文件格式上实现数据仓库功能的统一数据平台体系架构可以为当今的数据仓库系统提供具有竞争力的性能，并有助于应对数据仓库用户面临的许多挑战，尽管限制数据仓库的存储层以标准格式直接访问看起来似乎是一个重大限制，但诸如热数据缓存和冷数据数据布局优化之类的优化可以使 Lakehouse 获得很不错的性能，另外鉴于数据湖中已有大量数据，并且有机会大大简化企业数据架构，行业很可能会向 Lakehouse 架构逐步过渡。 ","date":"2021-01-06","objectID":"/lakehouse/:6:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"}]