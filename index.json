[{"categories":["云原生"],"content":"本文系统介绍了 ConfigMap、Secret、Pod 身份认证、容器资源和安全、InitContainer","date":"2021-07-17","objectID":"/application-configuration-management/","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"需求来源","date":"2021-07-17","objectID":"/application-configuration-management/:1:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"背景问题首先一起来看一下需求来源。大家应该都有过这样的经验，用一个容器镜像来启动一个 container。要启动这个容器，其实有很多需要配套的问题待解决： 第一，比如说一些可变的配置。因为我们不可能把一些可变的配置写到镜像里面，当这个配置需要变化的时候，可能需要我们重新编译一次镜像，这个肯定是不能接受的； 第二就是一些敏感信息的存储和使用。比如说应用需要使用一些密码，或者用一些 token； 第三就是我们容器要访问集群自身。比如我要访问 kube-apiserver，那么本身就有一个身份认证的问题； 第四就是容器在节点上运行之后，它的资源需求； 第五个就是容器在节点上，它们是共享内核的，那么它的一个安全管控怎么办？ 最后一点，容器启动之前的一个前置条件检验。比如说，一个容器启动之前，可能要确认一下 DNS 服务是不是好用？又或者确认一下网络是不是联通的？那么这些其实就是一些前置的校验。 ","date":"2021-07-17","objectID":"/application-configuration-management/:1:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Pod 的配置管理在 Kubernetes 里面，它是怎么做这些配置管理的呢？如下表所示： 配置 说明 ConfigMap 可变配置 Secret 敏感信息 ServiceAccount 身份认证 Spec.Containers[].Resources.limits/requests 资源配置 Spec.Containers[].Resources.SecrityContext 安全管控 Spec.InitContainers 前置校验 ","date":"2021-07-17","objectID":"/application-configuration-management/:1:2","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ConfigMap","date":"2021-07-17","objectID":"/application-configuration-management/:2:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ConfigMap 介绍ConfigMap 它是用来做什么的、以及它带来的一个好处。它其实主要是管理一些可变配置信息，比如说我们应用的一些配置文件，或者说它里面的一些环境变量，或者一些命令行参数。 它的好处在于它可以让一些可变配置和容器镜像进行解耦，这样也保证了容器的可移植性。 apiVersion:v1kind:ConfigMapmetadata:lables:app:flanneltier:nodename:kube-flannnel-cfgnamespace:kube-systemdata:cni-conf.json:|{ \"name\": \"cbr0\", \"type\": \"\"flannel\", \"delegate\": { \"isDefaultGateway\": true } }net-conf.json:|{ \"Network\": \"172.27.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } } 这是 ConfigMap 本身的一个定义，它包括两个部分：一个是 ConfigMap 元信息，我们关注 name 和 namespace 这两个信息。接下来这个 data 里面，可以看到它管理了两个配置文件。它的结构其实是这样的：从名字看ConfigMap中包含Map单词，Map 其实就是 key:value，key 是一个文件名，value 是这个文件的内容。 ","date":"2021-07-17","objectID":"/application-configuration-management/:2:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ConfigMap 创建推荐用 kubectl 这个命令来创建，它带的参数主要有两个：一个是指定 name，第二个是 DATA。其中 DATA 可以通过指定文件或者指定目录，以及直接指定键值对。 kubectl create configmap [NAME] [DATA] 指定文件的话，文件名就是 Map 中的 key，文件内容就是 Map 中的 value。然后指定键值对就是指定数据键值对，即：key:value 形式，直接映射到 Map 的key:value。 ","date":"2021-07-17","objectID":"/application-configuration-management/:2:2","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ConfigMap 使用创建完了之后，应该怎么使用呢？ 第一种是环境变量。环境变量的话通过 valueFrom，然后 ConfigMapKeyRef 这个字段，下面的 name 是指定 ConfigMap 名，key 是 ConfigMap.data 里面的 key。这样的话，在 busybox 容器启动后容器中执行 env 将看到一个 SPECIAL_LEVEL_KEY 环境变量； 第二个是命令行参数。命令行参数其实是第一行的环境变量直接拿到 cmd 这个字段里面来用； 最后一个是通过 volume 挂载的方式直接挂到容器的某一个目录下面去。上面的例子是把 special-config 这个 ConfigMap 里面的内容挂到容器里面的 /etc/config 目录下，这个也是使用的一种方式。 ","date":"2021-07-17","objectID":"/application-configuration-management/:2:3","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ConfigMap 注意要点现在对 ConfigMap 的使用做一个总结，以及它的一些注意点，注意点一共列了以下五条： ConfigMap 文件的大小。虽然说 ConfigMap 文件没有大小限制，但是在 ETCD 里面，数据的写入是有大小限制的，现在是限制在 1MB 以内； 第二个注意点是 pod 引入 ConfigMap 的时候，必须是相同的 Namespace 中的 ConfigMap，前面其实可以看到，ConfigMap.metadata 里面是有 namespace 字段的； 第三个是 pod 引用的 ConfigMap。假如这个 ConfigMap 不存在，那么这个 pod 是无法创建成功的，其实这也表示在创建 pod 前，必须先把要引用的 ConfigMap 创建好； 第四点就是使用 envFrom 的方式。把 ConfigMap 里面所有的信息导入成环境变量时，如果 ConfigMap 里有些 key 是无效的，比如 key 的名字里面带有数字，那么这个环境变量其实是不会注入容器的，它会被忽略。但是这个 pod 本身是可以创建的。这个和第三点是不一样的方式，是 ConfigMap 文件存在基础上，整体导入成环境变量的一种形式； 最后一点是：什么样的 pod 才能使用 ConfigMap？这里只有通过 K8s api 创建的 pod 才能使用 ConfigMap，比如说通过用命令行 kubectl 来创建的 pod，肯定是可以使用 ConfigMap 的，但其他方式创建的 pod，比如说 kubelet 通过 manifest 创建的 static pod，它是不能使用 ConfigMap 的。 ","date":"2021-07-17","objectID":"/application-configuration-management/:2:4","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Secret","date":"2021-07-17","objectID":"/application-configuration-management/:3:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Secret 介绍Secret 是一个主要用来存储密码 token 等一些敏感信息的资源对象。其中，敏感信息是采用 base-64 编码保存起来的。 Secret 类型种类比较多，下面列了常用的四种类型： 第一种是 Opaque，它是普通的 Secret 文件； 第二种是 service-account-token，是用于 service-account 身份认证用的 Secret； 第三种是 dockerconfigjson，这是拉取私有仓库镜像的用的一种 Secret； 第四种是 bootstrap.token，是用于节点接入集群校验用的 Secret。 ","date":"2021-07-17","objectID":"/application-configuration-management/:3:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Secret 创建Secret 有两种创建方式： 系统创建：比如 K8s 为每一个 namespace 的默认用户（default ServiceAccount）创建 Secret； 用户手动创建：手动创建命令，推荐 kubectl 这个命令行工具，它相对 ConfigMap 会多一个 type 参数。其中 data 也是一样，它也是可以指定文件和键值对的。type 的话，要是你不指定的话，默认是 Opaque 类型。 ","date":"2021-07-17","objectID":"/application-configuration-management/:3:2","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Secret 使用创建完 Secret 之后，再来看一下如何使用它。它主要是被 pod 来使用，一般是通过 volume 形式挂载到容器里指定的目录，然后容器里的业务进程再到目录下读取 Secret 来进行使用。另外在需要访问私有镜像仓库时，也是通过引用 Secret 来实现。 挂载到用户指定目录的方式： 第一种方式：用户直接指定 第二种方式：系统自动生成，过程中会生成两个文件，一个是 ca.crt，一个是 token。 ","date":"2021-07-17","objectID":"/application-configuration-management/:3:3","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Secret 使用注意要点 Secret 的文件大小限制。这个跟 ConfigMap 一样，也是 1MB。 Secret 采用了 base-64 编码，但是它跟明文也没有太大区别。所以说，如果有一些机密信息要用 Secret 来存储的话，还是要很慎重考虑。也就是说谁会来访问你这个集群，谁会来用你这个 Secret，还是要慎重考虑，因为它如果能够访问这个集群，就能拿到这个 Secret。如果是对 Secret 敏感信息要求很高，对加密这块有很强的需求，推荐可以使用 Kubernetes 和开源的 vault做一个解决方案，来解决敏感信息的加密和权限管理。 Secret 读取的最佳实践，建议不要用 list/watch，如果用 list/watch 操作的话，会把 namespace 下的所有 Secret 全部拉取下来，这样其实暴露了更多的信息。推荐使用 GET 的方法，这样只获取你自己需要的那个 Secret。 ","date":"2021-07-17","objectID":"/application-configuration-management/:3:4","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ServiceAccount","date":"2021-07-17","objectID":"/application-configuration-management/:4:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"ServiceAccount 介绍ServiceAccount 首先是用于解决 pod 在集群里面的身份认证问题，身份认证信息是存在于 Secret 里面。 ","date":"2021-07-17","objectID":"/application-configuration-management/:4:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Resource","date":"2021-07-17","objectID":"/application-configuration-management/:5:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"容器资源配合管理目前内部支持类型有三种：CPU、内存以及临时存储。如果用户觉得这三种不够，可以自己定义所需的资源，如 GPU。配置资源时，资源指定数量必须为整数。目前资源配置主要分成 request 和 limit 两种类型，一个是需要的数量，一个是资源的界限。CPU、内存以及临时存储都是在 container 下的 Resource 字段里进行一个声明。 ","date":"2021-07-17","objectID":"/application-configuration-management/:5:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"Pod 服务质量 (QoS) 配置根据 CPU 对容器内存资源的需求，我们对 pod 的服务质量进行一个分类，分别是 Guaranteed、Burstable 和 BestEffort。 Guaranteed：pod 里面每个容器都必须有内存和 CPU 的 request 以及 limit 的一个声明，且 request 和 limit 必须是一样的，这就是 Guaranteed； Burstable：Burstable 至少有一个容器存在内存和 CPU 的一个 request； BestEffort：只要不是 Guaranteed 和 Burstable，那就是 BestEffort。 服务质量是什么样的呢？资源配置好后，当这个节点上 pod 容器运行，比如说节点上 memory 配额资源不足，kubelet会把一些低优先级的，或者说服务质量要求不高的（如：BestEffort、Burstable）pod 驱逐掉。它们是按照先去除 BestEffort，再去除 Burstable 的一个顺序来驱逐 pod 的。 ","date":"2021-07-17","objectID":"/application-configuration-management/:5:2","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"SecurityContext","date":"2021-07-17","objectID":"/application-configuration-management/:6:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"SecurityContext 介绍SecurityContext 主要是用于限制容器的一个行为，它能保证系统和其他容器的安全。这一块的能力不是 Kubernetes 或者容器 runtime 本身的能力，而是 Kubernetes 和 runtime 通过用户的配置，最后下传到内核里，再通过内核的机制让 SecurityContext 来生效。 SecurityContext 主要分为三个级别： 第一个是容器级别，仅对容器生效； 第二个是 pod 级别，对 pod 里所有容器生效； 第三个是集群级别，就是 PSP，对集群内所有 pod 生效。 权限和访问控制设置项，现在一共列有七项（这个数量后续可能会变化）： 第一个就是通过用户 ID 和组 ID 来控制文件访问权限； 第二个是 SELinux，它是通过策略配置来控制用户或者进程对文件的访问控制； 第三个是特权容器； 第四个是 Capabilities，它也是给特定进程来配置一个 privileged 能力； 第五个是 AppArmor，它也是通过一些配置文件来控制可执行文件的一个访问控制权限，比如说一些端口的读写； 第六个是一个对系统调用的控制； 第七个是对子进程能否获取比父亲更多的权限的一个限制。 ","date":"2021-07-17","objectID":"/application-configuration-management/:6:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"InitContainer","date":"2021-07-17","objectID":"/application-configuration-management/:7:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"InitContainer 介绍首先介绍 InitContainer 和普通 container 的区别，有以下三点内容： InitContainer 首先会比普通 container 先启动，并且直到所有的 InitContainer 执行成功后，普通 container 才会被启动 InitContainer 之间是按定义的次序去启动执行的，执行成功一个之后再执行第二个，而普通的 container 是并发启动的； InitContainer 执行成功后就结束退出，而普通容器可能会一直在执行。它可能是一个 longtime 的，或者说失败了会重启，这个也是 InitContainer 和普通 container 不同的地方 InitContainer 其实主要为普通 container 服务，比如说它可以为普通 container 启动之前做一个初始化，或者为它准备一些配置文件， 配置文件可能是一些变化的东西。再比如做一些前置条件的校验，如网络是否联通。 ","date":"2021-07-17","objectID":"/application-configuration-management/:7:1","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"结束语 ConfigMap 和 Secret: 首先介绍了 ConfigMap 和 Secret 的创建方法和使用场景，然后对 ConfigMap 和 Secret 的常见使用注意点进行了分类和整理。最后介绍了私有仓库镜像的使用和配置； Pod 身份认证: 首先介绍了 ServiceAccount 和 Secret 的关联关系，然后从源码角度对 Pod 身份认证流程和实现细节进行剖析，同时引出了 Pod 的权限管理(即 RBAC 的配置管理)； 容器资源和安全： 首先介绍了容器常见资源类型 (CPU/Memory) 的配置，然后对 Pod 服务质量分类进行详细的介绍。同时对 SecurityContext 有效层级和权限配置项进行简要说明； InitContainer: 首先介绍了 InitContainer 和普通 container 的区别以及 InitContainer 的用途。然后基于实际用例对 InitContainer 的用途进行了说明。 ","date":"2021-07-17","objectID":"/application-configuration-management/:8:0","tags":["云原生","Kubernetes","配置管理"],"title":"应用配置管理","uri":"/application-configuration-management/"},{"categories":["云原生"],"content":"通过类比 Deployment 控制器，我们理解了一下DaemonSet 控制器的工作流程与方式，并且通过对 DaemonSet 的更新了解了滚动更新的概念和相对应的操作方式。","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"需求来源如果我们没有 DaemonSet 会怎么样？下面有几个需求： 首先如果希望每个节点都运行同样一个 pod 怎么办？ 如果新节点加入集群的时候，想要立刻感知到它，然后去部署一个 pod，帮助我们初始化一些东西，这个需求如何做？ 如果有节点退出的时候，希望对应的 pod 会被删除掉，应该怎么操作？ 如果 pod 状态异常的时候，我们需要及时地监控这个节点异常，然后做一些监控或者汇报的一些动作，那么这些东西运用什么控制器来做？ ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:1:0","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"DaemonSet：守护进程控制器DaemonSet 也是 Kubernetes 提供的一个 default controller，它实际是做一个守护进程的控制器，它能帮我们做到以下几件事情： 首先能保证集群内的每一个节点都运行一组相同的 pod； 同时还能根据节点的状态保证新加入的节点自动创建对应的 pod； 在移除节点的时候，能删除对应的 pod； 而且它会跟踪每个 pod 的状态，当这个 pod 出现异常、Crash 掉了，会及时地去 recovery 这个状态。 ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:2:0","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"用例解读","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:3:0","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"DaemonSet 语法 apiVersion:apps/v1kind:DaemonSetmetadata:name:fluentd-elasticsearchnamespace:kube-systemlables:k8s-app:fluentd-loggingspec:selector:matchLabels:name:fluentd-elasticsearchtemplate:metadata:labels:name:fluentd-elasticsearchspec:containers:- name:fluentd-elasticsearchimage:fluent/fluentd:v1.4-1 首先是 kind:DaemonSet。它会有 matchLabel，通过 matchLabel 去管理对应所属的 pod，这个 pod.label 也要和这个 DaemonSet.controller.label 想匹配，它才能去根据 label.selector 去找到对应的管理 Pod。下面 spec.container 里面的东西都是一致的。 DaemonSet 适用场景： 集群存储进程： GlusterFS 或者 Ceph 之类的东西，需要每台节点上都运行一个类似于 Agent 的东西，DaemonSet 就能很好地满足这个诉求； 日志收集进程：logstash 或者 fluentd，这些都是同样的需求，需要每台节点都运行一个 Agent，这样的话，我们可以很容易搜集到它的状态，把各个节点里面的信息及时地汇报到上面； 需要在每个节点运行的监控收集器，例如 Promethues。 ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:3:1","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"查看 DaemonSet 状态创建完 DaemonSet 之后，我们可以使用 kubectl get DaemonSet（DaemonSet 缩写为 ds）。 kubectl get ds NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluentd-elasticsearch 4 4 4 4 4 \u003cnone\u003e 4s 状态描述： DESIRED：需要的 pod 个数 CURRENT：当前已存在的 pod 个数 READY：就绪的个数 UP-TO-DATE：最新创建的个数 AVAILABLE：可用 pod 个数 NODE SELECTOR：节点选择标签，可用于选择部分节点去运行 pod，而不是全部节点运行 pod ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:3:2","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"更新 DaemonSet其实 DaemonSet 和 deployment 特别像，它也有两种更新策略：一个是 RollingUpdate，另一个是 OnDelete。 RollingUpdate 其实比较好理解，就是会一个一个的更新。先更新第一个 pod，然后老的 pod 被移除，通过健康检查之后再去见第二个 pod，这样对于业务上来说会比较平滑地升级，不会中断； OnDelete 其实也是一个很好的更新策略，就是模板更新之后，pod 不会有任何变化，需要我们手动控制。我们去删除某一个节点对应的 pod，它就会重建，不删除的话它就不会重建，这样的话对于一些我们需要手动控制的特殊需求也会有特别好的作用。 ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:3:3","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"架构设计","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:4:0","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"DaemonSet 管理模式 DaemonSet Architecture DaemonSet Controller 负责根据配置创建 Pod DaemonSet Controller 跟踪 Job 状态，根据配置及时重试 Pod 或者继续创建 DaemonSet Controller 会自动添加 affinity\u0026label 来跟踪对应的 pod，并根据配置在每个节点或者适应的部分节点创建 Pod ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:4:1","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"DaemonSet 控制器 DaemonSet Controller DaemonSet 其实和 Job controller 做的差不多：两者都需要根据 watch 这个 API Server 的状态。现在 DaemonSet 和 Job controller 唯一的不同点在于，DaemonsetSet Controller需要去 watch node 的状态，但其实这个 node 的状态还是通过 API Server 传递到 ETCD 上。 当有 node 状态节点发生变化时，它会通过一个内存消息队列发进来，然后DaemonSet controller 会去 watch 这个状态，看一下各个节点上是都有对应的 Pod，如果没有的话就去创建。当然它会去做一个对比，如果有的话，它会比较一下版本，然后加上刚才提到的是否去做 RollingUpdate？如果没有的话就会重新创建，Ondelete 删除 pod 的时候也会去做 check 它做一遍检查，是否去更新，或者去创建对应的 pod。 当然最后的时候，如果全部更新完了之后，它会把整个 DaemonSet 的状态去更新到 API Server 上，完成最后全部的更新。 ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:4:2","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"本节总结DaemonSet 基础操作与概念解析：通过类比 Deployment 控制器，我们理解了一下DaemonSet 控制器的工作流程与方式，并且通过对 DaemonSet 的更新了解了滚动更新的概念和相对应的操作方式。 ","date":"2021-07-10","objectID":"/application-orchestration-and-management-daemonset/:5:0","tags":["云原生","Kubernetes","应用编排","DaemonSet"],"title":"应用编排与管理：DaemonSet","uri":"/application-orchestration-and-management-daemonset/"},{"categories":["云原生"],"content":"如何管理 Pod 中的进程？一切秘密都在这里","date":"2021-07-04","objectID":"/application-orchestration-and-management-job/","tags":["云原生","Kubernetes","应用编排","Job","CronJob"],"title":"应用编排与管理：Job","uri":"/application-orchestration-and-management-job/"},{"categories":["云原生"],"content":"Job","date":"2021-07-04","objectID":"/application-orchestration-and-management-job/:1:0","tags":["云原生","Kubernetes","应用编排","Job","CronJob"],"title":"应用编排与管理：Job","uri":"/application-orchestration-and-management-job/"},{"categories":["云原生"],"content":"需求来源Job 背景问题 在 K8s 里，最小的调度单元是 Pod，可以直接通过 Pod 来运行任务进程。这样做会产生以下几种问题： 如何保证 Pod 内进程正确的结束？ 如何保证进程运行失败后重试？ 如何管理多个任务，且任务之间有依赖关系？ 如何并行地运行任务，并管理任务的队列大小？ Job：管理任务的控制器 Job 提供了以下功能： 首先 K8s 的 Job 是一个管理任务的控制器，可以创建一个或多个 Pod 来指定 Pod 的数量，并可以监控它是否成功地运行或终止； 可以根据 Pod 的状态来给 Job 设置重置的方式及重试的次数； 可以根据依赖关系，保证上一个任务运行完成之后再运行下一个任务； 可以控制任务的并行度，根据并行度来确保 Pod 运行过程中的并行次数和总体完成大小。 ","date":"2021-07-04","objectID":"/application-orchestration-and-management-job/:1:1","tags":["云原生","Kubernetes","应用编排","Job","CronJob"],"title":"应用编排与管理：Job","uri":"/application-orchestration-and-management-job/"},{"categories":["云原生"],"content":"用例解读Job 语法 apiVersion:batch/v1kind:Jobmetadata:name:pispec:template:spec:containers:- name:piimage:perlcommand:[\"perl\",\"-Mbignum=bpi\",\"-wle\",\"print bpi(2000)\"]restartPolicy:NeverbackoffLimit:4 restartPolicy：可以设置 Never、OnFailure、Always 这三种重试策略。在希望 Job 需要重新运行的时候，可以用 Never；希望在失败的时候再运行，再重试可以用 OnFailure；或者不论什么情况下都重新运行时 Always； backoffLimit：控制 Job 重试的次数。 并行运行 Job apiVersion:batch/v1kind:Jobmetadata:name:paral-1spec:completions:8parallelism:2template:spec:containers:- name:paralimage:ubuntucommand:[\"/bin/sh\"]args:[\"-c\",\"sleep 30; date\"]restartPolicy:OnFailure 有时会有并行运行的需求：希望 Job 运行的时候可以最大化的并行，并行出 n 个 Pod 去快速地执行。同时，也不希望同时并行的 Pod 数过多。 completions：指定本 Pod 队列执行次数，或者理解为指定可以运行的总次数； parallelism：并行执行的个数。 CronJob 语法 apiVersion:batch/v1beta1kind:CronJobmetadata:name:hellospec:schedule:\"*/1 * * * *\"jobTemplate:spec:template:spec:containers:- name:helloimage:busyboxargs:- /bin/sh- -c- date; echo Hello from the Kubernetes clusterrestartPolicy:OnFailurestartingDeadlineSeconds:10concurrencyPolicy:AllowsuccessfulJobsHistoryLimit:3 CronJob，也叫做定时运行 Job，可以设定一个事件去执行； schedule：设置时间格式，和 Linux 的 crontime 是一样的，直接根据 Linux 的 crontime 书写格式来写就可以了 startDeadlineSeconds：每次运行 Job 的时候，最长可以等多长时间，有时 Job 可能运行很长时间也不会启动。如果超过较长时间的话，CronJob 就会停止这个 Job； concurrencyPolicy：是否允许并行运行，如果设置允许，则不管前面的 Job 是否运行完成，后面的 Job 每分钟都会执行，如果是 false，则等待上个结束再运行下一个； JobsHistoryLimit：留存历史版本数量。 ","date":"2021-07-04","objectID":"/application-orchestration-and-management-job/:1:2","tags":["云原生","Kubernetes","应用编排","Job","CronJob"],"title":"应用编排与管理：Job","uri":"/application-orchestration-and-management-job/"},{"categories":["云原生"],"content":"架构设计 Job Architecture Job Controller 创建相对应的 pod； Job Controller 跟踪 Job 状态，根据配置及时重试 Pod 或者继续创建； Job Controller 会自动添加 label 来跟踪对应的 pod，并根据配置并行或串行创建 Pod。 Job 控制器 Job Controller 所有 Job 都是一个 controller，它会 watch 这个 API Server，每次提交一个 Job 的 yaml 都会经过 api-server 传到 etcd 里面去，然后 Job Controller 会注册几个 Handler，每当有添加、更新、删除等操作的时候，会通过一个内存级的消息队列，发到 controller 里面。 通过 Job Controller 检查当前是否有运行的 Pod，如果没有的话，通过 Scale up 把这个 Pod 创建出来；如果有的话，或者大于这个数，对它进行 Scale down，如果这时 Pod 发生变化，需要及时更新状态。 同时要去检查它是否是并行的 job，或者是串行的 job，根据设置的配置并行度、串行度，及时地把 pod 的数量给创建出来。最后，它会把 job 的整个的状态更新到 API Server 里面去，这样就能看到呈现出来的最终效果了。 ","date":"2021-07-04","objectID":"/application-orchestration-and-management-job/:1:3","tags":["云原生","Kubernetes","应用编排","Job","CronJob"],"title":"应用编排与管理：Job","uri":"/application-orchestration-and-management-job/"},{"categories":["云原生"],"content":"Deployment 保证集群里可用 Pod 数量，为所有 Pod 更新镜像版本，保证更新过程中的服务可用性，实现快速回滚","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"需求来源","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:1:0","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"背景问题现在存在三个应用，分别对应一些 Pod，那么我们可以直接管理集群中所有的 Pod 吗？ 如果管理所有的 Pod，那么如何保证集群里可用 Pod 数量？如何为所有 Pod 更新镜像版本？更新的过程中，如何保证服务可用性？更新的过程中，发现问题如何快速回滚？ ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:1:1","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"Deployment：管理部署发布的控制器可以通过 Deployment 将三个应用分别规划到不同的 Deployment，每个 Deployment 管理一组相同的应用 Pod，这组 Pod 是相同的一个副本。 Deployment Deployment 可以实现以下功能： Deployment 定义了 Pod 期望数量； 配置 Pod 发布方式，即 controller 会按照用户给定的策略来更新 Pod，而且更新过程中，可以设定不可用 Pod 数量在多少范围内； 更新过程中发现问题可以回滚。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:1:2","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"架构设计","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:0","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"管理模式 Management Model Deployment 只管理不同版本的 ReplicaSet，由 ReplicaSet 来管理具体的 Pod 副本数，每个 ReplicaSet 对应 Deployment template 的一个版本。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:1","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"Deployment 控制器 Deployment Controller 控制器通过 Informer 中的 Event 做一些 Handler 和 Watch，收到事件后会加入到队列中。而 Deployment controller 从队列中取出来之后，它的逻辑会判断 Check Paused，如果 Paused 设置为 true 的话，就表示这个 Deployment 只会做一个数量上的维持，不会做新的发布，如果为 false 的话，就会做 Rollout。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:2","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"ReplicaSet 控制器 Replicaset Controller 当 Deployment 分配 ReplicaSet 之后，ReplicaSet 控制器本身也是从 Informer 中 watch 一些事件，这些事件包含了 ReplicaSet 和 Pod 的事件。从队列中取出之后，ReplicaSet controller 的逻辑很简单，就只管理副本数。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:3","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"发布模拟 Deployment Process Deployment 当前初始版本为 template1，底下有三个 Pod：Pod1、Pod2、Pod3。 这时修改 template 中一个容器的 image，Deployment controller 就会新建一个对应 template2 的 ReplicaSet。创建出来之后 ReplicaSet 会逐渐修改两个 ReplicaSet 的数量，比如逐渐增加 ReplicaSet2 中 replicas 的期望数量，逐渐减少 ReplicaSet1 中的 Pod 数量。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:4","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"spec 字段解析 MinReadySeconds：Deployment 会根据 Pod ready 来看 Pod 是否可用，但是如果我们设置了 MinReadySeconds 之后，比如设置为 30 秒，那 Deployment 就一定会等到 Pod ready 超过 30 秒之后才认为 Pod 是 available 的。Pod available 的前提条件是 Pod ready，但是 ready 的 Pod 不一定是 available 的，它一定要超过 MinReadySeconds 之后，才会判断为 available revisionHistoryLimit：保留历史 revision，即保留历史 ReplicaSet 的数量，默认值为 10 个。这里可以设置为一个或两个，如果回滚可能性比较大的话，可以设置数量超过 10； paused：paused 是标识，Deployment 只做数量维持，不做新的发布，这里在 Debug 场景可能会用到； progressDeadlineSeconds：前面提到当 Deployment 处于扩容或者发布状态时，它的 condition 会处于一个 processing 的状态，processing 可以设置一个超时时间。如果超过超时时间还处于 processing，那么 controller 将认为这个 Pod 会进入 failed 的状态。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:5","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["云原生"],"content":"升级策略字段解析Deployment 在 RollingUpdate 中主要提供了两个策略，一个是 MaxUnavailable，另一个是 MaxSurge。 MaxUnavailable：滚动过程中最多有多少个 Pod 不可用； MaxSurge：滚动过程中最多存在多少个 Pod 超过预期 replicas 数量。 上文提到，ReplicaSet 为 3 的 Deployment 在发布的时候可能存在一种情况：新版本的 ReplicaSet 和旧版本的 ReplicaSet 都可能有两个 replicas，加在一起就是 4 个，超过了我们期望的数量三个。这是因为我们默认的 MaxUnavailable 和 MaxSurge 都是 25%，默认 Deployment 在发布的过程中，可能有 25% 的 replica 是不可用的，也可能超过 replica 数量 25% 是可用的，最高可以达到 125% 的 replica 数量。 这里其实可以根据用户实际场景来做设置。比如当用户的资源足够，且更注重发布过程中的可用性，可设置 MaxUnavailable 较小、MaxSurge 较大。但如果用户的资源比较紧张，可以设置 MaxSurge 较小，甚至设置为 0，这里要注意的是 MaxSurge 和 MaxUnavailable 不能同时为 0。 ","date":"2021-06-27","objectID":"/application-orchestration-and-management-deployment/:2:6","tags":["云原生","Kubernetes","应用编排","Deployment"],"title":"应用编排与管理：Deployment","uri":"/application-orchestration-and-management-deployment/"},{"categories":["LCTT"],"content":"可拓展性与灵活性、安全性和高可信的协作、不受供应商限制、顶尖人才和社区","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":" 使用开源技术可以帮助组织获得更好的业务结果。 构建技术栈是每个组织的主要决策。选择合适的工具将让团队获得成功，选择错误的解决方案或平台会对生产率和利润率产生毁灭性影响。为了在当今快节奏的世界中脱颖而出，组织必须明智地选择数字解决方案，好的数字解决方案可以提升团队行动力与运营敏捷性。 这就是为什么越来越多的组织都采用开源解决方案的原因，这些组织来自各行各业，规模有大有小。根据 麦肯锡 最近的报告，高绩效组织的最大区别是采用不同的开源方案。 采用开源技术可以帮助组织提高竞争优势、获得更好业务成果的原因有以下四点。 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:0:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":"1、可拓展性和灵活性可以说，技术世界发展很快。例如，在 2014 年之前，Kubernetes 并不存在，但今天，它却令人印象深刻，无处不在。根据 CNCF 2020 云原生调查，91% 的团队正在以某种形式使用 Kubernetes。 组织投资开源的一个主要原因是因为开源赋予组织行动敏捷性，组织可以迅速地将新技术集成到技术栈中。这与传统方法不同，在传统方法中，团队需要几个季度甚至几年来审查、实施、采用软件，这导致团队不可能实现火速转变。 开源解决方案完整地提供源代码，团队可以轻松将软件与他们每天使用的工具连接起来。 简而言之，开源让开发团队能够为手头的东西构建完美的工具，而不是被迫改变工作方式来适应不灵活的专有工具。 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:1:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":"2、安全性和高可信的协作在数据泄露备受瞩目的时代，组织需要高度安全的工具来保护敏感数据的安全。 专有解决方案中的漏洞不易被发现，被发现时为时已晚。不幸的是，使用这些平台的团队无法看到源代码，本质上是他们将安全性外包给特定供应商，并希望得到最好的结果。 采用开源的另一个主要原因是开源工具使组织能够自己把控安全。例如，开源项目——尤其是拥有大型开源社区的项目——往往会收到更负责任的漏洞披露，因为每个人在使用过程中都可以彻底检查源代码。 由于源代码是免费提供的，因此披露通常伴随着修复缺陷的详细建议解决方案。这些方案使得开发团队能够快速解决问题，不断增强软件。 在远程办公时代，对于分布式团队来说，在知道敏感数据受到保护的情况下进行协作比以往任何时候都更重要。开源解决方案允许组织审核安全性、完全掌控自己数据，因此开源方案可以促进远程环境下高可信协作方式的成长。 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:2:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":"3、不受供应商限制根据 最近的一项研究，68% 的 CIO 担心受供应商限制。当你受限于一项技术中，你会被迫接受别人的结论，而不是自己做结论。 当组织更换供应商时，专有解决方案通常会 给你带走数据带来挑战。另一方面，开源工具提供了组织需要的自由度和灵活性，以避免受供应商限制，开源工具可以让组织把数据带去任意地方。 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:3:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":"4、顶尖人才和社区随着越来越多的公司 接受远程办公，人才争夺战变得愈发激烈。 在软件开发领域，获得顶尖人才始于赋予工程师先进工具，让工程师在工作中充分发挥潜力。开发人员 越来越喜欢开源解决方案 而不是专有解决方案，组织应该强烈考虑用开源替代商业解决方案，以吸引市场上最好的开发人员。 除了雇佣、留住顶尖人才更容易，公司能够通过开源平台利用贡献者社区，得到解决问题的建议，从平台中得到最大收益。此外，社区成员还可以 直接为开源项目做贡献。 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:4:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["LCTT"],"content":"开源带来自由开源软件在企业团队中越来越受到欢迎——这是有原因的。它帮助团队灵活地构建完美的工作工具，同时使团队可以维护高度安全的环境。同时，开源允许团队掌控未来方向，而不是局限于供应商的路线图。开源还帮助公司接触才华横溢的工程师和开源社区成员。 via: https://opensource.com/article/21/4/open-source-competitive-advantage 作者：Jason Blais 选题：lujun9972 译者：DCOLIVERSUN 校对：wxy 本文由 LCTT 原创编译，Linux中国 荣誉推出 ","date":"2021-06-12","objectID":"/four-ways-open-source-gives-you-a-competitive-edge/:5:0","tags":["LCTT","开源"],"title":"开源为你带来竞争优势的 4 种方式","uri":"/four-ways-open-source-gives-you-a-competitive-edge/"},{"categories":["云原生"],"content":"本文介绍了K8s的控制器模式与工作逻辑，解释了为何命令式API不适用控制器模式的原因","date":"2021-06-11","objectID":"/application-orchestration-and-management/","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"资源元信息","date":"2021-06-11","objectID":"/application-orchestration-and-management/:1:0","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"K8s 资源对象K8s 的资源对象组成：主要包括了 Spec、Status 两部分。其中 Spec 部分用来描述期望的状态，Status 部分用来描述观测到的状态。 其实，K8s 还有另外一部分，即元数据部分。该部分主要包括了用来识别资源的标签：Labels，用来描述资源的注解：Annotations，用来描述多个资源之间相互关系的 OwnerReference。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:1:1","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"labels第一个元数据，也是最重要的元数据是：资源标签。资源标签是一种具有标识型的 Key：Value 元数据。 标签主要用来筛选资源和组合资源，可以使用类似于 SQL 查询 select，来根据 Label 查询相关的资源。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:1:2","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"AnnotationsAnnotations 是系统或者工具用来存储资源的非标志性信息，可以用来扩展资源的 spec/status 的描述。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:1:3","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"Ownereference所谓所有者，一般就是指集合类的资源，比如说 Pod 集合，就有 replicaset、statefulset。 集合类资源的控制器会创建对应的归属资源。比如：replicaset 控制器在操作中会创建 Pod，被创建 Pod 的 Ownereference 就指向了创建 Pod 的 replicaset。Ownereference 使得用户可以方便地查找一个创建资源的对象，另外，还可以用来实现级联删除的效果。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:1:4","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"控制器模式","date":"2021-06-11","objectID":"/application-orchestration-and-management/:2:0","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"控制循环控制型模式最核心的就是控制循环的概念。在控制循环中包括了控制器，被控制的系统，以及能够观测系统的传感器。 Control Operation 这些组件都是逻辑的，外界通过修改资源 spec 来控制资源，控制器比较资源 spec 和 status，从而计算一个 diff，diff 最后会用来决定对系统的控制操作，控制操作会使得系统产生新的输出，并被传感器以资源 status 形式上报，控制器的各个组件将都会是独立自主地运行，不断使系统向 spec 表示终态趋近。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:2:1","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"Sensor循环控制中逻辑的传感器主要由 Reflector、Informer、Indexer 三个组件构成。 Sensor Reflector 通过 List 和 Watch K8s server 来获取资源的数据。List 用来在 Controller 重启以及 Watch 中断的情况下，进行系统资源的全量更新；而 Watch 则在多次 List 之间进行增量的资源更新；Reflector 在获取新的资源数据后，会在 Delta 队列中塞入一个包括资源对象信息本身以及资源对象事件类型的 Delta 记录，Delta 队列中可以保证同一个对象在队列中仅有一条记录，从而避免 Reflector 重新 List 和 Watch 的时候产生重复的记录。 Informer 组件不断从 Delta 队列中弹出 delta 记录，然后把资源对象交给 indexer，让 indexer 把资源记录在一个缓存中，缓存在默认设置下是用资源的命名空间来做索引的，并且可以被 Controller Manager 或多个 Controller 所共享。之后，再把这个事件交给事件的回调函数。 控制循环中的控制器组件主要由事件处理函数以及 worker 组成，事件处理函数会相互关注资源的新增、更新、删除的事件，并根据控制器的逻辑去决定是否需要处理。对需要处理的事件，会把事件关联资源的命名空间以及名字塞入一个工作队列中，并且由后续的 worker 池中的一个 worker 来处理，工作队列会对存储的对象进行去重，从而避免多个 worker 处理同一个资源的情况。 worker 在处理资源对象时，一般需要用资源的名字来重新获得最新的资源数据，用来创建或更新资源对象，或者调用其他的外部服务，worker 如果处理失败的时候，一般情况下会把资源的名字重新加入到工作队列中，从而方便之后进行重试。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:2:2","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"控制循环例子-扩容 apiVersion:apps/v1kind:ReplicaSetmetadata:name:rsAnamespace:nsAspec:replicas:3selector:matchLabels:env:prodtemplate:metadata:labels:env:prodspec:containers:- images:registry.cn-hangzhou.aliyuncs.com/acs/nginxname:nginxstatus:replicas:2 更新 Reflector会 watch 到 ReplicaSet 和 Pod 两种资源的变换； 向 delta 队列中塞入 rsA 对象，类型为更新； Informer 取出记录； 将新的 ReplicaSet 更新到缓存中，并与 Namespace nsA 作为索引； 调用 Update 回调函数，ReplicaSet 控制器发现 ReplicaSet 发生变化后把字符串 nsA/rsA 字符串塞入到工作队列中； worker 从工作队列中取到 nsA/rsA 这个字符串的 key； 从缓存中取到了最新的 ReplicaSet 数据； worker 通过比较 ReplicaSet 中 spec 和 status 里的数值，发现需要进行扩容操作，因此创建一个 Pod，这个 Pod 中的 Ownereference 指向了 ReplicaSet rsA； 扩容 Reflector Watch 到 Pod 新增事件； Reflector 在 Delta 队列中额外加入 Add 类型的 delta 记录； Informer 取出记录传送给 Indexer，调用 ReplicaSet 控制器的 Add 回调函数； 将记录存储在缓存中； Add 回调函数通过检查 pod ownerReferences 找到了对应的 ReplicaSet，并把包括 ReplicaSet 命名空间和字符串塞入到了工作队列中； ReplicaSet 的 worker 在得到新工作项之后，从缓存中取到了新的 ReplicaSet 记录，并得到了其所有创建的 Pod； ReplicaSet 更新 status 使得 spec 和 status 达成一致。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:2:3","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"控制器模式总结","date":"2021-06-11","objectID":"/application-orchestration-and-management/:3:0","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"两种 API 设计方法K8s 控制器模式依赖声明式 API，另一种常见 API 类型是命令式 API。 声明式 API：只说目标决定，具体操作交给 worker 命令式 API：告诉 worker 具体操作 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:3:1","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"命令式 API 的问题 错误处理：在大规模分布式系统中，错误是无处不在的。一旦发出的命令没有响应，调用方只能通过反复重试的方式来试图恢复错误，然而盲目的重试可能会带来更大的问题。 命令式 API 后台往往还有一个巡检系统，用来修正命令处理超时、重试等一些场景造成数据不一致的问题。 容易在并发访问时出现问题：假如有多方并发的对一个资源请求进行操作，一旦其中有操作出现错误，就要重试。那么很难确认是哪个操作生效。很多命令式系统往往在操作前会对系统进行加锁，从而保证整个系统最后生效行为的可预见性，但是加锁行为会降低整个系统的操作执行效率。 声明式 API 系统天然记录了系统现在和最终的状态。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:3:2","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"总结 K8s 所采用的控制器模式，是由声明式 API 驱动的。确切来说，是基于对 K8s 资源对象的修改来驱动的； K8s 资源之后，是关注该资源的控制器。这些控制器将异步的控制系统向设置的终态驱近； 控制器是自主运行的，使得系统的自动化和无人值守成为可能； 因为 K8s 的控制器和资源都是可以自定义的，因此可以方便的扩展控制器模式。特别是对于有状态应用，往往通过自定义资源和控制器的方式，来自动化运维操作。 ","date":"2021-06-11","objectID":"/application-orchestration-and-management/:3:3","tags":["云原生","Kubernetes","应用编排"],"title":"应用编排与管理：核心原理","uri":"/application-orchestration-and-management/"},{"categories":["云原生"],"content":"本文介绍为什么需要Pod、Pod的实现机制与设计模式","date":"2021-05-31","objectID":"/pod-notebook/","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"为什么需要 Pod","date":"2021-05-31","objectID":"/pod-notebook/:1:0","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"容器的基本概念容器的本质实际上是一个进程，是一个视图被隔离，资源受限的进程。 容器里面 PID = 1 的进程就是应用本身，这意味着管理虚拟机等于管理基础设施，因为我们是在管理机器，但管理容器等于直接管理应用本身。 ","date":"2021-05-31","objectID":"/pod-notebook/:1:1","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"Pod 的类比概念我们说 K8s 就是云时代的操作系统，我们举个真实操作系统的例子。 例如 Helloworld 程序，这个程序实际上是由一组进程组成的 ({api、main、log、compute})，这些进程等同于 Linux 中的线程。 K8s 类比为一个操作系统，同时容器类比为进程，那么 Pod 就可以类比为进程组。 ","date":"2021-05-31","objectID":"/pod-notebook/:1:2","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"进程组概念先提出一个问题：假如用容器把 Helloworld 程序跑起来，会怎么做呢？ 最自然的解法，启动一个 Docker 容器，里面运行四个进程。这里会引出一个问题，这个容器里面 PID = 1 的进程该是谁？如果该进程是 main 进程，那么由谁去管理剩下的三个进程？ 问题的核心在于，容器设计本身是一种“单进程”模型，不是说容器里只能起一个进程，由于容器的应用等于进程，所以只能去管理 PID = 1 进程，其他再起来的进程其实是一个托管状态。所以说服务应用进程本身具有“进程管理”的能力。 比如说 Helloworld 的程序有 system 的能力，或者直接把容器里 PID = 1 的进程直接改为 systemd，否则这个应用，或者说容器没有办法去管理很多个进程。因为 PID = 1 进程是应用本身，如果现在把这个 PID = 1 的进程给 kill 了，或者它自己运行过程中死掉了，那么剩下三个进程资源无法回收。 反过来，真的把应用本身改为 systemd，或者在容器里运行一个 systemd，这会导致另一个问题：管理容器不是管理应用本身，而是管理 systemd。如果应用退出或者 fail，容器是没有办法知道的。 在 K8s 中，四个进程共同组成的应用 Helloworld，会被定义为一个拥有四个容器的 Pod，四个容器共享 Pod 内资源。需要注意的是，Pod 是一个逻辑单位，没有真实的东西对应 Pod。 Pod ","date":"2021-05-31","objectID":"/pod-notebook/:1:3","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"为什么 Pod 必须是原子调度单位？因为存在调度失败Task co-scheduling，调度失败指的是分配容器到 Node 时，因为不知道全局分配信息，导致有紧密协作的容器不能被分配到同一个 Node。 调度失败存在如下解决办法： Mesos 的资源囤积：当所有设置了 Affinity 约束的任务都达到时，才开始统一调度； Omega 系统的乐观调度：不管冲突的异常情况，先调度，同时设置一个回滚机制，冲突后利用回滚解决问题； K8s 的 Pod：将紧密协作的容器视为一个 Pod，以 Pod 为单位进行调度。 ","date":"2021-05-31","objectID":"/pod-notebook/:1:4","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"再次理解 Pod首先，Pod 里面的容器是 “超亲密关系”。与亲密关系不同的是，亲密关系是可以通过调度解决的，例如两个 Pod 运行在同一台宿主机上，而超亲密关系大致分为如下几类： 文件交换； 需要通过 localhost 或者本地 socket 进行通信； 需要非常频繁的 RPC 调用； 需要共享某些 Linux Namespace。 ","date":"2021-05-31","objectID":"/pod-notebook/:1:5","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"Pod 的实现机制Pod 要打破容器间的 Linux Namespace 和 cgroups 隔离，具体解法分为两部分：网络和存储。 共享网络 在每个 Pod 里，额外起一个 Infra container 小容器来共享整个 Pod 的 Network Namespace。 Infra container 是一个非常小的镜像，大概 100~200KB 左右，是一个汇编语言写的、永远处于“暂停”状态的容器。由于有了这样一个 Infra container 之后，其他所有容器都会通过 Join Namespace 的方式加入到 Infra container 的 Network Namespace 中。 所以说一个 Pod 里面的所有容器，它们看到的网络视图是完全一样的。即：它们看到的网络设备、IP地址、MAC地址等等，跟网络相关的信息，其实全是一份，这一份都来自于 Pod 第一次创建的这个 Infra container。 共享存储 比如说现在有两个容器，一个是 Nginx，另外一个是非常普通的容器，在 Nginx 里放一些文件，让我能通过 Nginx 访问到。所以它需要去 share 这个目录。share 文件或者是 share 目录在 Pod 里面是非常简单的，实际上就是把 volume 变成了 Pod level。然后所有容器，就是所有同属于一个 Pod 的容器，他们共享所有的 volume。 ","date":"2021-05-31","objectID":"/pod-notebook/:2:0","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"详解容器设计模式","date":"2021-05-31","objectID":"/pod-notebook/:3:0","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"InitContainer可以在 yaml 里首先定义一个 Init Container，完成操作后就退出。这个容器会比用户容器先启动，并且严格按照定义顺序来依次执行。 ","date":"2021-05-31","objectID":"/pod-notebook/:3:1","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"容器设计模式：SidecarSidecar 的含义是可以定义一些专门的容器，来执行主业务容器所需要的一些辅助工作，例如： 原本需要在容器里面执行 SSH 需要干的一些事情，可以写脚本、一些前置的条件，其实都可以通过像 Init Container 或者另外像 Sidecar 的方式去解决； 日志收集，日志收集本身是一个进程，是一个小容器，那么就可以把它打包进 Pod 里面去做这个收集工作； Debug 应用，实际上现在 Debug 整个应用都可以在应用 Pod 里面再次定义一个额外的小的 Container，它可以去 exec 应用 pod 的 namespace； 查看其他容器的工作状态。不再需要去 SSH 登陆到容器里去看，只要把监控组件装到额外的小容器里面就可以了，然后把它作为一个 Sidecar 启动起来，跟主业务容器进行协作，所以同样业务监控也都可以通过 Sidecar 方式来去做。 这种做法一个非常明显的优势就是在于其实将辅助功能从业务容器解耦了，所以我就能够独立发布 Sidecar 容器，并且更重要的是这个能力是可以重用的，即同样的一个监控 Sidecar 或者日志 Sidecar。 ","date":"2021-05-31","objectID":"/pod-notebook/:3:2","tags":["云原生","Kubernetes","Pod","设计模式"],"title":"Pod 概念与容器设计模式","uri":"/pod-notebook/"},{"categories":["云原生"],"content":"本文详细介绍了 K8s 的概念、核心功能、架构、核心概念与API，非常全面的 K8s 整理笔记","date":"2021-05-26","objectID":"/kubernetes-notebook/","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"什么是 KubernetesKubernetes 是一个自动化的容器编排平台，负责应用的部署、弹性以及管理，这些都是基于容器的。可以简称为 k8s，是将中间 8 个字母 “ubernete” 替换为 “8” 而导致的一个缩写。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:1:0","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"Kubernetes 的核心功能 服务发现与负载均衡； 容器的自动装箱，也就是调度scheduling，把一个容器放到一个集群的某一个机器上，K8s 会自动做存储的编排，让存储的生命周期和容器的生命周期能有一个连接； K8s 会做自动化的容器恢复。在一个集群中，经常会出现宿主机的问题或者说是 OS 的问题，导致容器本身的不可用，K8s 会自动恢复不可用的容器； K8s 会做应用的自动发布与应用的回滚，以及与应用相关的配置密文的管理； 对于 Job 类型任务，K8s 可以做批量执行； 为了让这个集群、这个应用更富有弹性，K8s 也支撑水平伸缩。 举个例子 调度 K8s 可以把用户提交的容器放到 K8s 管理的集群的某一台节点上去。K8s 的调度器是执行这项工作的组件，它会观察正在被调度的这个容器的大小、规格。 根据容器所需要的 CPU 和需要的 memory，在集群中找一台相对比较空闲的机器来放置。 自动修复 K8s 有一个节点健康检查的功能，会监测这个集群中所有的宿主机，当宿主机本身出现故障，或软件出现故障的时候，K8s 会发现这个情况。 下面，K8s 将运行在失败节点上的容器进行自动迁移，迁移到一个正在健康运行的宿主机上，来完成集群内容器的一个自动恢复。 水平伸缩 K8s 有业务负载检查能力，会监测业务上所承担的负载，如果这个业务本身的 CPU 利用率过高，或者响应时间过长，它可以对这个业务进行一次扩容。 比如，将这个业务分为三份，分布到不同的节点上，以此来提高响应的时间。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:2:0","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"Kubernetes 的架构K8s 架构是一个比较典型的二层架构和 server-client 架构。Master 作为中央的管控节点，会去与 Node 进行一个连接。 所有 UI 的 clients 和 user 侧的组件，只会和 Master 进行连接，把希望的状态或者想执行的命令下发给 Master，Master 会把这些命令或状态下发给相应的节点，执行最终的执行。 Kubernetes 架构 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:3:0","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的架构：MasterK8s 的 Master 包含四个主要的组件：API Server、Controller、Scheduler 以及 etcd。如下图所示： Kubernetes Master API Server： 顾名思义是用来处理 API 操作的，K8s 中所有组件都会和 API Server 进行连接，组件与组件之间一般不进行独立的连接，都依赖于 API Server 进行消息的传送； Controller： 控制器，用来完成对集群状态的一些管理； Scheduler： 调度器，依据用户提交 Container 所需的 CPU 和 memory 请求大小，找到合适的节点进行放置； etcd： 分布式存储系统，API Server 中所需的原信息放置在 etcd 中，etcd 本身也是一个高可用系统，通过 etcd 保证整个 K8s 的 Master 组件的高可用性。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:3:1","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的架构：NodeK8s 的 Node 是真正运行业务负载的，每个业务负载会以 Pod 的形式运行。一个 Pod 中运行的一个或者多个容器，真正去运行这些 Pod 的组件的是叫做 kubelet，也就是 Node 上最为关键的组件，它通过 API Server 接收到所需要 Pod 运行的状态，然后提交到 Container Runtime 组件中。 Kubernetes Node 在 OS 上去创建容器所需要运行的环境，最终把容器或者 Pod 运行起来，也需要对存储和网络进行管理。K8s 并不会直接进行网络存储的操作，他们会靠 Storage Plugin 或者是网络的 Plugin 来进行操作。用户自己或者云厂商都会去写相应的 Storage Plugin 或者 Network Plugin，去完成存储操作或网络操作。 在 K8s 自己的环境中，也会有 K8s 的 Network，它是为了提供 Service network 来进行搭网组网的。真正完成 service 组网的组件的是 Kube-proxy，它是利用了 iptable 的能力来进行组建 K8s 的 Network，就是 cluster network，以上就是 Node 上面的四个组件。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:3:2","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"组件之间的通信 Component Communication 用户可以通过 UI 或 CLI 提交一个 Pod 给 K8s 进行部署，这个 Pod 请求首先会通过 CLI 或者 UI 提交给 K8s API Server； API Server 会把这个信息写入到它的存储系统 etcd； 之后 Scheduler 会通过 API Server 的 watch 或者叫做 notification 机制得到这个信息：有一个 Pod 需要被调度； Scheduler 会根据内存状态进行一次调度决策，完成调度后通知 API Server； API Server 收到这次操作后会把结果再次写到 etcd 中； API Server 会通知相应节点进行这次 Pod 真正的执行启动； 相应节点的 kubelet 得到通知，调 Container runtime 去启动容器和环境，调度 Storage Plugin 去配置存储，network Plugin 去配置网络。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:3:3","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念与 API","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:0","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念：PodPod 是 K8s 的一个最小调度以及资源单元。用户可以通过 K8s 的 Pod API 生产一个 Pod，让 K8s 对这个 Pod 进行调度，也就是把它放在某一个 K8s 管理的节点上运行起来。一个 Pod 简单来说是对一组容器的抽象，它里面会包含一个或多个容器。 在 Pod 里面，可以去定义容器所需要运行的方式。比如说运行容器的 Command，以及运行容器的环境变量等等。Pod 这个抽象也给这些容器提供了一个共享的运行环境，它们会共享同一个网络环境，这些容器可以用 localhost 来进行直接的连接。而 Pod 与 Pod 之间，是互相有 isolation 隔离的。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:1","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念：VolumeVolume 就是卷的概念，它是用来管理 K8s 存储的，是用来声明在 Pod 中的容器可以访问文件目录的，一个卷可以被挂载在 Pod 中一个或者多个容器的指定路径下面。 Kubernetes Conception: Volume 而 Volume 本身是一个抽象的概念，一个 Volume 可以去支持多种的后端的存储。比如说 K8s 的 Volume 就支持了很多存储插件，它可以支持本地的存储，可以支持分布式的存储，比如说像 ceph，GlusterFS ；它也可以支持云存储，比如说阿里云上的云盘、AWS 上的云盘、Google 上的云盘等等。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:2","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念：DeploymentDeployment 是在 Pod 这个抽象上更为上层的一个抽象，它可以定义一组 Pod 的副本数目、以及这个 Pod 的版本。一般大家用 Deployment 这个抽象来做应用的真正的管理，而 Pod 是组成 Deployment 最小的单元。 Kubernetes Conception: Deployment Kubernetes 是通过 Controller，也就是我们刚才提到的控制器去维护 Deployment 中 Pod 的数目，它也会去帮助 Deployment 自动恢复失败的 Pod。 比如说我可以定义一个 Deployment，这个 Deployment 里面需要两个 Pod，当一个 Pod 失败的时候，控制器就会监测到，它重新把 Deployment 中的 Pod 数目从一个恢复到两个，通过再去新生成一个 Pod。通过控制器，我们也会帮助完成发布的策略。比如说进行滚动升级，进行重新生成的升级，或者进行版本的回滚。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:3","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念：ServiceService 提供了一个或者多个 Pod 实例的稳定访问地址。 比如在上面的例子中，我们看到：一个 Deployment 可能有两个甚至更多个完全相同的 Pod。对于一个外部的用户来讲，访问哪个 Pod 其实都是一样的，所以它希望做一次负载均衡，在做负载均衡的同时，我只想访问某一个固定的 VIP，也就是 Virtual IP 地址，而不希望得知每一个具体的 Pod 的 IP 地址。 我们刚才提到，这个 pod 本身可能 terminal go（终止），如果一个 Pod 失败了，可能会换成另外一个新的。 对一个外部用户来讲，提供了多个具体的 Pod 地址，这个用户要不停地去更新 Pod 地址，当这个 Pod 再失败重启之后，我们希望有一个抽象，把所有 Pod 的访问能力抽象成一个第三方的一个 IP 地址，实现这个的 K8s 的抽象就叫 Service。 实现 Service 有多种方式，K8s 支持 Cluster IP，上面我们讲过的 kuber-proxy 的组网，它也支持 nodePort、 LoadBalancer 等其他的一些访问的能力。 Kubernetes Conception: Service ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:4","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的核心概念：NamespaceNamespace 是用来做一个集群内部的逻辑隔离的，它包括鉴权、资源管理等。K8s 的每个资源，比如刚才讲的 Pod、Deployment、Service 都属于一个 Namespace，同一个 Namespace 中的资源需要命名的唯一性，不同的 Namespace 中的资源可以重名。 Kubernetes Conception: Namespace ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:5","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"K8s 的 API从 high-level 上看，K8s API 是由 HTTP+JSON 组成的：用户访问的方式是 HTTP，访问的 API 中 content 的内容是 JSON 格式的。 K8s 的 kubectl 也就是 command tool，Kubernetes UI，或者有时候用 curl，直接与 K8s 进行沟通，都是使用 HTTP + JSON 这种形式。 Kubernetes API 如图所示，对于这个 Pod 类型的资源，它的 HTTP 访问路径，就是 API，然后是 apiVersion: V1，之后是相应的 Namespace，以及 Pods 资源，最终是 Podname。 如果我们去提交一个 Pod，或者 get 一个 Pod 的时候，它的 content 内容都是用 JSON 或者是 YAML 表达的。上图中右侧是 yaml 的例子，在这个 yaml file 中，对 Pod 资源的描述也分为几个部分。 第一个部分，一般来讲会是 API 的 version。比如在这个例子中是 V1，它也会描述我在操作哪个资源；比如说我的 kind 如果是 pod，在 Metadata 中，就写上这个 Pod 的名字；比如说 nginx，我们也会给它打一些 label，我们等下会讲到 label 的概念。在 Metadata 中，有时候也会去写 annotation，也就是对资源的额外的一些用户层次的描述。 比较重要的一个部分叫做 Spec，Spec 也就是我们希望 Pod 达到的一个预期的状态。比如说它内部需要有哪些 container 被运行；比如说这里面有一个 nginx 的 container，它的 image 是什么？它暴露的 port 是什么？ 当我们从 Kubernetes API 中去获取这个资源的时候，一般来讲在 Spec 下面会有一个项目叫 status，它表达了这个资源当前的状态；比如说一个 Pod 的状态可能是正在被调度、或者是已经 running、或者是已经被 terminates，就是被执行完毕了。 刚刚在 API 之中，我们讲了一个比较有意思的 metadata 叫做“label”，这个 label 可以是一组 KeyValuePair。 这些 label 是可以被 selector，也就是选择器所查询的。这个能力实际上跟我们的 sql 类型的 select 语句是非常相似的。 通过 label，k8s 的 API 层就可以对这些资源进行一个筛选，那这些筛选也是 kubernetes 对资源的集合所表达默认的一种方式。 ","date":"2021-05-26","objectID":"/kubernetes-notebook/:4:6","tags":["云原生","Kubernetes"],"title":"Kubernetes 核心概念","uri":"/kubernetes-notebook/"},{"categories":["云原生"],"content":"什么是容器与镜像？容器的生命周期如何？容器与虚拟机的区别？","date":"2021-05-11","objectID":"/docker-notebook/","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"容器与镜像","date":"2021-05-11","objectID":"/docker-notebook/:1:0","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"什么是容器？在介绍容器的具体概念之前，先简单回顾一下操作系统是如何管理进程的。 操作系统与容器对比 首先，当我们登陆到操作系统之后，可以通过 ps 等操作看到各式各样的进程，这些进程包括系统自带的服务和用户的应用进程。那么，这些进程都有什么样的特点？ 这些进程可以相互看到、相互通信； 它们使用的是同一个文件系统，可以对同一个文件进行读写操作； 这些进程会使用相同的系统资源。 这样的三个特点会带来什么问题呢？ 因为这些进程相互看到并且进行通信，高级权限的进程可以攻击其他进程； 因为它们使用同一个文件系统，因此会带来两个问题：这些进程可以对于已有的数据进行增删改查，具有高级权限的进程可能会将其他进程的数据删除掉，破坏掉其他进程的正常运行；此外，进程与进程之间的依赖可能会存在冲突，如此一来就会给运维带来很大的压力； 因为这些进程使用同一个宿主机的资源，应用之间可能会存在资源抢占的问题，当一个应用需要消耗大量 CPU 和内存资源的时候，就可能破坏其他应用的运行，导致其他应用无法正常地提供服务。 针对上述的三个问题，如何为进程提供一个独立的运行环境呢？ 针对不同进程使用同一个文件系统所造成的问题而言，Linux 和 Unix 操作系统可以通过 chroot 系统调用将子目录变成根目录，达到视图级别的隔离；进程在 chroot 的帮助下可以具有独立的文件系统，对于这样的文件系统进行增删改查不会影响到其他进程； 因为进程之间相互可见并且可以相互通信，使用 Namespace 技术来实现进程在资源的视图上进行隔离。在 chroot 和 Namespace 的帮助下，进程就能够运行在一个独立的环境下了； 在独立的环境下，进程所使用的还是同一个操作系统的资源，一些进程可能会侵蚀掉整个系统的资源。为了减少进程彼此之间的影响，可以通过 Cgroup 来限制其资源使用率，设置其能够使用的 CPU 以及内存量。 那么，应该如何定义这样的进程集合呢？ 其实，容器就是一个视图隔离、资源可限制、独立文件系统的进程集合。所谓“视图隔离”就是能够看到部分进程以及具有独立的主机名等；控制资源使用率则是可以对于内存大小以及 CPU 使用个数等进行限制。容器就是一个进程集合，它将系统的其他资源隔离开来，具有自己独立的资源视图。 容器具有一个独立的文件系统，因为使用的是系统的资源，所以在独立的文件系统内不需要具备内核相关的代码或工具，我们只需要提供容器所需的二进制文件、配置文件以及依赖即可。只要容器运行时所需的文件集合都能够具备，那么这个容器就能够运行起来。 ","date":"2021-05-11","objectID":"/docker-notebook/:1:1","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"什么是镜像？综上所述，我们将这些容器运行时所需要的所有文件集合称之为容器镜像。 那么，一般通过什么方式来构建镜像的呢？通常情况下，我们会采用 Dockerfile 来构建镜像，这是因为 Dockerfile 提供了非常便利的语法，能够帮助我们很好地描述构建的每个步骤。当然，每个构建步骤都会对已有的文件系统进行操作，这样就会带来文件系统内容的变化，我们将这些变化称之为 changeset。当我们把构建步骤所产生的变化依次作用到一个空文件夹上，就能得到一个完整的镜像。 changeset 分层示意 changeset 的分层以及复用特点能够带来几点优势： 提高分发效率，简单试想一下，对于大镜像而言，如果将其拆分成各个小块就能够提高镜像的分发效率，这是因为镜像拆分之后就可以并行下载。 因为数据是相互共享的，也就意味着当本地存储上包含了一些数据的时候，只需要下载本地没有的数据即可。 因为镜像数据是共享的，因此可以节约大量的磁盘空间，简单设想一下，当本地存储具有了 alpine 镜像和 golang 镜像，在没有复用能力之前，alpine 镜像具有 5M 大小，Golang 镜像有 300M 大小，因此就会占用 305M 空间；而当具有复用能力之后，只需要 300 M 空间即可，这是因为 Golang 镜像是基于 alpine 镜像构建的。 ","date":"2021-05-11","objectID":"/docker-notebook/:1:2","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"如何构建镜像？如下所示的 Dockerfile 适用于描述如何构建 Golang 应用的。 # base on golang:1.12-alpine imageFROMgolang:1.12-alpine# setting current working dir (PWD -\u003e /go/src/app)WORKDIR/go/src/app# copy local files into /go/src/appCOPY ..# get all the dependenciesRUN go get -d -v ./...# build the application and install itRUN go install -v ./...# by default, run the appCMD[\"app\"] 如上代码所示： FROM 行表示以下的构建步骤基于什么镜像进行构建，正如前述，镜像是可以复用的； WORKDIR 行表示会把接下来的构建步骤都在哪一个相应的具体目录下进行，其起到的作用类似于 shell 里面的 cd； COPY 行表示的是可以将宿主机上的文件拷贝到容器镜像内； RUN 行表示在具体的文件系统内执行相应的动作。当我们运行完毕后就可以得到一个应用了； CMD 行表示使用镜像时的默认程序名字。 当有了 Dockerfile 之后，就可以通过 docker build 命令构建出所需要的应用。构建出的结果存储在本地，一般情况下，镜像构建会在打包机或者其他的隔离环境下完成。 ","date":"2021-05-11","objectID":"/docker-notebook/:1:3","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"如何运行容器 容器运行过程 运行一个容器一般情况下分为三步： 第一步：从镜像仓库中将相应的镜像下载下来； 第二步：当镜像下载完成之后，就可以通过 docker images 来查看本地镜像； 第三步：当选中镜像之后，就可以通过 docker run 来运行这个镜像得到想要的容器，当然可以通过多次运行得到多个容器。一个镜像就相当于是一个模板，一个容器就像是一个具体的运行实例，因此镜像就具有了一次构建、到处运行的特点。 ","date":"2021-05-11","objectID":"/docker-notebook/:1:4","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"容器的生命周期容器是一组具有隔离特性的进程集合，在使用 docker run 的时候会选择一个镜像来提供独立的文件系统并指定相应的运行程序。这里指定的运行程序称之为 initial 进程，这个 initial 进程启动的时候，容器也会随之启动，当 initial 进程退出的时候，容器也会随之退出。 因此，可以认为容器的生命周期和 initial 进程的生命周期是一致的。当然，因为容器内不只有这样的一个 initial 进程，initial 进程本身也可以产生其他的子进程或者通过 docker exec 产生出来的运维操作，也属于 initial 进程管理的范围内。当 initial 进程退出的时候，所有的子进程也会随之退出，这样也是为了防止资源的泄漏。 但是这样的做法也会存在一些问题，首先应用里面的程序往往是有状态的，其可能会产生一些重要的数据，当一个容器退出被删除之后，数据也就会丢失了，这对于应用方而言是不能接受的，所以需要将容器所产生出来的重要数据持久化下来。容器能够直接将数据持久化到指定的目录上，这个目录就称之为数据卷。 数据卷有一些特点，其中非常明显的就是数据卷的生命周期是独立于容器的生命周期的，也就是说容器的创建、运行、停止、删除等操作都和数据卷没有任何关系，因为它是一个特殊的目录，是用于帮助容器进行持久化的。简单而言，我们会将数据卷挂载到容器内，这样一来容器就能够将数据写入到相应的目录里面了，而且容器的退出并不会导致数据的丢失。 通常情况下，数据卷管理主要有两种方式： 第一种是通过 bind 的方式，直接将宿主机的目录直接挂载到容器内；这种方式比较简单，但是会带来运维成本，因为其依赖于宿主机的目录，需要对于所有的宿主机进行统一管理。 第二种是将目录管理交给运行引擎。 # bind host dir into container docker run -v /tmp:/tmp busybox:1.25 sh -c \"data \u003e /tmp/demo.log\" # check result cat /tmp/demo.log # let it handled by docker container engine docker create volume demo # demo is volume name docker run -v demo:/tmp busybox:1.25 sh -c \"date \u003e /tmp/demo.log\" # check result docker run -v demo:/tmp busybox:1.25 sh -c \"cat /tmp/demo.log\" ","date":"2021-05-11","objectID":"/docker-notebook/:2:0","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"容器项目架构：moby 容器引擎架构moby 是目前最流行的容器管理引擎，moby daemon 会对上提供有关于容器、镜像、网络以及 Volume的管理。moby daemon 所依赖的最重要的组件就是 containerd，containerd 是一个容器运行时管理引擎，其独立于 moby daemon ，可以对上提供容器、镜像的相关管理。 moby 引擎 containerd 底层有 containerd shim 模块，其类似于一个守护进程，这样设计的原因有几点： 首先，containerd 需要管理容器生命周期，而容器可能是由不同的容器运行时所创建出来的，因此需要提供一个灵活的插件化管理。而 shim 就是针对于不同的容器运行时所开发的，这样就能够从 containerd 中脱离出来，通过插件的形式进行管理。 其次，因为 shim 插件化的实现，使其能够被 containerd 动态接管。如果不具备这样的能力，当 moby daemon 或者 containerd daemon 意外退出的时候，容器就没人管理了，那么它也会随之消失、退出，这样就会影响到应用的运行。 最后，因为随时可能会对 moby 或者 containerd 进行升级，如果不提供 shim 机制，那么就无法做到原地升级，也无法做到不影响业务的升级，因此 containerd shim 非常重要，它实现了动态接管的能力。 ","date":"2021-05-11","objectID":"/docker-notebook/:3:0","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"容器 VS VM 容器与虚拟机对比示意 容器 VM 进程级的隔离效果比 VM 要差 因为每个 Guest OS 都有独立的内核，所以 VM 提供一个更好的隔离效果 所需磁盘空间小 所需磁盘空间大 响应时间快，因为文件隔离都是进程级别的 响应时间慢，因为一部分计算资源交给虚拟化 ","date":"2021-05-11","objectID":"/docker-notebook/:4:0","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"总结 容器是一个进程集合，具有自己独特的视图视角； 镜像是容器所需要的所有文件集合，其具备一次构建、到处运行的特点； 容器的生命周期和 initial 进程的生命周期是一样的； 容器和 VM 相比，各有优劣，容器技术在向着强隔离方向发展。 ","date":"2021-05-11","objectID":"/docker-notebook/:5:0","tags":["Docker","Image","云原生"],"title":"容器基本概念","uri":"/docker-notebook/"},{"categories":["云原生"],"content":"学好 Yaml 语法，打好 K8s 基础","date":"2021-05-08","objectID":"/yaml-notebook/","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"简单说明Yaml 是一个可读性高，用来表达数据序列的格式。Yaml 的意思是：仍是一种标记语言，强调这种语言以数据做为中心，而不是以标记语言为重点 ","date":"2021-05-08","objectID":"/yaml-notebook/:1:0","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"基本语法 缩进时不允许使用 Tab 键，只允许使用空格 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 标识注释，从这个字符一直到行尾，都会被解释器忽略 ","date":"2021-05-08","objectID":"/yaml-notebook/:2:0","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"Yaml 支持的数据结构 对象：键值对的集合，又称为映射mapping映射mapping/ 哈希hash哈希hash/ 字典dictionary字典dictionary 数组：一组按次序排列的值，又称为序列sequence序列sequence/ 列表list列表list 纯量scalars纯量scalars：单个的、不可再分的值 ","date":"2021-05-08","objectID":"/yaml-notebook/:3:0","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"对象类型：对象的一组键值对，使用冒号结构表示 name:Oliverage:24 Yaml 也允许另一种写法，将所有键值对写成一个行内对象 hash:{name: Oliver, age:24} Yaml 允许存在层级键值对 major:name:computername:law ","date":"2021-05-08","objectID":"/yaml-notebook/:3:1","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"数组类型：一组连词线开头的行，构成一个数组 Sport:- Swim- Run 数组也可以采用行内表示法 Sport:[Swim, Run] 数组的一个相对复杂的实现方式 products:-id:1name:eggprice:8-id:2name:meatprice:33 ","date":"2021-05-08","objectID":"/yaml-notebook/:3:2","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"复合结构：对象和数组可以结合使用，形成复合结构 sport:- swim- run- basketballcity:haidian:beijing.haidianjinan:shandong.jinanhefei:anhui.hefei ","date":"2021-05-08","objectID":"/yaml-notebook/:3:3","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"纯量：纯量是最基本的、不可再分的值 字符串、布尔值、整数、浮点数、Null 时间、日期 数值 数值直接以字面量的形式表示 number:12.30 布尔值 布尔值用 true 和 false 表示 isSet:true Null null 用～表示 parent:~ 时间 时间采用 ISO8601 格式 iso8601:2001-12-14t21:59:43.10-05:00 日期 日期采用 ISO8601 格式的年、月、日表示 date:1976-07-31 类型转换 Yaml 允许使用两个感叹号，强制转换数据类型 e:!!str123f:!!strtrue 字符串 字符串默认不使用引号表示 str:我是字符串 如果字符串之中包含空格或特殊字符，需要放在引号中 str:'内容： 字符串' 单引号之中如果还有单引号，必须连续使用两个单引号转义 str:'labor’‘s day' 字符串可以写成多行，从第二行开始，必须有一个单空格缩进，换行符会被转为空格 str:我是一段多行字符串 多行字符串可以使用 | 保留换行符，也可以使用 \u003e 折叠换行 this:|Foo Barthat:\u003eFoo Bar + 表示保留文字块末尾的换行，- 表示删除字符串末尾的换行 s1:|Foos2:|+Foos3:|-Foo ","date":"2021-05-08","objectID":"/yaml-notebook/:3:4","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["云原生"],"content":"引用\u0026 锚点和 * 别名，可以用来引用: defaults:\u0026defaultsadapter:postgreshost:localhostdevelopment:database:myapp_development\u003c\u003c:*defaultstest:database:myapp_test\u003c\u003c:*defaults 相当于 defaults:adapter:postgreshost:localhostdevelopment:database:myapp_developmentadapter:postgreshost:localhosttest:database:myapp_testadapter:postgreshost:localhost \u0026 用来建立锚点（defaults），« 表示合并到当前数据，* 用来引用锚点。 ","date":"2021-05-08","objectID":"/yaml-notebook/:4:0","tags":["Yaml","云原生","Kubernetes"],"title":"最全 Yaml 语法详解","uri":"/yaml-notebook/"},{"categories":["LCTT"],"content":"Git 提供了几种方式可以帮你快速查看提交中哪些文件被改变。","date":"2021-04-24","objectID":"/git-whatchanged/","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"如果你每天使用 Git，应该会提交不少改动。如果你每天和其他人在一个项目中使用 Git，假设 每个人 每天的提交都是安全的，你会意识到 Git 日志会变得多么混乱，似乎永恒地滚动着变化，却没有任何迹象表明修改了什么。 那么，你该怎样查看指定提交中文件发生哪些变化？这比你想的容易。 ","date":"2021-04-24","objectID":"/git-whatchanged/:0:0","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"查看提交中文件发生的变化要想知道指定提交中哪些文件发生变化，可以使用 git log --raw 命令。这是发现一个提交影响了哪些文件的最快速、最方便的方法。git log 命令一般都没有被充分利用，主要是因为它有太多的格式化选项，许多用户在面对很多选择以及在一些情况下不明所以的文档时，会望而却步。 然而，Git 的日志机制非常灵活，--raw 选项提供了当前分支中的提交日志，以及更改的文件列表。 以下是标准的 git log 输出： $ git log commit fbbbe083aed75b24f2c77b1825ecab10def0953c (HEAD -\u003e dev, origin/dev) Author: tux \u003ctux@example.com\u003e Date: Sun Nov 5 21:40:37 2020 +1300 exit immediately from failed download commit 094f9948cd995acfc331a6965032ea0d38e01f03 (origin/master, master) Author: Tux \u003ctux@example.com\u003e Date: Fri Aug 5 02:05:19 2020 +1200 export makeopts from etc/example.conf commit 76b7b46dc53ec13316abb49cc7b37914215acd47 Author: Tux \u003ctux@example.com\u003e Date: Sun Jul 31 21:45:24 2020 +1200 fix typo in help message 即使作者在提交消息中指定了哪些文件发生变化，日志也相当简洁。 以下是 git log --raw 输出： $ git log --raw commit fbbbe083aed75b24f2c77b1825ecab10def0953c (HEAD -\u003e dev, origin/dev) Author: tux \u003ctux@example.com\u003e Date: Sun Nov 5 21:40:37 2020 +1300 exit immediately from failed download :100755 100755 cbcf1f3 4cac92f M src/example.lua commit 094f9948cd995acfc331a6965032ea0d38e01f03 (origin/master, master) Author: Tux \u003ctux@example.com\u003e Date: Fri Aug 5 02:05:19 2020 +1200 export makeopts from etc/example.conf :100755 100755 4c815c0 cbcf1f3 M src/example.lua :100755 100755 71653e1 8f5d5a6 M src/example.spec :100644 100644 9d21a6f e33caba R100 etc/example.conf etc/example.conf-default commit 76b7b46dc53ec13316abb49cc7b37914215acd47 Author: Tux \u003ctux@example.com\u003e Date: Sun Jul 31 21:45:24 2020 +1200 fix typo in help message :100755 100755 e253aaf 4c815c0 M src/example.lua 这会准确告诉你哪个文件被添加到提交中，哪些文件发生改变（A 是添加，M 是修改，R 是重命名，D 是删除）。 ","date":"2021-04-24","objectID":"/git-whatchanged/:1:0","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"Git whatchangedgit whatchanged 命令是一个遗留命令，它的前身是日志功能。文档说用户不应该用该命令替代 git log --raw，并且暗示它实质上已经被废弃了。不过，我还是觉得它是一个很有用的捷径，可以得到同样的输出结果（尽管合并提交的内容不包括在内），如果它被删除的话，我打算为它创建一个别名。如果你只想查看已更改的文件，不想在日志中看到合并提交，可以尝试 git whatchanged 作为简单的助记符。 ","date":"2021-04-24","objectID":"/git-whatchanged/:2:0","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"查看变化你不仅可以看到哪些文件发生更改，还可以使用 git log 显示文件中发生了哪些变化。你的 Git 日志可以生成一个内联差异，用 --patch 选项可以逐行显示每个文件的所有更改： commit 62a2daf8411eccbec0af69e4736a0fcf0a469ab1 (HEAD -\u003e master) Author: Tux \u003cTux@example.com\u003e Date: Wed Mar 10 06:46:58 2021 +1300 commit diff --git a/hello.txt b/hello.txt index 65a56c3..36a0a7d 100644 --- a/hello.txt +++ b/hello.txt @@ -1,2 +1,2 @@ Hello -world +opensource.com 在这个例子中，“world” 这行字从 hello.txt 中删掉，“opensource.com” 这行字则添加进去。 如果你需要在其他地方手动进行相同的修改，这些补丁patch可以与常见的 Unix 命令一起使用，例如 diff 与 patch。补丁也是一个好方法，可以总结指定提交中引入新信息的重要部分内容。当你在冲刺阶段引入一个 bug 时，你会发现这里的内容就是非常有价值的概述。为了更快地找到错误的原因，你可以忽略文件中没有更改的部分，只检查新代码。 ","date":"2021-04-24","objectID":"/git-whatchanged/:3:0","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"用简单命令得到复杂的结果你不必理解引用、分支和提交哈希，就可以查看提交中更改了哪些文件。你的 Git 日志旨在向你报告 Git 的活动，如果你想以特定方式格式化它或者提取特定的信息，通常需要费力地浏览许多文档来组合出正确的命令。幸运的是，关于 Git 历史记录最常用的请求之一只需要一两个选项：--raw 与 --patch。如果你不记得 --raw，就想想“Git，什么改变了？”，然后输入 git whatchanged。 via: https://opensource.com/article/21/4/git-whatchanged 作者：Seth Kenlon 选题：lujun9972 译者：DCOLIVERSUN 校对：wxy 本文由 LCTT 原创编译，Linux中国 荣誉推出 ","date":"2021-04-24","objectID":"/git-whatchanged/:4:0","tags":["LCTT","Git"],"title":"查看 Git 提交中发生了什么变化","uri":"/git-whatchanged/"},{"categories":["LCTT"],"content":"一款简洁、好用的 Feed 阅读器，让你及时跟进想看的内容","date":"2021-04-13","objectID":"/newsflash/","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":" 有些人可能认为 RSS 阅读器已经不再，但它们仍然坚持在这里，特别是当你不想让大科技算法来决定你应该阅读什么的时候。Feed 阅读器可以帮你自助选择阅读来源。 我最近遇到一个很棒的 RSS 阅读器 NewsFlash。它支持通过基于网页的 Feed 阅读器增加 feed，例如 Feedly 和 NewsBlur。这是一个很大的安慰，因为如果你已经使用这种服务，就不必人工导入 feed，这节省了你的工作。 NewsFlash 恰好是 FeedReadeer 的精神继承者，原来的 FeedReader 开发人员也参与其中。 如果你正在找适用的 RSS 阅读器，我们整理了 Linux Feed 阅读器 列表供您参考。 ","date":"2021-04-13","objectID":"/newsflash/:0:0","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":"NewsFlash: 一款补充网页 RSS 阅读器账户的 Feed 阅读器 请注意，NewsFlash 并不只是针对基于网页的 RSS feed 账户量身定做的，你也可以选择使用本地 RSS feed，而不必在多设备间同步。 不过，如果你在用是任何一款支持的基于网页的 feed 阅读器，那么 NewsFlash 特别有用。 这里，我将重点介绍 NewsFlash 提供的一些功能。 ","date":"2021-04-13","objectID":"/newsflash/:1:0","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":"NewsFlash 功能 支持桌面通知 快速搜索、过滤 支持标签 便捷、可重定义的键盘快捷键 本地 feed OPML 文件导入/导出 无需注册即可在 Feedly 库中轻松找到不同 RSS Feed 支持自定义字体 支持多主题（包括深色主题） 启动/禁止缩略图 细粒度调整定期同步间隔时间 支持基于网页的 Feed 账户，例如 Feedly、Fever、NewsBlur、feedbin、Miniflux 除上述功能外，当你调整窗口大小时，还可以打开阅读器视图，这是一个细腻的补充功能。 账户重新设置也很容易，这将删除所有本地数据。是的，你可以手动清除缓存并设置到期时间，并为你关注的所有 feed 设置一个用户数据存在本地的到期时间。 ","date":"2021-04-13","objectID":"/newsflash/:2:0","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":"在 Linux 上安装 NewsFlash你无法找到适用于各种 Linux 发行版的官方软件包，只有 Flatpak。 对于 Arch 用户，可以从 AUR 下载。 幸运的是，Flatpak 软件包可以让你轻松在 Linux 发行版上安装 NewsFlash。具体请参阅我们的 Flatpak 指南。 你可以参考 NewsFlash 的 GitLab 页面 去解决大部分问题。 ","date":"2021-04-13","objectID":"/newsflash/:3:0","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":"结束语我现在用 NewsFlash 作为桌面本地解决方案，不用基于网页的服务。你可以通过直接导出 OPML 文件在移动 feed 应用上得到相同的 feed。这已经被我验证过了。 用户界面易于使用，也提供了数一数二的新版 UX。虽然这个 RSS 阅读器看似简单，但提供了你可以找到的所有重要功能。 你怎么看 NewsFlash？你喜欢用其他类似产品吗？欢迎在评论区中分享你的想法。 via: https://itsfoss.com/newsflash-feedreader/ 作者：Ankush Das 选题：lujun9972 译者：DCOLIVERSUN 校对：wxy 本文由 LCTT 原创编译，Linux中国 荣誉推出 ","date":"2021-04-13","objectID":"/newsflash/:4:0","tags":["LCTT","RSS","Feed"],"title":"NewsFlash: 一款支持 Feedly 的现代开源 Feed 阅读器","uri":"/newsflash/"},{"categories":["LCTT"],"content":"没有提交原始 Dockerfile 的时候，可以通过逆向工程查看镜像构建过程。","date":"2021-04-05","objectID":"/reverse-engineering-a-docker-image/","tags":["LCTT","Docker","逆向工程"],"title":"一次 Docker 镜像的逆向工程","uri":"/reverse-engineering-a-docker-image/"},{"categories":["LCTT"],"content":"这要从一次咨询的失误说起：政府组织 A 让政府组织 B 开发一个 Web 应用程序。政府机构 B 把部分工作外包给某个人。后来，项目的托管和维护被外包给一家私人公司 C。C 公司发现，之前外包的人（已经离开很久了）构建了一个自定义的 Docker 镜像，并将其成为系统构建的依赖项，但这个人没有提交原始的 Dockerfile。C 公司有合同义务管理这个 Docker 镜像，可是他们他们没有源代码。C 公司偶尔叫我进去做各种工作，所以处理一些关于这个神秘 Docker 镜像的事情就成了我的工作。 幸运的是，Docker 镜像的格式比想象的透明多了。虽然还需要做一些侦查工作，但只要解剖一个镜像文件，就能发现很多东西。例如，这里有一个 Prettier 代码格式化 的镜像可供快速浏览。 首先，让 Docker 守护进程daemon拉取镜像，然后将镜像提取到文件中： docker pull tmknom/prettier:2.0.5 docker save tmknom/prettier:2.0.5 \u003e prettier.tar 是的，该文件只是一个典型 tarball 格式的归档文件： $ tar xvf prettier.tar 6c37da2ee7de579a0bf5495df32ba3e7807b0a42e2a02779206d165f55f1ba70/ 6c37da2ee7de579a0bf5495df32ba3e7807b0a42e2a02779206d165f55f1ba70/VERSION 6c37da2ee7de579a0bf5495df32ba3e7807b0a42e2a02779206d165f55f1ba70/json 6c37da2ee7de579a0bf5495df32ba3e7807b0a42e2a02779206d165f55f1ba70/layer.tar 88f38be28f05f38dba94ce0c1328ebe2b963b65848ab96594f8172a9c3b0f25b.json a9cc4ace48cd792ef888ade20810f82f6c24aaf2436f30337a2a712cd054dc97/ a9cc4ace48cd792ef888ade20810f82f6c24aaf2436f30337a2a712cd054dc97/VERSION a9cc4ace48cd792ef888ade20810f82f6c24aaf2436f30337a2a712cd054dc97/json a9cc4ace48cd792ef888ade20810f82f6c24aaf2436f30337a2a712cd054dc97/layer.tar d4f612de5397f1fc91272cfbad245b89eac8fa4ad9f0fc10a40ffbb54a356cb4/ d4f612de5397f1fc91272cfbad245b89eac8fa4ad9f0fc10a40ffbb54a356cb4/VERSION d4f612de5397f1fc91272cfbad245b89eac8fa4ad9f0fc10a40ffbb54a356cb4/json d4f612de5397f1fc91272cfbad245b89eac8fa4ad9f0fc10a40ffbb54a356cb4/layer.tar manifest.json repositories 如你所见，Docker 在命名时经常使用哈希hash。我们看看 manifest.json。它是以难以阅读的压缩 JSON 写的，不过 JSON 瑞士军刀 jq 可以很好地打印 JSON： $ jq . manifest.json [ { \"Config\": \"88f38be28f05f38dba94ce0c1328ebe2b963b65848ab96594f8172a9c3b0f25b.json\", \"RepoTags\": [ \"tmknom/prettier:2.0.5\" ], \"Layers\": [ \"a9cc4ace48cd792ef888ade20810f82f6c24aaf2436f30337a2a712cd054dc97/layer.tar\", \"d4f612de5397f1fc91272cfbad245b89eac8fa4ad9f0fc10a40ffbb54a356cb4/layer.tar\", \"6c37da2ee7de579a0bf5495df32ba3e7807b0a42e2a02779206d165f55f1ba70/layer.tar\" ] } ] 请注意，这三个层Layer对应三个以哈希命名的目录。我们以后再看。现在，让我们看看 Config 键指向的 JSON 文件。它有点长，所以我只在这里转储第一部分： $ jq . 88f38be28f05f38dba94ce0c1328ebe2b963b65848ab96594f8172a9c3b0f25b.json | head -n 20 { \"architecture\": \"amd64\", \"config\": { \"Hostname\": \"\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"--help\" ], \"ArgsEscaped\": true, \"Image\": \"sha256:93e72874b338c1e0734025e1d8ebe259d4f16265dc2840f88c4c754e1c01ba0a\", 最重要的是 history 列表，它列出了镜像中的每一层。Docker 镜像由这些层堆叠而成。Dockerfile 中几乎每条命令都会变成一个层，描述该命令对镜像所做的更改。如果你执行 RUN script.sh 命令创建了 really_big_file，然后用 RUN rm really_big_file 命令删除文件，Docker 镜像实际生成两层：一个包含 really_big_file，一个包含 .wh.really_big_file 记录来删除它。整个镜像文件大小不变。这就是为什么你会经常看到像 RUN script.sh \u0026\u0026 rm really_big_file 这样的 Dockerfile 命令链接在一起——它保障所有更改都合并到一层中。 以下是该 Docker 镜像中记录的所有层。注意，大多数层不改变文件系统镜像，并且 empty_layer 标记为 true。以下只有三个层是非空的，与我们之前描述的相符。 $ jq .history 88f38be28f05f38dba94ce0c1328ebe2b963b65848ab96594f8172a9c3b0f25b.json [ { \"created\": \"2020-04-24T01:05:03.608058404Z\", \"created_by\": \"/bin/sh -c #(nop) ADD file:b91adb67b670d3a6ff9463e48b7def903ed516be66fc4282d22c53e41512be49 in / \" }, { \"created\": \"2020-04-24T01:05:03.92860976Z\", \"created_by\": \"/bin/sh -c #(nop) CMD [\\\"/bin/sh\\\"]\", \"empty_layer\": true }, { \"created\": \"2020-04-29T06:34:06.617130538Z\", \"created_by\": \"/bin/sh -c #(nop) ARG BUILD_DATE\", \"empty_layer\": true }, { \"created\": \"2020-04-29T06:34:07.020521808Z\", \"created_by\": \"/bin/sh -c #(nop) ARG VCS_REF\", \"empty_layer\": true }, { \"created\": \"2020-04-29T06:34:07.36915054Z\", \"created_by\": \"/bin/sh -c #(nop) ARG VERSION\", \"empty_layer\": true }, { \"created\": \"2020-04-29T06:34:07.708820086Z\", \"created_by\": \"/bin/sh -c #(nop) ARG REPO_NAME\", \"empty_layer\": true }, { \"created\": \"2020-04-29T06:34:08.06429638Z\", \"created_by\": \"/bin/sh -c #(nop) LABEL org.label-schema.vendor=tmknom org.label-schema.name=tmknom/prettier org.label-schema.description=Pre","date":"2021-04-05","objectID":"/reverse-engineering-a-docker-image/:0:0","tags":["LCTT","Docker","逆向工程"],"title":"一次 Docker 镜像的逆向工程","uri":"/reverse-engineering-a-docker-image/"},{"categories":["LCTT"],"content":"字典数据结构可以帮助你快速访问信息。","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"字典是 Python 编程语言使用的数据结构。一个 Python 字典由多个键值对组成；每个键值对将键映射到其关联的值上。 例如你是一名老师，想把学生姓名与成绩对应起来。你可以使用 Python 字典，将学生姓名映射到他们关联的成绩上。此时，键值对中键是姓名，值是对应的成绩。 如果你想知道某个学生的考试成绩，你可以从字典中访问。这种快捷查询方式可以为你节省解析整个列表找到学生成绩的时间。 本文介绍了如何通过键访问对应的字典值。学习前，请确保你已经安装了 Anaconda 包管理器 和 Jupyter 笔记本。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:0:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"1、在 Jupyter 中打开一个新的笔记本首先在 Web 浏览器中打开并运行 Jupyter。然后， 转到左上角的 “File”。 选择 “New Notebook”，点击 “Python 3”。 Create Jupyter notebook 开始时，新建的笔记本是无标题的，你可以将其重命名为任何名称。我为我的笔记本取名为 “OpenSource.com Data Dictionary Tutorial”。 笔记本中标有行号的位置就是你写代码的区域，也是你输入的位置。 在 macOS 上，可以同时按 Shift + Return 键得到输出。在创建新的代码区域前，请确保完成上述动作；否则，你写的任何附加代码可能无法运行。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:1:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"2、新建一个键值对在字典中输入你希望访问的键与值。输入前，你需要在字典上下文中定义它们的含义： empty_dictionary = {} grades = { \"Kelsey\": 87, \"Finley\": 92 } one_line = {a: 1, b: 2} Code for defining key-value pairs in the dictionary 这段代码让字典将特定键与其各自的值关联起来。字典按名称存储数据，从而可以更快地查询。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:2:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"3、通过键访问字典值现在你想查询指定的字典值；在上述例子中，字典值指特定学生的成绩。首先，点击 “Insert” 后选择 “Insert Cell Below”。 Inserting a new cell in Jupyter 在新单元格中，定义字典中的键与值。 然后，告诉字典打印该值的键，找到需要的值。例如，查询名为 Kelsey 的学生的成绩： # 访问字典中的数据 grades = { \"Kelsey\": 87, \"Finley\": 92 } print(grades[\"Kelsey\"]) 87 Code to look for a specific value 当你查询 Kelsey 的成绩（也就是你想要查询的值）时，如果你用的是 macOS，只需要同时按 Shift+Return 键。 你会在单元格下方看到 Kelsey 的成绩。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:3:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"4、更新已有的键当把一位学生的错误成绩添加到字典时，你会怎么办？可以通过更新字典、存储新值来修正这类错误。 首先，选择你想更新的那个键。在上述例子中，假设你错误地输入了 Finley 的成绩，那么 Finley 就是你需要更新的键。 为了更新 Finley 的成绩，你需要在下方插入新的单元格，然后创建一个新的键值对。同时按 Shift+Return 键打印字典全部信息： grades[\"Finley\"] = 90 print(grades) {'Kelsey': 87; \"Finley\": 90} Code for updating a key 单元格下方输出带有 Finley 更新成绩的字典。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:4:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"5、添加新键假设你得到一位新学生的考试成绩。你可以用新键值对将那名学生的姓名与成绩补充到字典中。 插入新的单元格，以键值对形式添加新学生的姓名与成绩。当你完成这些后，同时按 Shift+Return 键打印字典全部信息： grades[\"Alex\"] = 88 print(grades) {'Kelsey': 87, 'Finley': 90, 'Alex': 88} Add a new key 所有的键值对输出在单元格下方。 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:5:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":["LCTT"],"content":"使用字典请记住，键与值可以是任意数据类型，但它们很少是扩展数据类型non-primitive types。此外，字典不能以指定的顺序存储、组织里面的数据。如果你想要数据有序，最好使用 Python 列表，而非字典。 如果你考虑使用字典，首先要确认你的数据结构是否是合适的，例如像电话簿的结构。如果不是，列表、元组、树或者其他数据结构可能是更好的选择。 via: https://opensource.com/article/21/3/dictionary-values-python 作者：Lauren Maffeo 选题：lujun9972 译者：DCOLIVERSUN 校对：wxy 本文由 LCTT 原创编译，Linux中国 荣誉推出 ","date":"2021-04-01","objectID":"/learn-python-dictionary-with-jupyter/:6:0","tags":["LCTT","Jupyter","Python","数据结构"],"title":"用 Jupyter 学习 Python 字典","uri":"/learn-python-dictionary-with-jupyter/"},{"categories":null,"content":" 各位看官，如果本站内容对您有帮助，欢迎赞助我一杯咖啡☕️ 毕竟，来都来了 😁 支付宝 微信支付 ","date":"2021-03-31","objectID":"/donate/:0:0","tags":null,"title":"Donate","uri":"/donate/"},{"categories":["LCTT"],"content":"开启技术翻译道路","date":"2021-03-31","objectID":"/lctt-foreword/","tags":["LCTT","开源"],"title":"LCTT项目序言","uri":"/lctt-foreword/"},{"categories":["LCTT"],"content":"什么是LCTTLCTT 是 “Linux中国” 的翻译组，负责从国外优秀媒体翻译 Linux 相关的技术、资讯、杂文等内容。 ","date":"2021-03-31","objectID":"/lctt-foreword/:1:0","tags":["LCTT","开源"],"title":"LCTT项目序言","uri":"/lctt-foreword/"},{"categories":["LCTT"],"content":"加入 LCTT 的初衷前不久我参与了《On Java 8》的翻译工作。在这个过程中，我需要查询很多专业名词的翻译，往往纠结句子中个别单词怎么翻译比较好。虽然有痛苦，但也为我带来欢乐。我热衷于揣摩作者的表达意图，常常想他是在怎样的技术储备下写出这本书。更重要的是，在翻译过程中我对 Java 有了新的认识，而这些认识也是我之前看中文文献时疑惑的点。 不难看出，这次翻译工作为我带来了技术上的成长，也让我看到个人的能力不足。读研期间，导师让我阅读原始文献，避免受到翻译软件、译者的干扰，直接学习作者表达的内容。我想，在未来的工作中，我的阅读侧重点也应该倾向于原始文献。于是，我开始查找国内有没有优秀的国外文献阅读、学习的社区，LCTT 就是这个时候映入我的眼帘。 LCTT 社区中的文章由专业的选题人员选择，一般为技术访谈、博客等。后期由译者翻译、校对人员审核后再发布到 Linux中国社区。我作为译者参与社区工作，选择自己感兴趣的文章，翻译后提交给社区。我的工作让我见识到国外优秀的技术文章，它们介绍了实用的代码工具、翔实的技术总结、完整的调试过程，这些都可以拓宽我的技术视野，提升我的技术实力。同时，我也希望我的工作能为国内的开发者提供一些帮助，让大家在技术分享中共同进步。 ","date":"2021-03-31","objectID":"/lctt-foreword/:2:0","tags":["LCTT","开源"],"title":"LCTT项目序言","uri":"/lctt-foreword/"},{"categories":["LCTT"],"content":"我的工作展示欢迎大家访问我的 LCTT 主页 👉 Qian.Sun 我负责的所有文章均在主页中列出 ","date":"2021-03-31","objectID":"/lctt-foreword/:3:0","tags":["LCTT","开源"],"title":"LCTT项目序言","uri":"/lctt-foreword/"},{"categories":["LCTT"],"content":"特别说明 本博客中 LCTT 专栏仅转载本人翻译的文章； 翻译工作和译文发表仅用于学习和交流目的； 翻译工作遵照 CC-BY-NC-SA 协议规定，如果我的工作有侵犯到您的权益，请及时联系我； 转载敬请在正文中标注并保留原文/译文链接和作者/译者等信息； LCTT 专栏内所有译文由 LCTT 原创翻译，Linux 中国首发。 ","date":"2021-03-31","objectID":"/lctt-foreword/:4:0","tags":["LCTT","开源"],"title":"LCTT项目序言","uri":"/lctt-foreword/"},{"categories":null,"content":"我是谁大家好！👏 我是孙乾。自小成长在美丽的威海，如今居住在北京。 现在在中国科学院计算技术研究所，攻读计算机系统结构💻硕士学位。本科就读大连海事大学，获得了工学学士学位。目前，我主要关注5G基带算法的研究与优化。待研究结束后，我将致力于软件的开发工作。 在雁栖湖生活的日子里，我找到了美好的爱情，她是一位善良💗、可爱、大气、正直的小仙女。我们在一起经历了许多许多，品尝着平凡生活的酸甜苦辣。她帮助我改掉生活的陋习、培养品味与审美，支持我，鼓励我，是我的良师益友。我会在博客中分享我们的故事。 我爱好电影、美食、游泳，也喜欢在观众面前分享我的见解。我喜欢整理，欣赏整整齐齐的东西，包括整洁的代码与文档、干爽的桌面与客厅，有些时候达到了强迫症的程度。我喜欢做系统性的研究，研究对象是不设限制的，乐于分享最终的结果，也期待他人的见解 :) ","date":"2021-03-30","objectID":"/about/:0:1","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"博客会有哪些内容博客中会分享我对计算机技术的整理与见解、我在职场中的成长与感悟。 还会有我对前沿论文的解读、对大型工程的源码剖析。 不定期有生活中的趣事与总结，例如上文中挖好的坑、年度总结。 ","date":"2021-03-30","objectID":"/about/:0:2","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"沟通交流最重要我十分期待你的反馈！:) 如果你不认同我博客中的内容，一定要联系我！让我们在沟通中共同成长进步！ 可以通过首页中展示的任何渠道联系到我☎️ ","date":"2021-03-30","objectID":"/about/:0:3","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"我的分享我会列出我最近的十次分享(可能也没有十次😅) 标题 活动 地点 时间 链接 轻松进大厂的简单方法 我把经验留下来 中科院计算所 2021.2.27 Slide ","date":"2021-03-30","objectID":"/about/:0:4","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"推荐的博客我会列出优秀的技术博客📝，这些博主都是我学习的榜样。他们的技术实力与工作履历十分精彩。 Cizixs ice1000 io-meter ","date":"2021-03-30","objectID":"/about/:0:5","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"简历我会不定期更新我的简历，大家可以关注我的领英: Qian Sun ","date":"2021-03-30","objectID":"/about/:0:6","tags":null,"title":"About me","uri":"/about/"},{"categories":["Java"],"content":"本文介绍了 Java 线程池中所有参数配置项与要求，针对不同业务场景提供对应的配置建议","date":"2021-03-30","objectID":"/java-concurrency-3/","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"配置线程池大小线程池的理想大小取决于被提交任务的类型以及所部署系统的特性。在实际工程中，通常不会固定线程池大小，应该通过某种配置机制来提供，或者根据 Runtime.availableProcessors 来动态计算。 在配置线程池大小时，应该避免“过大”和“过小”这两种极端情况。如果线程池过大，那么大量的线程将在相对很少的 CPU 和内存资源上发生竞争，不仅消耗更高的内存，而且还可能耗尽资源。如果线程池过小，将导致许多空闲处理器无法执行工作，从而降低吞吐率。 对于计算密集型的任务，在拥有 $N_{cpu}$ 个处理器系统上，当线程池的大小为 $N_{cpu}+1$ 时，通常能实现最优的利用率。 对于包含 I/O 操作或者其他阻塞操作的任务，由于线程并不会一直执行，因此线程池的规模应该更大。要正确设置线程池大小，你必须估算出任务的等待时间与计算时间的比值。这种估算不需要很精确，并且可以通过一些分析或监控工具来获得。 要使处理器达到期望的使用率，线程池的最优大小等于： $$N_{threads}=N_{cpu}\\times U_{cpu}\\times \\left( 1+\\frac{W}{C} \\right)$$ 式中，$N_{cpu}$ 代表 CPU 的数量，$U_{cpu}$ 代表 CPU 目标利用率，$U_{cpu}\\in [0,1]$，$\\frac{W}{C}$ 代表等待时间与计算时间的比值。 可以通过 Runtime 来获得 CPU 的数目： int N_CPUS = Runtime.getRuntime().availableProcessors(); CPU 周期并不是唯一影响线程池大小的资源，还包括内存、文件句柄、套接字句柄和数据库连接等。计算这些资源对线程池的约束条件是更容易的： 计算每个任务对该资源的需求量； 用该资源的可用总量除以每个任务的需求量，所得结果就是线程池大小上限。 ","date":"2021-03-30","objectID":"/java-concurrency-3/:1:0","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"配置 ThreadPoolExecutor public ThreadPoolExecutor (int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u003cRunnable\u003e workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { ... } ","date":"2021-03-30","objectID":"/java-concurrency-3/:2:0","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"线程的创建与销毁以下三个参数主要负责线程的创建与销毁： corePoolSize，线程池基本大小，即在没有任务执行时线程池的大小，并且只有在工作队列满了的情况下才会创建超出这个数量的线程。 maximumPoolSize，线程池最大大小，表示可同时活动的线程数量的上限。 keepAliveTime，线程池存活时间。如果某个线程的空闲时间超过了存活时间，那么将被标记为可回收的。 线程池的基本大小、最大大小和存储时间等因素共同负责线程的创建与销毁。当线程池的当前大小超过了基本大小时，被标记为可回收的线程将被终止。通过调节基本大小和存活时间，可以帮助线程池回收空闲线程占有的资源，从而使得这些资源可以用于执行其他工作。 ","date":"2021-03-30","objectID":"/java-concurrency-3/:2:1","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"管理队列任务ThreadPoolExecutor 允许提供一个 BlockingQueue 来保存等待执行的任务。基本的任务排队方法有 3 种：无界队列、有界队列和同步移交（Synchronous Handoff）。 ArrayBlockingQueue：一个基于数组结构的有界阻塞队列，按照 FIFO 原则对任务进行排序； LinkedBlockingQueue：一个基于链表结构的无界阻塞队列，按照 FIFO 原则排序任务，吞吐量通常要高于 ArrayBlockingQueue。 SynchronousQueue：一个不存储元素的线程间同步移交，要将一个元素放入 SynchronousQueue 中，必须有另一个线程正在等待接受这个元素。如果没有线程正在等待，并且线程池的当前大小小于最大值，那么 ThreadPoolExecutor 将创建一个新的线程，否则根据饱和策略，这个任务被拒绝。 PriorityBlockingQueue：一个具有优先级的有界阻塞队列，这个队列根据优先级来安排任务，任务的优先级是通过自然顺序或 Comparator 来定义的。 ","date":"2021-03-30","objectID":"/java-concurrency-3/:2:2","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"饱和策略当有界队列被填满后，饱和策略开始发挥作用。ThreadPoolExecutor 的饱和策略可以通过调用 setRejectedExecutionHandler 来修改。JDK 提供了几种不同的实现，每种实现包含不同的饱和策略： ThreadPoolExecutor.AbortPolicy：中止策略是默认的饱和策略，该策略将在线程池数量等于最大线程数时，抛出未检查的 RejectedExecutionException。涉及到的任务将不会执行。调用者可以捕获这个异常，然后根据需求编写自己的处理代码。 ThreadPoolExecutor.CallerRunsPolicy：既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量。它不会在线程池的某个线程中执行新提交的任务，而是在一个调用了 execute 的线程中执行该任务。 ThreadPoolExecutor.DiscardPolicy：该策略在线程池中数量等于最大线程数时，会悄悄丢弃不能执行的新增任务，不报任何异常； ThreadPoolExecutor.DiscardOldestPolicy：该策略在线程池中数量等于最大线程数时，会抛弃线程池中工作队列头部的任务（即等待时间最久的任务），并执行当前任务。 ","date":"2021-03-30","objectID":"/java-concurrency-3/:2:3","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"线程工厂每当线程池需要创建一个线程时，都是通过线程工厂方法来完成的。默认的线程工厂方法将创建一个新的、非守护的线程，并且不包含特殊的配置信息。通过指定一个线程工厂方法，可以定制线程池的配置信息。 定制线程池配置信息在某些场景下有需求，例如希望为线程池中的线程指定一个 UncaughtExecptionHandler，或者实例化一个定制的 Thread 类用于执行调试信息的记录。 下面的例子展示了自定义线程工厂为每个创建的线程池设置更有意义的名字，在 Debug 和定位问题时非常有帮助。 public class MyThreadFactory implements ThreadFactory { private final String poolName; public MyThreadFactory(String poolName) { this.poolName = poolName; } public Thread newThread(Runnable runnable) { return new MyAppThread(runnable, poolName); } } ","date":"2021-03-30","objectID":"/java-concurrency-3/:2:4","tags":["Java","并发"],"title":"配置 Java 线程池","uri":"/java-concurrency-3/"},{"categories":["Java"],"content":"Executor框架的执行策略对线程池性能造成的影响随着业务场景不同而变化，本文介绍了四类任务对执行策略、线程池性能的影响","date":"2021-03-19","objectID":"/java-concurrency-2/","tags":["Java","并发"],"title":"任务执行策略与线程池性能","uri":"/java-concurrency-2/"},{"categories":["Java"],"content":" ThreadPool Executor框架的执行策略可以将任务的提交与执行解耦开来，为任务的制定、执行提供了相当大的灵活性。但并非所有的任务都适用于Executor框架执行策略，有些任务需要明确的指定执行策略。 依赖性任务。依赖性任务注重任务之间的执行顺序。如果线程池执行依赖性任务，需要隐含为执行策略带来约束，避免产生活跃性问题。 使用线程封闭机制的任务。任务要求 Executor 是单线程的，如果将 Executor 从单线程环境改为线程池环境，将会失去线程安全性。 对响应时间敏感的任务。这类任务需要及时响应。如果将一个运行时间较长的任务提交到单线程的 Executor 中，或者将多个运行时间较长的任务提交到一个只包含少量线程的线程池中，那么将降低该 Executor 管理的服务的响应性。 使用ThreadLocal任务。ThreadLocal 使每个线程都拥有某个变量的一个私有“版本”。只要条件允许，Executor 可以自由地重用这些线程。只有当线程本地值的生命周期受限于任务生命周期时，在线程池的线程中使用 ThreadLocal 才有意义，而在线程池的线程中不应该使用 ThreadLocal 在任务之间传递值。 只有当任务都是同类型且相互独立时，线程池的性能才能达到最佳。 ","date":"2021-03-19","objectID":"/java-concurrency-2/:0:0","tags":["Java","并发"],"title":"任务执行策略与线程池性能","uri":"/java-concurrency-2/"},{"categories":["Java"],"content":"线程饥饿死锁依赖性任务可能造成线程池死锁。线程池中如果所有正在执行任务的线程都因等待其他仍处于工作队列中的任务而阻塞，就会引发线程饥饿死锁Thread Starvation Deadlock。 ","date":"2021-03-19","objectID":"/java-concurrency-2/:1:0","tags":["Java","并发"],"title":"任务执行策略与线程池性能","uri":"/java-concurrency-2/"},{"categories":["Java"],"content":"运行时间较长的任务如果任务阻塞的时间过长，线程池的响应性也会变得糟糕。此外，运行时间较长的任务也会增加短任务的服务时间。 有一项技术可以缓解执行时间较长任务造成的影响，即限定任务等待资源的时间。在平台类库的大多数可阻塞方法中，都同时定义了限时版本和无限时版本，例如 Thread.join、BlockingQueue.put、CountDownLatch.await 以及 Selector.select 等。如果等待超时，那么可以把任务标识为失败，然后中止任务或重新返回队列以便随后执行。 ","date":"2021-03-19","objectID":"/java-concurrency-2/:2:0","tags":["Java","并发"],"title":"任务执行策略与线程池性能","uri":"/java-concurrency-2/"},{"categories":["杂谈分享"],"content":"避免租房踩坑","date":"2021-03-15","objectID":"/house-renting/","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"毕业季总会为我们带来各种感受，既有步入新环境的兴奋，也有为生活琐事操心的烦忧。在生活琐事中，住房问题永远是大家最关注的点。大家希望在有所居的基础上，享受洁净、明亮、宽敞的居住空间。 本文是我将租房经验加以整理、抽象得出的租房注意事项，欢迎大家补充。如果您的补充被本文采纳，我会在下方列出您的名字表示您对本文的贡献👇 注意 感谢以下同学为本文的贡献： Qian Sun 张兵 Changhao Liu Rain ","date":"2021-03-15","objectID":"/house-renting/:0:0","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"房源信息总体来说，房源信息来源分为两类：房东直租与中介介绍。 ","date":"2021-03-15","objectID":"/house-renting/:1:0","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"房东直租房东可以将自己的出租信息挂在网上，例如高校论坛、豆瓣、闲鱼等，租客也可以直接去小区、小区物业去打听此类信息。房东直租可以省去中介费的负担，在租金上可能也有优惠。不过需要大家擦亮眼睛，明辨出租房是否为业主所有，检查房中的水电、家居情况，与房东协商一系列使用、维修注意事项。 网上的直租信息存在中介钓鱼的可能性，需自行判断对方身份。 ","date":"2021-03-15","objectID":"/house-renting/:1:1","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"中介介绍房东直接委托中介办理房屋出租，租客可以将自己的需求明确告诉中介，中介为你推荐合适的房子。这类方式比较省事，中介可以帮你过滤不合适的信息，替你与业主协商使用、维修注意事项，如有需要也会帮你谈价格。租客承担中介费一般为一个月租金，续签时候的中介费问题需要看各个中介公司的情况。这种方式可以快速匹配租客与房源，省心省力。 上海租房可以尝试“六六直租”APP，但需要筛选房源。 ","date":"2021-03-15","objectID":"/house-renting/:1:2","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"个人建议如果你没有租房经验，建议找一家大型中介为你找房子，中介费权当你进入租房、购房市场的学费了。原因如下： 中介手中掌握大量房源信息，可以根据你的需求帮你快速定位。毕业季租房市场火爆，快速找到适合自己的房子是关键的第一步。 中介可以为你争取到市场上绝大多数租客可以享受到的服务，例如维修责任方、必要的家具。 双方交付房子时，中介会帮助你检查房中一切情况并登记在案，退房退押金时可作为依据。 有正规的租赁合同，省去找人看合同的麻烦。 如果有租房经验，可以直接去小区或者网上寻找房东直租信息，省去中介费岂不美滋滋。 ","date":"2021-03-15","objectID":"/house-renting/:1:3","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"看房关注的点","date":"2021-03-15","objectID":"/house-renting/:2:0","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"风险信息 核实出租人身份，查看身份证与房产证信息页是否匹配 检查是否有权利瑕疵，例如抵押、查封、拍卖等等 检查房屋是否非法改造，主体结构是否安全，是否动了承重墙，具体方法为看房产证 检查房屋是否存在空气质量问题 约定好违约行为，一般是市场默认的违约行为 异地租房一定要现场确认房屋情况，不要轻易确定 定金最好不要交，直接签合同交租金。如果需要交定金，一定不要轻易交 ","date":"2021-03-15","objectID":"/house-renting/:2:1","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"房屋质量 房屋的布局是否合理，是否符合你的要求 房屋采光如何，能否保证你希望的足够采光时长、强度。更详细请看建筑采光设计标准 房屋是否漏水，楼上是否漏水 房屋取暖方式，优先集体供暖 房屋燃气管、燃气阀、燃表情况，是否需要更换 房屋窗户是否漏风 家具是否齐全，家具功能是否正常 房屋电路是否正常，虽然插座出问题概率小，但必要时仍需检查 长期不住人的房屋更要小心仔细检查 房屋的隔音条件，周边环境是否有噪音 ","date":"2021-03-15","objectID":"/house-renting/:2:2","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"安全问题 小区、住宅楼的安全管理情况如何 ","date":"2021-03-15","objectID":"/house-renting/:2:3","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"费用问题 付款频率，一般来说是押一付三 退房返押金的标准，需要交付时候与房东确认好房屋细节 维修费用的承担，租客原因租客承担，非租客原因需要商量好 物业费、车位费：物业费房东承担，车位费没了解过 水、电、燃气、宽带、取暖费用租客承担 约定好违约金 ","date":"2021-03-15","objectID":"/house-renting/:2:4","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"其他有用的点 把自己收拾的干净一些，让房东相信你可以照顾好他的房子 明确和房东、中介讲清自己的需求，一开始约定好比后面追加条件要容易多 房东喜欢稳定、可长租的租客，可以表现你的工作、收入稳定 情侣比朋友合租更有优势 房东希望尽快入住，避免房屋空闲，签约速度也是房东选择租客的要素之一 ","date":"2021-03-15","objectID":"/house-renting/:3:0","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["杂谈分享"],"content":"总结要是没有经验，推荐找中介。有后续风险问题都可以找中介解决，第三信任方是交易顺利完成的保障。自己在看房子的时候需要仔细、仔细、仔细检查风险、房屋质量、安全与费用等问题。谈好的条件一定要求落实在合同中。 最后祝大家找到自己满意的房子。如果你有其他问题，可以从首页中任意联系方式找到我。 如果你有其他建议，欢迎分享给我，方式如上👆 我会添加在文中，并留下你的姓名表示感谢。 ","date":"2021-03-15","objectID":"/house-renting/:4:0","tags":["租房","注意事项"],"title":"应届生租房注意事项（持续更新）","uri":"/house-renting/"},{"categories":["Java"],"content":"本文介绍了Executor框架，框架采用了任务提交、执行的解耦方案。为了让该方案适配不同场景，需要将多种因素考虑进执行策略中。不同的执行策略也衍生出不同的线程池，我们在使用前需要分析真实环境去选择适当的线程池。线程池异步执行多个任务，导致任务可能处于不同的状态。为了管理整个线程池的生命周期，ExecutorService提供了多种方法，一般采取awaitTermination、shutdown组合使用的方式，达到同步关闭的效果。最后，本文介绍了ScheduledThreadPool在延迟任务、周期任务的优越性，如果构建调度服务，可以采用DelayQueue。","date":"2021-03-12","objectID":"/java-concurrency-1/","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"Executor框架我们将执行的逻辑工作单元抽象为任务，那么线程就是使任务异步执行的容器。如果把所有任务放在单个线程执行，将对应用的响应性和吞吐量造成灾难性影响。 为每个任务分配一个线程似乎是不错的解决方案，不过这对资源管理有着较高的要求。线程池缓解了这一压力，它承担了管理线程工作。java.util.concurrent 提供了一种灵活的线程池作为Executor框架的一部分。 在 Java 类库中，任务执行的主要抽象不是 Thread，而是 Executor。 public interface Executor { void execute (Runnable command); } Executor 为灵活且强大的异步任务执行框架提供了基础，为任务的提交与执行提供了标准的方法，将两个过程解耦开来。此外，Executor 实现了对生命周期的支持，以及监控管理等机制。 Executor 基于生产者-消费者模式，提交任务的操作相当于生产者，执行任务的操作类似于消费者。反之，如果想实现简单的生产消费模型，可以采用 Executor 实现。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:1:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"执行策略Executor 框架需要设计一套任务执行策略，以解决多任务执行混乱的问题。执行策略包括以下内容： 在什么线程中执行任务？ 任务按照什么顺序执行（FIFO、LIFO、优先级）？ 有多少个任务能并发执行？ 在队列中有多少个任务在等待执行？ 如果系统需要拒绝一个任务，应该选择哪个任务？如何通知应用有任务被拒绝？ 在执行一个任务前后，应该进行哪些动作？ ","date":"2021-03-12","objectID":"/java-concurrency-1/:2:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"线程池线程池是管理诸多线程的资源池，依靠任务保持所有等待执行的任务。工作者线程Work Thread的任务很简单：从工作队列获取一个任务，执行任务，然后返回线程池并等待下一个任务。 为每个任务分配一个线程可能引入线程新建、销毁的开销，不如让多个任务在线程池中执行。通过重用现有的线程可以分摊多个线程的开销。此外，任务不会因等待线程创建而延迟执行，提升整体响应性。用户可以通过配置线程池大小，创建足够多的线程使处理器保持忙碌状态，还可防止多线程相互竞争资源而使应用程序耗尽内存或失败。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:3:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"newFixedThreadPoolnewFixedThreadPool 是拥有固定数量线程的线程池，每当提交一个任务就创建一个线程，直到达到最大数量。如果某个线程因异常结束，那么线程池会补充一个新线程。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:3:1","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"newCachedThreadPoolnewCachedThreadPool 是可缓存的线程池，如果线程池当前规模超过处理需求，将回收空闲线程，而当需求增加时，可以添加新线程。线程池规模不存在任何限制。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:3:2","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"newSingleThreadExecutornewSingleThreadExecutor 是单线程的 Executor，如果线程因异常结束，newSingleThreadExecutor 会创建另一个线程来替代。任务按照队列顺序而执行。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:3:3","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"newScheduledThreadPoolnewScheduledThreadPool 的线程数量也是固定的，但可以以延迟或定时方式执行任务。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:3:4","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"Executor 的生命周期Executor 通常会创建线程来执行任务，只有当所有线程全部终止后才会退出，如果无法正确关闭 Executor，JVM 将无法结束。 因为 Executor 以异步方式执行任务，可能引发任务状态不同步的问题。同一时刻，不同任务可能处于执行、完成、等待执行三个状态。为了解决执行任务的生命周期问题，Executor 扩展了 ExecutorService 接口，添加了一些用于生命周期管理的方法。 public interface ExecutorService extends Executor { void shutdown(); List\u003cRunnable\u003e shutdownNow(); boolean isShutdown(); boolean isTerminated(); boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // ... } ExecutorService 的生命周期有三种状态：运行、关闭和已终止。shutdown 方法将平缓地关闭 ExecutorService：不再接受新任务，同时等待已经提交的任务执行完成——包括还未开始执行的任务。shutdownNow 方法将粗暴地关闭 ExecutorService：直接取消所有执行中的任务，并且不再启动队列中尚未开始执行的任务。 awaitTermination 方式可以等待 ExecutorService 到达终止状态，或者调用 isTerminated 来轮询 ExecutorService 是否已经终止。通常在调用 awaitTermination 方法后立即调用 shutdown，从而产生同步关闭的效果。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:4:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"延迟任务与周期任务Timer 类负责管理延迟任务以及周期任务。但 Timer 类支持的是基于绝对时间的调度机制，对系统时钟变化的容忍度很低，存在天然缺陷。ScheduledThreadPool 只支持基于相对时间的调度，可以通过 ScheduledThreadPool 的构造函数或 newScheduledThreadPool 工厂方法来创建该类的对象。 如果构建调度服务，可以使用 DelayQueue，它实现了 BlockingQueue，并为 ScheduledThreadPoolExecutor 提供调度功能。DelayQueue 管理着一组 Delayed 对象。每个 Delayed 对象都有一个相应的延迟时间。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:5:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["Java"],"content":"总结本文一开始介绍了 Executor 框架，框架采用了任务提交、执行的解耦方案。为了让该方案适配不同场景，需要将多种因素考虑进执行策略中。不同的执行策略也衍生出不同的线程池，我们在使用前需要分析真实环境去选择适当的线程池。线程池异步执行多个任务，导致任务可能处于不同的状态。为了管理整个线程池的生命周期，ExecutorService 提供了多种方法，一般采取 awaitTermination、shutdown 组合使用的方式，达到同步关闭的效果。最后，本文介绍了 ScheduledThreadPool 在延迟任务、周期任务的优越性，如果构建调度服务，可以采用 DelayQueue。 ","date":"2021-03-12","objectID":"/java-concurrency-1/:6:0","tags":["Java","并发"],"title":"Executor与线程池","uri":"/java-concurrency-1/"},{"categories":["读书笔记"],"content":"思维混乱常常导致我们决策失误，戴上六顶思考帽，专注思考方向，全面、快速、清晰地思考","date":"2021-02-14","objectID":"/six-thinking-hats/","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["读书笔记"],"content":" 封面 ","date":"2021-02-14","objectID":"/six-thinking-hats/:0:0","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["读书笔记"],"content":"关于本书 关于本书 ","date":"2021-02-14","objectID":"/six-thinking-hats/:1:0","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["读书笔记"],"content":"内容简介本书介绍了一种思考模式，可以应对探索性问题或者单类型问题。 六顶思考帽代表一种思维方式。每次思考时专注于一顶帽子，避免出现思维混乱的情况。 通过训练、使用，可以全面、客观、快速认清问题，并提出解决方案。 ","date":"2021-02-14","objectID":"/six-thinking-hats/:2:0","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["读书笔记"],"content":"书摘 思维导图总结 ","date":"2021-02-14","objectID":"/six-thinking-hats/:3:0","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["读书笔记"],"content":"读后感 思考是人类最大的宝藏，但非常容易出现思考混乱，情感、信息、逻辑、希望和创意搅在一起。 可以不必关注“是什么”，关注“能够怎样”，辩证法告诉我们角度不同、结论不同。 现代新事物涌现太快，需要更快速、全面地认清新事务，这套方法比较适用。 经验告诉我，使用这套方法应该是润物细无声的方式，生拉硬拽会适得其反。 ","date":"2021-02-14","objectID":"/six-thinking-hats/:4:0","tags":["读书笔记","思考方式","水平思考"],"title":"《六顶思考帽》读书笔记","uri":"/six-thinking-hats/"},{"categories":["论文学习"],"content":"自治DBMS可以降低DBA工作负担，为企业带来数据驱动决策的便利。该文提出Peloton DBMS自治架构，并认为在深度神经网络、新硬件和高性能数据库架构下，自治DBMS是可以实现的","date":"2021-01-22","objectID":"/self-driving-dbms/","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"注意 自治 DBMS 可以降低 DBA 工作负担，为企业带来数据驱动决策的便利。该文提出 Peloton DBMS 自治架构，并认为在深度神经网络、新硬件和高性能数据库架构下，自治 DBMS 是可以实现的。 原文在这里 👉 Self-Driving DBMS ","date":"2021-01-22","objectID":"/self-driving-dbms/:0:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"摘要过去，研究员和供应商都搭建了查询工具去帮助 DBA 实现系统调优、物理设计。然而，之前大部分工作是不完整的，因为 DBA 仍无法摆脱数据库更改的裁定工作，并且在问题发生后仍需要采取应对措施。 真正的“自治”数据库管理系统Database Manage System所需要的是一种为自治而设计的新架构。这与以前的工作不同，因为系统所有方面都受集成规划组件控制，该组件不仅优化系统以适应当前负载，也预测未来的负载趋势，以便系统能够相应地进行准备。这样，DBMS 就可以支持所有以前的调优技术，而不需要人力确定正确的方式和适当的时间去部署它们。它还支持一些对现代高性能 DBMS 很重要的优化，这点在今天是很难的，因为管理这些系统的复杂性已经突破专家的能力上限。 本文介绍了第一代 Self-Driving DBMS——Peloton 的架构。由于深度学习算法的进步以及硬件、自适应数据库架构的改进，Peloton 的自主能力现在有了可能性。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:1:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"介绍从 1970 年以来，关系模型和声明式查询语言就以消除数据管理负担为卖点。40年后，DBMS 变得更加复杂，功能越来越多。使用现有的自动调优工具是一项繁重的任务，因为它们需要费力准备工作负载样本、空闲的硬件来测试更新，最重要的原因是需要直观了解 DBMS 内部结构。如果 DBMS 可以自动完成这些事，那么它将消除部署数据库所涉及的许多复杂性和成本。 以前关于调优系统的工作关注点在针对数据库单个方面的独立工具上。例如，一些工具能够选择数据库的最佳逻辑或物理设计，如索引、分区方案、数据组织或物化视图。其他工具可以为应用选择调优参数。这些工具中的大多数都以相同的方式操作：DBA 为其提供样本数据库和工作负载跟踪，以指导搜索过程去找到最佳或接近最佳的配置。主要的 DBMS 供应商（包括 Oracle、Microsoft 和 IBM）都以这种方式操作。最近有一种推动集成组件支持自适应架构的趋势，但这同样只专注于解决一个问题。同样，云厂商使用动态资源分配服务，不会对单个数据库进行调优。 所有这些对一个完全自治的系统来说都是不够的，因为它们在 DBMS 之外，不能同时解决多个问题。也就是说，它们从系统外部观察 DBMS 的行为，并在问题发生后建议 DBA 如何修正问题。调优工具假定操作它们的人有足够的知识，可以在特定时间内更新 DBMS，不对应用产生大影响。然而，数据库领域在过去十年中发生了巨大的变化，我们不能假定 DBMS 是由一个了解数据库优化的专家部署而成。再说，即使这些这些工具可以实现自治， DBMS 架构也会在重大更新时给 DBMS 施加很大压力，也无法突破未来的瓶颈。 本文中，作者证明了自治数据库系统是可以实现的。下文首先会讨论该系统所面临的主要挑战。然后，作者提出了 Peloton 的架构，以及使用 Peloton 中集成深度学习框架的测量结果。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:2:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"问题概述自治 DBMS 面临的第一个挑战是理解应用的负载。最基本的级别是将查询定义为 OLTP 或 OLAP 应用。如果 DBMS 确定了应用属于二者中的哪一类，那么它就可以决定如何优化数据库。例如，如果是 OLTP，DBMS 应该将元组存储在面向行的布局中，并为写进行优化。如果是 OLAP，那么 DBMS 使用面向列的布局，这样更适应访问表列子集的只读查询 。处理这个问题的一种方法是部署专门用于 OLTP 和 OLAP 负载的独立 DBMS，然后在它们之间定期流更新。但是这不适合 HTAP，因为它在数据由 OLTP 写入时就执行 OLAP 查询，所以无法将 DBMS 独立部署开。更好的方法是部署一个支持混合 HTAP 负载的 DBMS 。这种系统会自动为不同数据库段选择适当的 OLTP 或 OLAP 优化。 除了要理解应用的负载，DMS 也需要预测资源利用趋势。这帮助它能够预测未来需求和部署优化，同时对性能影响最小。许多程序的使用模式密切契合人类的日常生活。这也是为什么 DBA 会在非高峰时间安排更新，以避免正常业务时间的服务中断。不可否认，有些工作负载异常是 DBMS 无法预料的。但这些模型可以预先警告，使得 DBMS 能够比外部监控系统更快地实施缓解措施。 现在， DBMS 可以依靠这些模型去确定数据库调优与优化操作，以更好应对预期的工作负载。自治 DBMS 不支持 DBA 的任务，这些任务需要系统外部信息，比如权限、数据清洗和版本控制。如下表所示，自治 DBMS 可以支持三种优化类型。第一个是数据库物理设计，第二个是数据组织的修改，最后是影响 DBMS 的运行时行为。对于每一个优化操作，DBMS 将需要评估它们对数据库的潜在影响。这些评估不仅包括行动部署后消耗的资源，还包括 DBMS 部署行动时消耗的资源。 Types Actions PHYSICAL Indexes AddIndex, DropIndex, Rebuild, Convert PHYSICAL Materialized Views AddMatView, DropMatView PHYSICAL Storage Layout Row👉Columnar, Columnar👉Row, Compress DATA Location MoveUpTier, MoveDownTier, Migrate DATA Partitioning RepartitionTable, ReplicateTable RUNTIME Resources AddNode, RemoveNode RUNTIME Configuration Tuning IncrementKnob, DecrementKnob, SetKnob RUNTIME Query Optimizations CostModelTune, Compilation, Prefetch 即使系统能够预测程序的工作负载，选择要使用的操作 ，并确定实施操作的最佳时间，仍然存在额外的挑战。如果 DBMS 不能有效地应用这些优化，没有带来较大的性能下降，那么系统将无法快速适应变化。这也是目前自治 DBMS 不可能实现的另一个原因。如果系统只能每周更新一次，那么它很难规划如何纠正系统。因此，论文认为需要一个灵活的基于内存的 DBMS 体系结构，可以在部署过程中逐步优化，而不会对应用程序产生可察觉的影响。 最后，一个自治 DBMS 有两个额外的约束，它必须关联如今的应用程序。首先，DBMS 不能要求开发人员重写应用代码以适应自治 DBMS。第二，不能依赖只支持某些编程环境的程序分析工具。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:3:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"自治架构研发发现现有 DBMS 对于自治操作过于笨重 ，因为它们需要在更改时重新启动，而且上表中的许多操作太慢了。因此，DBMS 需要一个崭新的架构，对集成的自治组件有更全面、更细致的控制 。 接下来描述 Peloton 架构中的组件。Peloton 架构是一种全新的架构，而不是改造现有的 DBMS（例如，Postgres/MySQL）。最重要的是，它使用了多版本并发控制，在不阻塞 OLAP 查询的前提下交叉 OLTP 事务和操作。另一个特点是，它使用了一个内存存储管理器，具有无锁的数据结构和灵活的布局，可以快速执行 HTAP 工作负载。这些设计已经使我们能够支持 Peloton 的优化操作。 Peloton架构 Peloton 的自治流程如上图所示。除了环境设置（比如内存阈值与目录路径），论文的目标是让Peloton 在没有任何人为提供的指导信息的情况下高效运行。系统自动学习如何提高应用程序查询和事物的延迟。延迟是 DBMS 中最重要的度量标准，因为它全面代表性能情况。本文的其余部分假设延迟是主要优化目标。可以为分布式环境中的其他重要指标添加额外的约束，例如服务成本和资源。 Peloton 包含一个嵌入式监视器，跟踪系统的内部事件流的执行查询。每个查询条目都标注了其资源利用率。流还定期被 DBMS/OS 遥测数据和优化操作的开始/结束事件打断。然后，DBMS 根据这些监控数据为应用程序的预期工作负载构建预测模型。它使用这些模型来识别瓶颈和其他问题（例如，缺少索引、超载节点），然后选择最佳操作。系统执行此操作的同时仍然处理应用程序的常规工作负载，并收集新的监控数据，以了解这些操作如何影响其性能。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:4:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"工作负载分类第一个组件是 DBMS 的集群器，它使用无监督学习方法对具有类似特征的应用程序查询进行分类。集群工作负载减少了 DBMS 维护的预测模型数量，从而使预测应用程序的行为更容易（也更准确）。Peloton 的初始实现使用 DBSCAN 算法。这种方法已被用于对静态 OLTP 工作负载进行聚集操作。 这种集群的一大问题是使用什么查询特征。这两种类型的特性是查询的时间度量和查询的逻辑语义。虽然前者使 DBMS 能够更好地聚集类似的查询，且不需要理解它们的含义，但它们对数据库内容或其物理设计设计的变化更敏感。即使数据库没有更改，在高并发工作负载中也可能发生类似的问题。另一个方法是根据逻辑执行计划的结构（例如表、谓词）对查询进行分类，这些特征独立于数据库的内容及其物理设计。这些特征是否会产生集群以生成良好的预测模型还有待观察，或者运行时指标的准确性是否超过了再训练的成本。运行时度量可能会使系统在更短的时间内收敛到稳定的状态，因此系统不必经常重新训练它的预测模型。或者，即使集群经常变化，硬件加速训练也能使系统以最小 的开销加速重建模型。 下一个问题是如何确定集群何时失效。当这种情况发生时，DBMS 必须重新构建集群，这可能打乱已有的分类，并要求它重新训练所有的预测模型。Peloton 使用标准交叉验证技术来确定集群的错误率何时超过阈值。DBMS 还可以利用操作对查询的影响来决定何时重建集群。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:4:1","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"负载预测下一步是训练预测模型，预测每个工作负载集群的查询抵达率。除了异常热点外，这种预测使系统能够识别工作负载周期性和数据增长趋势，为负载波动做好准备。在 DBMS 执行一个查询后，它用它的集群标识符标记每个查询，然后填充一个直方图📊，该直方图跟踪在一段时间内每个集群收到的查询数量。Peloton 使用这些数据来训练预测模型，预测应用程序未来将执行的每个集群的查询数。DBMS 还为事件流中的其他 DBMS/OS 指标构建了类似的模型。 以前在自治系统方面的尝试使用了 auto-regressive-moving average 模型（ARMA）来预测在云中自动伸缩的 web 服务工作负载。ARMA 可以捕获时间序列数据中的线性关系，但它们通常需要一个人来识别模型中的差分顺序和 term 的数量。此外，线性假设对于许多数据库工作负载可能并不有效，因为它们受到外部因素的影响。 RNN 是预测非线形系统时间序列模式的一种有效方法。LSTM 是 RNN 的一种变体，允许网络学习时间序列数据中的周期性和重复趋势，而常规的 RNN 无法做到这一点。LSTM 包含一些特殊的 block，用于决定是否保留旧的信息以及何时将其输出到网络中。尽管 RNN 被吹捧为能够解决许多以前难以解决的问题，但仍需要研究如何使其适用于自治 DBMS。 RNN 的准确性也依赖于它的训练集数据的大小。但是跟踪在 DBMS 中执行的每个查询都会增加模型构建的计算成本。幸运的是，我们没有必要知道将来查询的确切数量。相反，Peloton 为每个组维护多个 RNN，它们在不同的时间范围和间隔粒度上预测工作负载。尽管这些粗粒度的 RNN 不准确，但它们减少了 DBMS 必须维护的训练数据和运行时的预测成本。组合多个 RNN 使 DBMS 能够处理对精确度要求更高的即时问题，也能够适应评估范围更广的长期计划。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:4:2","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"行动计划与执行最后一个组件是 Peloton 的控制框架，能够持续监控系统和选择优化行动，以提高应用程序的性能。这就是自治组件和 DBMS 架构紧密耦合的最明显好处，因为它使不同的部分能够相互提供反馈。作者也相信有机会在系统更多部分中使用强化学习，包括并发控制和查询优化。 行动生成：系统搜索可能提高性能的操作。Peloton 将这些操作存储在一个目录中，并记录在操作调用的历史。这种搜索是由预测模型指导的。它还可以删除冗余操作，降低搜索复杂度。 在需求竞争低时，Peloton 可以用更多的核去完成动作部署，在竞争高时，只能用更少的核。 行动计划：现在有了行动目录，DBMS 根据预测、当前数据库配置和目标函数去选择部署哪个行动。滚动时域控制模型receding-horizon control model，RHCM的基本思想：在每个时间单元，系统使用预测来估计某个有限时域的工作负载。然后，它会搜索动作，以最小化目标函数。但它只能应用于第一个动作，在下个时间单元重复流程前只好等待部署完成。这就是高性能 DBMS 至关重要的原因。如果动作在几分钟内完成，则系统不必监视工作负载是否已经转移，并决定中止正在执行的动作。 在 RHCM 下，计算过程被建模为一棵树，其中每一层包含 DBMS 可以调用每个操作的时间。该系统通过估算行动的成本效益来搜索树，并选择最低成本的行动序列。也可以选择在一个时间单元内不执行任何操作。一种降低这个过程复杂性的方法是，在搜索树的更深层次随机选择要考虑的行动，而不是评估所有可能的行动。这种抽样会加上权重，以便更有可能考虑为数据库当前状态及其预期工作负载提供最优行动。它还避免了最近调用的行动，但后来系统取消这条规则。 一个行动的成本是对部署它所需的时间估计，以及在这段时间内 DBMS 性能会下降多少。由于以前没有部署过很多行动，因此不可能总是从以前的历史记录生成这些信息。系统使用分析模型来评估每个操作类型的成本，然后通过反馈机制自动改进它们。这样的好处是在部署行动后查询的延迟时间发生了变化。这个好处来自 DBMS 的内部查询计划器成本模型。它是加权计算的行动部署后查询样本延迟改进的总和，权重是预测模型预测的期望查询完成率。 除了上述的成本-收益分析之外，系统还必须评估一个行动如何随时间推移影响 Peloton 的内存使用。任何导致 DBMS 超过内存阈值的行动都需要舍弃。 重要的是，系统在考虑行动时应该考虑多长时间跨度。太短会阻止 DBMS 及时为即将到来的负载峰值做好准备，但太长会使 DBMS 无法缓解突然出现的问题，因为模型太慢。此外，由于计算每个时间纪元的成本-收益是昂贵的，它可能创建另一个深度学习网络来近似它们值函数。 部署：Peloton 支持非阻塞方法部署行动。例如，重新组织表的布局或将其移动到不同的位置并不会阻止查询该表。有些操作，如添加索引，需要特别考虑。这样 DBMS 就不会因为在操作进行时数据被修改，导致任何错误发生。 DBMS 还从其集成的机器学习组件处理资源调度和竞争问题。使用单独的协处理器或 GPU 来处理繁重的计算任务，可以避免降低 DBMS 的速度。否则，DBMS 将不得不使用一台单独的机器，专门用于所有的预测和计算组件。这将使系统的设计复杂化，并由于协调而增加额外的开销。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:4:3","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"其他注意事项要让自治 DBMS 得到广泛应用，还需要克服一些非技术挑战。最重要的是 DBA 不愿意将数据库控制权交给自动化系统。为了简化过渡，自治 DBMS 可以以可视化方式公开其决策过程。例如，如果它选择添加一个索引，可以向 DBA 解释，它的模型表明当前的工作负载与过去某个时间点相似，这个时间点使用这样的索引是有好处的。 还必须支持来自 DBA 的提示，说明系统是否应该更多地关注工作负载的 OLTP 或 OLAP 部分。类似的，对于多租户部署，系统需要知道是否应该对每个数据库进行相同的调优，或者一个数据库是否比其他数据库更重要。 最后，可能需要为 DBA 提供一个覆盖机制。人类发起的改变被当作其他一样的行动，Peloton 记录操作历史来决定该行动是否有益。唯一的区别是系统不允许逆转。为了防止 DBA 做出的错误决策被永久保留，DBMS 可以要求 DBA 手动设置生命周期。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:4:4","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"初步结果作者在 Peloton 中集成了 Tensorflow，使用从一个在线讨论网站一个月的流量数据中提取 5200 万个查询来训练两个 RNN。使用 75% 的数据去训练模型，25% 数据去验证模型。在输入上应用了两个堆叠的 LSTM 层，然后连接到一个线形回归层，对这些层使用 10% 的 Drop 以避免过拟合。 第一个模型以分钟为粒度去预测下一小时将到达的查询数量。该模型收入是一个向量，表示过去两小时内每分钟的工作负载，而输出是一个标量，表示一小时后预测的工作负载。第二个模型使用 24 小时时域，粒度为 1 小时。 RNN与真实负载数据的比较结果 两个RNN的训练时间分别为 11 和 18 分钟，从下图可看出，模型能够以 11.3% 的错误率预测 1 小时后的工作负载，以 13.2% 的错误率预测一天后的。对于计算开销，前者大约是 2MB，DBMS 探测每个模型以获得新预测需要 2ms，向其添加一个新的数据点需要 5ms。 使用这些模型，可以在 Peloton 中实现数据优化操作。根据访问这些表的查询类型，将表迁移到不同布局中。每个表的“热”元组存储在行布局中，已经针对 OLTP 优化，而同一张表中“冷”元组存储在列布局中，已经针对 OLAP 优化。使用 HTAP 工作负载，白天执行 OLTP 操作，晚上执行 OLAP 查询。当启动自动布局时，在 Peloton 中执行查询序列，并将其与静态的行和列布局进行比较。 自动交叉布局与静态布局在HTAP应用中的性能比较 从上图可看出，Peloton 随着时间推移收敛到一种适合工作负载的布局。在第一段后，DBMS 将行元组迁移到列布局，适应 OLAP。接下来，当工作负载转移到 OLTP 查询时，自治 DBMS 比静态系统更好，因为它执行的内存写操作更少。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:5:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"总结随着大数据运动兴起，对自治 DBMS 的需求越来越大。这类系统将降低人力成本在部署任何规模数据库上的浪费，并使各组织更容易享受数据驱动决策带来的便利。论文概述了 Peloton DBMS 的自治架构。作者认为，由于深度神经网络、新硬件和高性能数据库架构，自治 DBMS 系统是可以实现的。 ","date":"2021-01-22","objectID":"/self-driving-dbms/:6:0","tags":["论文","DBMS","数据库管理"],"title":"自治数据库管理系统","uri":"/self-driving-dbms/"},{"categories":["论文学习"],"content":"Lakehouse架构支持开放的数据格式、机器学习，提供卓越的性能，是第三代数据分析平台的代表","date":"2021-01-06","objectID":"/lakehouse/","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"注意 Lakehouse架构逐渐在工业界铺开，第三代数据分析平台进入大众视野！ 原文在这里 👉 Lakehouse ","date":"2021-01-06","objectID":"/lakehouse/:0:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"摘要论文认为数据仓库架构在未来一段时间内会逐渐消亡，取而代之的是一种新型 Lakehouse架构，该架构具有如下特特性： 基于开放的数据格式，例如 Apache Parquet 完全支持机器学习和数据科学 提供卓越的性能 Lakehouse 解决数据仓库面临的主要挑战——数据陈旧、可靠性不高、总成本大、数据格式受限、场景支持受限。论文下面会讨论 Lakehouse 架构为何取得工业界青睐以及如何影响数据管理。 ","date":"2021-01-06","objectID":"/lakehouse/:1:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"数据分析平台发展数据仓库将业务数据库的数据收集到集中式仓库，帮助企业领导分析数据，之后被用于决策支持和商业智能Business Intelligence。数据仓库使用写模式schema-on-write写入数据，优化下游BI消费的数据模型。这就是第一代数据分析平台。 第一代数据分析平台 后来第一代平台开始面临诸多挑战。首先是计算与存储耦合使得扩容成本增加，这迫使企业支付用户负载和数据管理峰值的成本，这个成本随着数据规模增加而迅速增加。其次，越来越多的数据集是非结构化的，例如视频、音频和文本文档，而数据仓库无法存储、查询这类数据。 为了解决这些问题，第二代数据分析平台将所有原始数据导入数据湖：具有文件 API 的低成本存储系统，该系统可存储开放数据格式，例如 Apache Parquet 和 ORC。这个方法源起于 Apache Hadoop，基于 HDFS 实现低成本存储。数据湖是一种读模式schema-on-read架构，可以灵活、低成本地存储数据，也解决了数据质量和下游管理的问题。该架构中的一小部分数据在进行 ETL 后注入下游数据仓库，再进行决策支持和 BI 分析。开放数据格式使得绝大多数分析引擎可以直接访问数据湖数据。 第二代数据分析平台（数据湖+数据仓库） 2015年起，云数据湖（S3、ADLS、GCS、OSS等）开始取代HDFS，它们具有超强的持久性、冗余可靠、超低存储成本。云上架构与第二代平台架构相同，例如Redshift、Snowflake。数据湖 + 数据仓库 两层架构当今在工业界中占主导地位。 如今，这种架构面临新的挑战。尽管存储和计算的分离使得云数据湖 + 数据仓库架构的成本降低，可增加了用户的使用成本。在第一代平台中，所有业务数据库中的数据经过ETL后直接注入数据仓库。第二代平台却在中间引进了数据湖，增加了额外的复杂性、延迟与故障率。同时，数据湖 + 数据仓库二层架构不能很好支持机器学习之类的高级分析。具体来看，可以归纳为四个问题： 可靠性。保持数据湖与数据仓库的一致性是成本高昂且困难的事情。需要对两个系统之间的 ETL 作业进行仔细设计，如此方可进行高性能决策支持与 BI 分析。每个ETL步骤还有发生故障或引入错误的风险，例如由于数据湖和数据仓库引擎之间的细微差别而导致数据质量降低的风险。 数据陈旧。数据仓库数据的时效性低于数据湖数据，新数据的加载通常要花费几天。与第一代分析系统相比，这是个倒退，第一代分析系统可以直接查询新的业务数据。根据 Dimensional Research 与 Fivetran 调查，86% 的分析使用过时数据，62% 的报告每月需要等待几次引擎资源。 对高级分析支持有限。企业希望使用数据进行预测，例如“我应该为哪些顾客提供折扣？”。 尽管许多研究关注机器学习与数据管理结合，但主流机器学习系统没有一个可以工作在数据仓库上，包括 TensorFlow、PyTorch 和 XGBoost。与 BI 查询少量数据不同，这些机器学习系统需要使用复杂的 No-SQL 代码处理大型数据集，但通过 ODBC/JDBC 读取数据效率很低，并且无法直接访问数据仓库内部专有格式的数据。对于这类场景，数据仓库供应商建议导出数据为文件，但这增加了复杂性和滞后性，因为添加了第三个 ETL。或者，用户可以在支持开发格式的数据湖上运行这些系统，这会抛弃数据仓库丰富的数据管理功能，例如 ACID 事务、数据版本控制与索引。 总成本。除了支付 ETL 作业费用外，用户还得为复制到数据仓库的数据支付两倍的存储成本，而数据仓库使用的内部格式额外引入数据或工作负载迁移到其他系统的成本。 一种被广泛采用的解决方案是不使用数据湖 ，将所有数据存储在计算、存储分离的数据仓库中。论文认为这种方案可行性有限，因为不支持视频、音频和文本数据或从机器学习和数据科学工作负载中直接访问。 论文作者提出了一个问题：是否可以将基于开放数据格式（Parquet 与 ORC）的数据湖转为一个高性能系统，该系统既拥有数据仓库强大的性能、管理功能，又可直接、快速访问高级分析工作负载？随着越来越多的业务应用开始依赖运营数据和高级分析，Lakehouse 架构可以消除数据仓库的上述挑战。 作者相信 Lakehouse 的时机已经到来！ Lakehouse架构 Lakehouse可解决以下问题： 数据湖上可靠的数据管理：Lakehouse 需要存储原始数据，同时支持 ETL/ELT 流程来提高数据分析质量。传统数据湖将半结构化数据以“一堆文件”形式进行管理，很难提供一些简化ETL/ELT的关键管理功能，例如事务、回滚、零拷贝。然而，以 Delta Lake 和 Apache Iceberg 为代表的新型数据湖框架提供了数据湖的事物视图，并提供了管理功能，减少 ETL 步骤，并且分析人员可以高效查询原始数据表，这与第一代分析平台很像。 支持机器学习和数据科学：机器学习系统支持直接读取数据湖数据格式，很多系统采用 DataFrames 作为操作数据的抽象，而声明式 DataFrame APIs 可以为机器学习工作负载中的数据访问进行查询优化，可以让机器学习工作负载直接享受 Lakehouse 的优化点。 SQL性能：Lakehouse 需要在海量 Parquet/ORC 数据集上提供很好的 SQL 性能，相比之下经典数据仓库对 SQL 优化更彻底。尽管如此，论文提出需要维护 Parquet/ORC 数据集的辅助数据，在现有格式内优化数据布局以实现更好的性能。 ","date":"2021-01-06","objectID":"/lakehouse/:2:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"出发点：数据仓库的挑战当前工业界对数据仓库不是很满意。首先是数据质量和可靠性不高，维护数据流分析的准确性是一件很困难的工作。其次，越来越多的商业分析需要最新的数据，但数据仓库不可避免地引入数据滞后性。第三，如今的非结构化数据比重大幅增加，但数据仓库并不能提供很好的非结构化数据分析。最后，现在工业界部署的机器学习与数据科学应用无法从数据仓库和数据湖中得到很好的支持。 当前工业界对数据湖 + 数据仓库的两层架构并不满意。首先是几乎所有的数据仓库近期都增加了对 Parquet 和 ORC 格式的外部表支持，允许数据仓库用户可以从相同的 SQL 引擎查询数据湖表，但这没有降低数据湖管理难度，也没有消除数据仓库 ETL 复杂度、滞后性和高级分析挑战。实际上，这些支持的性能通常较差，因为 SQL 引擎主要针对其内部数据格式进行了优化。其次，直接针对数据湖存储的 SQL 引擎也有广泛产品，例如 Spark SQL、Presto、Hive 和 AWS Athena。然而，这些引擎不能解决数据湖所有问题，也不能取代数据仓库，数据湖仍然缺少包括 ACID 事务的基础管理功能和有效访问方法，例如与数据仓库性能匹配的索引。 ","date":"2021-01-06","objectID":"/lakehouse/:3:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"Lakehouse 架构论文为 Lakehouse 提出一个定义：基于低成本、直接访问存储的数据管理系统，该系统具有传统分析型 DBMS 管理和性能，例如 ACID 事务、数据版本管理、数据审计、索引、缓存和查询优化。可以看出，Lakehouse 结合了数据湖和数据仓库的核心优势。问题的关键在于是否可以有效结合这些优势，特别是 Lakehouse 对直接访问的支持意味着其放弃了部分数据独立性。 Lakehouse 天然适合计算、存储分离的云环境：不同的计算应用程序按需分配在完全独立的计算节点（例如 ML 的 GPU 集群），同时直接访问相同的存储数据，但也可以在本地存储系统（如 HDFS）上实现 Lakehouse。 ","date":"2021-01-06","objectID":"/lakehouse/:4:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"实现 Lakehouse 系统实现 Lakehouse 的第一个关键思想是使用标准文件格式（如 Parquet）将数据存储在低成本的对象存储（如 Amazon S3、OSS）中，并在对象存储上实现元数据层，其定义了哪些对象是表版本一部分。这使系统可以在元数据层实现如 ACID 事务处理或版本控制之类的管理功能，同时将大量数据保存在低成本对象存储中，并允许客户端使用使用标准文件格式直接从该存储中读取对象。 尽管元数据层增加了管理功能，但不足以实现良好的 SQL 性能。数据仓库使用多种技术获得性能提升，比如将热数据存储在 SSD 等高速设备、维护统计信息、构建有效的访问方法（如索引）以及优化数据格式和计算引擎。基于现有存储格式的 Lakehouse 无法变更格式，但是也可以实现保持数据文件不变情况下的其他优化，包括缓存、辅助数据结构（例如索引和统计信息）和数据布局优化。 最终，Lakehouse 既可以加快高级分析负载，又可以为其提供更好的数据管理功能。许多机器学习库（如 Tensorflow 和 Spark MLlib）已经可以读取数据湖文件格式（如 Parquet）。因此将它们与 Lakeehouse 集成最简单方法是查询元数据层，查询哪些 Parquet 文件属于表，然后将它们传递给机器学习库。这些系统支持 DataFrame API，以便进行更好的优化。R 与 Pandas 推广了 DataFrames，为用户提供包含多种操作符的表抽象，其中大多数映射到关系代数。Spark SQL 等系统通过惰性计算转换与传递结果操作步骤到优化器实现该 API 声明式。因此，这些 API 可利用 Lakehouse 新优化特性实现机器学习加速，例如缓存和辅助数据。 APIs and Lakehouse ","date":"2021-01-06","objectID":"/lakehouse/:4:1","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"用于数据管理的元数据层Lakehouses 的第一个组件是元数据层，其可以实现 ACID 事务和其他管理功能。诸如 S3 或 HDFS 之类的数据湖存储系统仅提供了低级的对象存储或文件系统接口，在这些接口中，即使是简单的操作（如更新跨多个文件的表）也不是原子的，这个问题使得一些组织开始设计更丰富的数据管理层，从 Apache Hive ACID 开始，其使用 OLTP DBMS 跟踪给定表版本中哪些数据文件是 Hive 表的一部分，并允许操作以事务方式更新此集合。近年来一些新系统提供了更多功能和改进的可伸缩性，如 2016 年 Databricks 开发的 Delta Lake，其将有关哪些对象是表中一部分的信息存储在数据湖中，作为 Parquet 格式的事务日志，使其能够扩展到每张表数十亿个对象；Netflix 的 Apache Iceberg 也使用类似的设计，并支持 Parquet 和 ORC 存储；Apache Hudi 始于 Uber 也类似，尽管它不支持并发写入（正在支持中），该系统侧重于简化流式数据入数据湖。 这些系统的经验表明它们可以提供与原始 Parquet/ORC 数据湖类似或更好的性能，同时还增加了非常有用的管理功能，例如事务处理，零拷贝和回滚。 元数据层对数据质量非常重要，例如可以对 Schema 进行校验，使其不破坏数据质量。 另外元数据层可以实现诸如访问控制和审核日志记录之类的治理功能，例如元数据层可以在授予客户端凭据以从云对象存储读取表中的原始数据之前，检查是否允许客户端访问表，并且记录所有访问行为。 未来方向和替代设计。由于数据湖的元数据层非常新，因此存在许多悬而未决的问题和替代设计。例如 Delta Lake 设计为将事务日志存储在它运行的同一对象存储中（例如 S3）以简化管理（消除了运行单独存储系统的需要）并提供高可用性和高读取带宽，但对象存储的高延迟限制了它可以支持的每秒事务处理速率，在某些情况下将元数据使用更快的存储系统的设计可能更可取。同样Delta Lake、Iceberg 和 Hudi 仅支持单表事务，但也可以扩展以支持跨表事务，优化事务日志的格式和管理对象的大小也是未解决的问题。 ","date":"2021-01-06","objectID":"/lakehouse/:4:2","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"Lakehouse 中的 SQL 性能Lakehouse 方案的最大技术问题可能是如何提供最新的 SQL 性能，同时又放弃了传统 DBMS 设计中很大一部分的数据独立性，有很多解决方案，例如可以在对象存储上添加一个缓存层，以及是否可以更改数据对象存储格式而不使用现有的标准（例如 Parquet 和 ORC（不断改进这些格式的新设计不断涌现））。无论采用何种设计，核心挑战在于数据存储格式已成为系统公共 API 的一部分以允许快速直接访问，这与传统 DBMS 不同。 我们提出了几种技术可以在 Lakehouse 中优化 SQL 性能，并且与数据格式无关，因此可以将其与现有格式或未来数据格式一起使用，这些与格式无关的优化大致如下： 缓存：使用元数据层时，Lakehouse 系统可以安全地将云对象存储中的文件缓存在处理节点上更快的存储设备（例如 SSD 和 RAM）上，正在运行的事务可以确定读取缓存的文件是否还有效，此外缓存可以采用转码格式，其对于查询引擎运行效率更高，例如在 Databricks 的缓存会解压了部分它加载的 Parquet 数据。 辅助数据：即使 Lakehouse 为支持直接 I/O 访问需要开放表存储格式（如 Parquet），它也可以维护其他数据来帮助优化查询，如在 Parquet 文件中维护表中每个数据文件的列最小-最大统计信息，有助于跳过数据，以及基于 Bloom 过滤器的索引。可以实现各种各样的辅助数据结构，类似于为\"原始\"数据建立索引。 数据布局：数据布局在访问性能中起着重要作用。Lakehouse 系统也可以优化多个布局决策，最明显的是记录排序：哪些记录聚集在一起可以最容易被批量读取，Delta 中使用 Z-Order，Hudi 中使用基于哪些列进行 Clustering。 对于分析系统中的典型访问模式，这三个优化可以很好地协同工作。典型的工作负载中大多数查询倾向于集中在数据的\"热\"子集上，Lakehouse 可以使用与数据仓库相同的优化数据结构对其进行缓存，以提供相同的查询性能。对于云对象存储中的\"冷\"数据，性能的主要决定于每个查询读取的数据量，在该情况下数据布局优化（将共同访问的数据聚类）和辅助数据结构（如区域图，使引擎快速确定要读取的数据文件范围）的组合可以使Lakehouse系统与数仓一样最小化 I/O 开销，尽管使用标准的开放文件格式（相比于数仓内置文件格式）。 性能结果 TPC-DS比较 未来方向和替代设计。设计性能良好且可以直接访问的 Lakehouse 系统是未来工作的重点。一个尚待探索的方向是设计更好适应此类场景的数据湖存储格式，例如为 Lakehouse 系统实现数据布局优化或索引提供更大灵活性的存储格式，或者更适合现代硬件。 即使不改变数据格式，也有许多缓存策略、辅助数据结构和数据布局策略。哪一种对云对象存储中的海量数据集更有效是一个开放式问题。 最后，另一个值得研究的点是确定何时以及如何使用 serverless 计算系统来响应查询、优化存储、元数据层和查询引擎，以实现延迟最小化效果。 ","date":"2021-01-06","objectID":"/lakehouse/:4:3","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"高级分析高效访问高级分析库通常不是使用 SQL 命令编写，其需要访问大量数据。作者认为需要研究以下问题：如何设计数据访问层，最大程度地提高顶部运行代码的灵活性，并且可以从 Lakehouse 的优化中受益？ 优化方案 作者验证了如上图所示的优化方案：将缓存、数据筛选、数据布局优化同时应用，实现了机器学习算法加速。 机器学习 API 迅速发展，但是一些数据访问 API（例如 TensorFlow 的 tf.data）没有尝试将查询语义推入底层存储系统，一些 API 还专注于 CPU 到 GPU 的传输和 GPU 计算，这在数据仓库中并未引起太多关注。 未来方向和替代设计。论文提出需要多关注机器学习系统的数据访问接口。近期一些机器学习框架将算法逻辑融合进 SQL join 操作，其他应用在算法中的查询优化应用在 SQL 中。最后，作者认为需要标准机器学习接口以使数据科学家能够充分利用 Lakehouse（甚至数据仓库）中强大的数据管理功能，如事务，数据版本控制和回滚等。 ","date":"2021-01-06","objectID":"/lakehouse/:4:4","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"研究问题和启示Lakehouse 还提出了其他一些研究问题，功能日益丰富的数据湖的行业趋势也对数据系统研究的其他领域产生了影响。 还有其他方法可以实现 Lakehouse 目标吗？可以想像其他方法来实现 Lakehouse 的主要目标，例如构建用于数据仓库的大规模并行服务层，可以支持对高级分析工作负载的并行读取，但是与工作负载直接访问对象存储库相比成本将更高，难以管理，并且性能可能会降低。这种服务层并未得到广泛应用，例如 Hive LLAP。 除了在性能、可用性、成本和锁定方面的挑战外，还有一些重要的管理原因，如企业可能更喜欢将数据保留为开放格式。随着对数据管理的法规要求不断提高，组织可能需要在短时间内搜索旧数据集，删除各种数据或更改其数据处理基础结构，并且采用开放格式进行标准化意味着它们将始终可以直接访问数据，软件行业的长期趋势一直是开放数据格式，企业数据应该继续保持这种趋势。 什么是正确的存储格式和访问 API？Lakehouse 的访问接口包括原始存储格式以及直接读取此格式的客户端库（例如使用 TensorFlow 读取时）以及高级 SQL 接口。有很多不同的方法可以在这些层上放置丰富的功能，例如通过要求读者执行更复杂的“可编程”解码逻辑，可以为系统提供更大的灵活性的存储方案。有待观察哪种存储格式、元数据层设计和访问 API 的组合效果最佳。 Lakehouse 如何影响其他数据管理研究和趋势？数据湖的流行以及对丰富管理接口的使用不断增加，无论它们是元数据层还是完整的 Lakehouse 设计，都对数据管理研究的其他领域产生了影响。 Polystore 旨在解决跨不同存储引擎查询数据这一难题，该问题在企业中持续存在，但是在云数据湖中以开放格式提供的数据比例越来越高，也可以通过直接针对云对象存储运行许多 polystore 查询，即使基础数据文件是逻辑上分开的 Lakehouse 的一部分。 还可以在 Lakehouse 上设计数据集成和清理工具，并可以快速并行访问所有数据，这可以用于大型联接和聚类等新算法。 可以将 HTAP 系统构建为 Lakehouse 前面的\"附加\"层，通过使用其事务管理 API 将数据直接归档到 Lakehouse 系统中，Lakehouse 将能够查询数据的一致快照。 ML 的数据管理也会变得更加简单和强大，如今组织正在构建各种可重新实现标准 DBMS 功能的，特定于 ML 的数据版本控制和特征存储系统，使用带有内置 DBMS 管理功能的数据湖来实现特征存储功能可能会更简单。 Serverless 引擎之类的云原生 DBMS 设计将需要与更丰富的元数据层集成，而不是直接扫描数据湖中的原始文件，可以能够提高查询性能。 最后 Lakehouse 的设计易于分布式协作，因为可以从对象存储库直接访问所有数据集，这使得共享数据变得很简单。 ","date":"2021-01-06","objectID":"/lakehouse/:5:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"},{"categories":["论文学习"],"content":"结论在开放的数据湖文件格式上实现数据仓库功能的统一数据平台体系架构可以为当今的数据仓库系统提供具有竞争力的性能，并有助于应对数据仓库用户面临的许多挑战，尽管限制数据仓库的存储层以标准格式直接访问看起来似乎是一个重大限制，但诸如热数据缓存和冷数据数据布局优化之类的优化可以使 Lakehouse 获得很不错的性能，另外鉴于数据湖中已有大量数据，并且有机会大大简化企业数据架构，行业很可能会向 Lakehouse 架构逐步过渡。 ","date":"2021-01-06","objectID":"/lakehouse/:6:0","tags":["论文","数据湖","数据仓库","数据分析"],"title":"Lakehouse：统一数据仓库和高级分析的开放数据平台","uri":"/lakehouse/"}]